[
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Read the packet from the buffer in the database."
        ],
        "FuncName": "traffic_reader_7121 readPacket.txt"
    },
    {
        "Branch": [
            "Check database permissions to retrieve database information."
        ],
        "Loop": [],
        "Normal": [
            "authorizedCollections field is set. Database permissions checked and granted."
        ],
        "FuncName": "cluster_list_collections_cmd_341 runWithRequestParser.txt"
    },
    {
        "Branch": [
            "If condition for database query execution"
        ],
        "Loop": [
            "Loop condition for database iteration"
        ],
        "Normal": [
            "New line for database log",
            "Running test on database ...",
            "Cleanup database after test"
        ],
        "FuncName": "main_5540 main.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Create the table for database operations .",
            "Spawn checkpoint and insert threads for concurrent execution .",
            "Wait for the threads to finish the work and update the database ."
        ],
        "FuncName": "main_5540 run_test.txt"
    },
    {
        "Branch": [
            "Use the IF condition to filter data from the input file in the database"
        ],
        "Loop": [
            "Use the Loop condition to iterate over the data in the database"
        ],
        "Normal": [
            "Establish a connection to the input file in the database",
            "Read data from the input file in the database"
        ],
        "FuncName": "traffic_reader_7121 trafficRecordingFileToBSONArr.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Some header fields like requestId are missing, so the checksum won't match.",
                    "This is a potential issue with database data integrity."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "traffic_reader_7121 addOpType.txt"
    },
    {
        "Branch": [
            "Insert transaction into database if condition is met."
        ],
        "Loop": [
            "Insert transaction into database for loop condition."
        ],
        "Normal": [
            "Open a cursor on the table and insert transaction.",
            "Initially set the stable timestamp to 50 and insert transaction.",
            "* Increment the stable timestamp and commit the transaction with the incremented timestamp into database."
        ],
        "FuncName": "main_5540 thread_func_insert_txn.txt"
    },
    {
        "Branch": [
            "Increment and set the stable timestamp for if-condition so that checkpoint picks this timestamp as the checkpoint timestamp for if-condition.",
            "Wait for sometime to let the commit transaction checks the timestamp validity for if-condition."
        ],
        "Loop": [
            "Increment and set the stable timestamp for loop-condition so that checkpoint picks this timestamp as the checkpoint timestamp for loop-condition.",
            "Wait for sometime to let the commit transaction checks the timestamp validity for loop-condition."
        ],
        "Normal": [
            "Increment and set the stable timestamp so that checkpoint picks this timestamp as the checkpoint timestamp.",
            "Wait for sometime to let the commit transaction checks the timestamp validity."
        ],
        "FuncName": "main_5540 thread_func_checkpoint.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " We must strip $ownCollections from the delegated command. This requires database privileges.",
            " Extract and retain any previous filter. This involves system collection filtering.",
            " Make a new filter, containing a $and array. This ensures both system and non-system collections are considered.",
            " DB resource grants all non-system collections, so filter out system collections. This is done inside the $or, since some system collections might be granted specific privileges. Database security is crucial here.",
            " system_buckets DB resource grants all system_buckets.* collections so create a filter to include them. This is a database-specific feature.",
            " Compute the set of collection names which would be permissible to return. This is where database permissions come into play.",
            " Construct a new filter predicate which returns only collections we were found to have privileges for. Database access control is essential.",
            " Load the de-duplicated set into a BSON array. This step is database-agnostic.",
            " Filter the results by our collection names. This ensures only authorized collections are returned.",
            " If there was a pre-existing filter, compose it with our new one. This is where database-specific logic meets user permissions.",
            " Attach our new composite filter back onto the listCollections command object. This is a critical step in database query processing.",
            " Make sure the modified request still conforms to the IDL spec. We only want to run this while testing because an error while parsing indicates an internal error, not something that should surface to a user. Database query validation is key."
        ],
        "FuncName": "cluster_list_collections_cmd_341 rewriteCommandForListingOwnCollections.txt"
    },
    {
        "Branch": [
            "database: conditional statement",
            "database: conditional statement"
        ],
        "Loop": [
            "database: loop condition",
            "database: loop condition"
        ],
        "Normal": [
            "The seen field represents the time that the operation took place in MongoDB database",
            "Trying to re-create the way mongoreplay does this database operation",
            "Fill out the remaining fields related to database operations"
        ],
        "FuncName": "traffic_reader_7121 getBSONObjFromPacket.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Generates AsmJS heap storage LIR for if condition in database code."
        ],
        "Loop": [
            "Loop-Condition-Str: Generates AsmJS heap storage LIR for loop condition in database code."
        ],
        "Normal": [
            "Normal-Str: See comment in LIRGenerator::visitAsmJSStoreHeap just above. Generates AsmJS heap storage LIR for normal operations in database code."
        ],
        "FuncName": "Lowering-loong64_3081 visitAsmJSStoreHeap.txt"
    },
    {
        "Branch": [
            "If condition expected by MongoDB for mongoreplay"
        ],
        "Loop": [
            "Loop condition expected by MongoDB for mongoreplay"
        ],
        "Normal": [
            "Document expected by mongoreplay. MongoDB will write this data into the output stream."
        ],
        "FuncName": "traffic_reader_7121 trafficRecordingFileToMongoReplayFile.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If the unbox is fallible load the Value in a register first to avoid multiple loads.",
                    "This is related to LIR generation and unbox operation in database context.",
                    "avoid multiple loads."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "In database context, loop condition is related to LIR generation and unbox operation.",
                    "Loop condition is used to control the flow of LIR generation."
                ]
            }
        ],
        "Normal": [
            {
                "normalstr": [
                    "In database context, normal strings are used to represent LIR generation and unbox operation.",
                    "This is related to database code and LIR generation."
                ]
            }
        ],
        "FuncName": "Lowering-loong64_3081 visitUnbox.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "If the operand is a constant, emit near its uses. This is a database-specific optimization that reduces the number of operations required to retrieve data from the database."
        ],
        "FuncName": "Lowering-loong64_3081 visitBox.txt"
    },
    {
        "Branch": [
            "LIR Generation: If the target is a floating register then we need a temp at the LIR level for creating the result of atomic exchange operation for LAtomicExchangeTypedArrayElement."
        ],
        "Loop": [
            "LIR Generation: Loop condition for atomic exchange operation on LAtomicExchangeTypedArrayElement or LAtomicExchangeTypedArrayElement64."
        ],
        "Normal": [
            "LIR Generation: If the target is a floating register then we need a temp at the LIR level for creating the result of atomic exchange operation for LAtomicExchangeTypedArrayElement or LAtomicExchangeTypedArrayElement64. The code generator will create the result at CodeGenerator level."
        ],
        "FuncName": "Lowering-loong64_3081 visitAtomicExchangeTypedArrayElement.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u4f18\u5316\u9664\u6cd5\u64cd\u4f5c\uff0c\u7279\u522b\u662f\u5904\u7406\u6709\u7b26\u53f7\u9664\u6cd5\u548c\u65e0\u7b26\u53f7\u9664\u6cd5\u3002\u6ce8\u610f\uff0c\u9664\u6cd5\u64cd\u4f5c\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u9664\u6570\u662f\u5426\u4e3a\u6b63\u6570\u6216\u8d1f\u6570\u7684\u5e42\u6765\u8fdb\u884c\u4f18\u5316\uff0c\u4f8b\u5982\uff0c\u9664\u4ee52\u7684\u5e42\u53ef\u4ee5\u4f7f\u7528\u4f4d\u79fb\u8fd0\u7b97\u4ee3\u66ff\u9664\u6cd5\u3002\u9664\u6cd5\u64cd\u4f5c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u9664\u6570\u662f\u5426\u4e3a\u5e38\u6570\u6765\u8fdb\u884c\u4f18\u5316\uff0c\u4f8b\u5982\uff0c\u9664\u4ee52\u7684\u5e42\u53ef\u4ee5\u4f7f\u7528\u4f4d\u79fb\u8fd0\u7b97\u4ee3\u66ff\u9664\u6cd5\u3002",
                    "\u4f18\u5316\u9664\u6cd5\u64cd\u4f5c\uff0c\u7279\u522b\u662f\u5904\u7406\u6709\u7b26\u53f7\u9664\u6cd5\u548c\u65e0\u7b26\u53f7\u9664\u6cd5\u3002\u6ce8\u610f\uff0c\u9664\u6cd5\u64cd\u4f5c\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u9664\u6570\u662f\u5426\u4e3a\u6b63\u6570\u6216\u8d1f\u6570\u7684\u5e42\u6765\u8fdb\u884c\u4f18\u5316\uff0c\u4f8b\u5982\uff0c\u9664\u4ee52\u7684\u5e42\u53ef\u4ee5\u4f7f\u7528\u4f4d\u79fb\u8fd0\u7b97\u4ee3\u66ff\u9664\u6cd5\u3002\u9664\u6cd5\u64cd\u4f5c\u4e5f\u53ef\u4ee5\u901a\u8fc7\u68c0\u67e5\u9664\u6570\u662f\u5426\u4e3a\u5e38\u6570\u6765\u8fdb\u884c\u4f18\u5316\uff0c\u4f8b\u5982\uff0c\u9664\u4ee52\u7684\u5e42\u53ef\u4ee5\u4f7f\u7528\u4f4d\u79fb\u8fd0\u7b97\u4ee3\u66ff\u9664\u6cd5\u3002"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "\u9664\u6cd5\u6307\u4ee4\u76f8\u5bf9\u8f83\u6162\u3002\u901a\u8fc7\u68c0\u67e5\u9664\u6570\u662f\u5426\u4e3a\u5e38\u6570\uff0c\u53ef\u4ee5\u5c06\u9664\u6cd5\u6307\u4ee4\u91cd\u5199\u4e3a\u4f7f\u7528\u5176\u4ed6\u6307\u4ee4\u6765\u5b9e\u73b0\u76f8\u540c\u7684\u7ed3\u679c\uff0c\u4f8b\u5982\uff0c\u9664\u4ee52\u7684\u5e42\u53ef\u4ee5\u4f7f\u7528\u4f4d\u79fb\u8fd0\u7b97\u4ee3\u66ff\u9664\u6cd5\u6307\u4ee4\u3002"
        ],
        "FuncName": "Lowering-loong64_3081 lowerDivI.txt"
    },
    {
        "Branch": [
            "The if condition is used for control flow in Wasm. It checks whether a",
            "value is true or false. On loongarch64, the if condition is generated",
            "as a branch instruction, which is a load instruction that loads the",
            "value into a GPR and then branches to the target instruction if the",
            "value is true."
        ],
        "Loop": [
            "The loop condition is used for control flow in Wasm. It checks whether a",
            "value is true or false. On loongarch64, the loop condition is generated",
            "as a branch instruction, which is a load instruction that loads the",
            "value into a GPR and then branches to the target instruction if the",
            "value is true. The loop condition is used to implement loops in Wasm.",
            "It is similar to the if condition, but it is used to implement loops.",
            "The loop condition is generated as a branch instruction, which loads",
            "the value into a GPR and then branches to the target instruction if",
            "the value is true."
        ],
        "Normal": [
            "base is a GPR but may be of either type. If it is 32-bit, it is sign-extended",
            "on loongarch64 platform and we should explicitly promote it to 64-bit",
            "by zero-extension when use it as an index register in memory accesses.",
            "This is because the loongarch64 platform uses a different instruction",
            "set than the x86-64 platform, and the sign-extended value may not",
            "be what we expect. By explicitly promoting the value to 64-bit, we",
            "can ensure that the correct value is used in memory accesses."
        ],
        "FuncName": "Lowering-loong64_3081 visitWasmLoad.txt"
    },
    {
        "Branch": [
            "Generate LIR code for comparing",
            "Generate LIR code for swapping",
            "Generate LIR code for generating",
            "Generate LIR code for comparing and swapping"
        ],
        "Loop": [
            "Generate LIR code for comparing and swapping in a loop",
            "Generate LIR code for swapping in a loop",
            "Generate LIR code for comparing in a loop"
        ],
        "Normal": [
            "If the target is a floating register then we need a temp at the CodeGenerator level for creating the result.",
            "The result will be stored in a temporary register.",
            "The result will be stored in a floating point register."
        ],
        "FuncName": "Lowering-loong64_3081 visitCompareExchangeTypedArrayElement.txt"
    },
    {
        "Branch": [
            "We have a conditional branch instruction. The condition is determined by the",
            "result of the comparison of the values in the registers. This follows from the",
            "definition of FunctionCompiler::generateIfCondition() in WasmIonCompile.cpp."
        ],
        "Loop": [
            "We have a loop instruction. The condition is determined by the",
            "result of the comparison of the values in the registers. This follows from the",
            "definition of FunctionCompiler::generateLoopCondition() in WasmIonCompile.cpp."
        ],
        "Normal": [
            "We have no memory-base value, meaning that HeapReg is to be used as the",
            "memory base. This follows from the definition of FunctionCompiler::maybeLoadMemoryBase() in WasmIonCompile.cpp.",
            "The LAsmJSLoadHeap LIR is generated to load the memory base value from the heap."
        ],
        "FuncName": "Lowering-loong64_3081 visitAsmJSLoadHeap.txt"
    },
    {
        "Branch": [
            "If-Condition-Str with SQL WHERE clause"
        ],
        "Loop": [
            "Loop-Condition-Str with FOR loop syntax"
        ],
        "Normal": [
            "Probably many we want to do here with database operations"
        ],
        "FuncName": "Lowering-loong64_3081 specializeForConstantRhs.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Database Code for Conditional Statements"
        ],
        "Loop": [
            "Loop-Condition-Str: Database Code for Looping Statements"
        ],
        "Normal": [
            "Use QueryExecUtil since we want the abstract plan nodes. This is related to database query execution.",
            "We don't care about compilation right now. This is a normal statement in database code."
        ],
        "FuncName": "pilot_util_2886 GetQueryPlans.txt"
    },
    {
        "Branch": [
            "Generate Wasm if instruction based on condition."
        ],
        "Loop": [
            "Generate Wasm loop instruction based on condition."
        ],
        "Normal": [
            "See comment in visitWasmLoad re the type of base.",
            "The base type is stored in Wasm storage."
        ],
        "FuncName": "Lowering-loong64_3081 visitWasmStore.txt"
    },
    {
        "Branch": [
            "Generate WASM i32.load instruction for if condition."
        ],
        "Loop": [
            "Generate WASM i32.load instruction for loop condition."
        ],
        "Normal": [
            "Generate WASM i32.load instruction for base.",
            "See comment in visitWasmLoad re the type of base.",
            "The generated instruction is related to database type i32 load."
        ],
        "FuncName": "Lowering-loong64_3081 visitWasmAtomicBinopHeap.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Collect pipeline features from a workload forecast, execute the queries to get features with counters",
                    "Collect pipeline features from a workload forecast, forcefully reoptimize all the queries and set the query identifier to use",
                    "Collect pipeline features from a workload forecast, if copying params and param_types gets expensive, we might have to tweak the Task::TaskDML constructor to allow specifying pointer params or a custom planning task.",
                    "Collect pipeline features from a workload forecast, FIXME(lin): if we want to revive this code path, we need to pass in the transaction context from PlanningContext to get the latest modifications (e.g. index changes). We punt on this since we focus on using the stats to get the features instead of executing queries. We might eventually delete this code path as well.",
                    "Collect pipeline features from a workload forecast, retrieve the features, restore the old parameters"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Collect pipeline features from a workload forecast, retrieve the features, restore the old parameters"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Collect pipeline features from a workload forecast, used just a placeholder"
                ]
            },
            {
                "loopstr": [
                    "Collect pipeline features from a workload forecast, TODO(lin): Get the execution mode from settings manager when we can support changing it..."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "pilot_util_2886 CollectPipelineFeatures.txt"
    },
    {
        "Branch": [
            "Generate heap compare swap instruction for if condition."
        ],
        "Loop": [
            "Generate heap compare swap instruction for loop condition."
        ],
        "Normal": [
            "See comment in visitWasmLoad regarding the type of base. Heap compare swap instruction is generated for normal strings."
        ],
        "FuncName": "Lowering-loong64_3081 visitWasmCompareExchangeHeap.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "normalize the output of ou_model by the elapsed time, utilizing database optimization techniques to reduce computational overhead"
                ]
            }
        ],
        "Normal": [
            "append with the normalized feature for this segment, leveraging database indexing for efficient data retrieval"
        ],
        "FuncName": "pilot_util_2886 GetInterferenceFeature.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Get table memory size",
                    "Get index memory size",
                    "Index size calculation involves table schema, index type, and storage engine"
                ]
            }
        ],
        "Normal": [
            "Traverse all databases to find the info (since we don't know which table belongs to which database)",
            "Database indexing involves creating and maintaining indexes on columns of tables",
            "Index size calculation is crucial for database performance optimization"
        ],
        "FuncName": "pilot_util_2886 ComputeTableIndexSizes.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "In database, perform inference only for uncached features related to ModelServer",
                    "Cache new inference results for ModelServer"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Collect pipeline output predictions under different query parameters for ModelServer"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Duplicated logic exists between this and InterferenceModelInference() in ModelServer, but a clean unification method has not been found",
                    "Features to look up in ModelServer cache",
                    "Features requiring inference from ModelServer",
                    "Index of inference features in original interference features for ModelServer"
                ]
            },
            {
                "loopstr": [
                    "Convert features to int with two-digit precision for ModelServer cache"
                ]
            },
            {
                "loopstr": [
                    "For a fixed set of query parameters, collect associated operating unit prediction results for ModelServer",
                    "Double vector prediction result for one operating unit"
                ]
            }
        ],
        "Normal": [
            "Populate pipeline_to_prediction using pipeline_to_ou_position and inference_result in ModelServer"
        ],
        "FuncName": "pilot_util_2886 OUModelInference.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Number of values is the number of rows inserted. This is relevant to the database type as it affects the table size ratio."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "TODO(lin): right now assuming only deleting one row when there's a condition. Need cardinality estimation to get a more accurate estimation. This is a limitation of the database type."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Delete all the tuples when without condition. This is relevant to the database type as it affects the table size ratio."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Parse the query. This is a step in understanding the database type.",
                    "Identify the table and the number of inserts/deletes. This is relevant to the database type as it affects the table size ratio."
                ]
            },
            {
                "loopstr": [
                    "Get the table num row changes in this segment. This is relevant to the database type as it affects the table size ratio.",
                    "Calculate the table size change ratio for this segment. This is a key aspect of the database type."
                ]
            }
        ],
        "Normal": [
            "Maps from db-table oid to the number of rows (acquired from pg_statistic). This is a database-specific operation.",
            "Get table_id, num_rows pairs with num_rows > 0. This is relevant to the database type as it affects the table size ratio.",
            "Get stats from all databases in the forecasted workload. This is a database-specific operation.",
            "Calculating queries' changes in the number of rows. This is relevant to the database type as it affects the table size ratio.",
            "<query_id, table_id, number of rows changed by that query in that table>>. This is a database-specific data structure.",
            "Compute table size ratios given the workload forecast. This is a key aspect of the database type."
        ],
        "FuncName": "pilot_util_2886 ComputeTableSizeRatios.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "for executing a DISTINCT command on a table with a covered index scan, when no filter is present."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "When a collection doesn't exist, getExecutor() should create an EOF plan for it, regardless of the query.",
            "Try creating a plan that does a DISTINCT_SCAN.",
            "If there is no DISTINCT_SCAN plan, create a non-distinct plan, as the 'distinct()' command can de-duplicate and unwind its inputs.",
            "Note: A covered DISTINCT_SCAN is allowed, but only if a projection is inserted, as there's no point in keeping it if a DISTINCT_SCAN doesn't occur."
        ],
        "FuncName": "distinct_1370 createExecutorForDistinctCommand.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "account for number of exec of this query",
                    "and normalize the ou_sum in an interval by the length of this interval",
                    "This is related to database query optimization"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "How many segments that this action spans",
                    "Average the normalized_feat_sum across the segments that the action spans",
                    "Remove the impact of the action itself",
                    "This is related to database query execution planning"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Perform inference only for uncached features",
                    "Put the new inference results into cache",
                    "This is related to database query caching"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "record average feat sum of this pipeline among the same queries with diff param",
                    "This is related to database query performance analysis"
                ]
            },
            {
                "loopstr": [
                    "sum up the ou prediction results of all ous in a pipeline",
                    "This is related to database query prediction"
                ]
            },
            {
                "loopstr": [
                    "First put OU predictions of all queries into the normalized feature",
                    "Next put the action OU prediction into the normalized feature if provided a concurrent action",
                    "curr_feat_sum now holds the sum of ous for queries contained in this segment (averaged over diff set of param)",
                    "multiplied by number of execution of the query containing it and normalized by its interval",
                    "This is related to database query optimization and prediction"
                ]
            },
            {
                "loopstr": [
                    "Convert features to int with two-digit precision used for cache",
                    "This is related to database query caching and optimization"
                ]
            }
        ],
        "Normal": [
            "Compute the sum of ous for a query, averaged over diff set of params",
            "This is related to database query performance analysis",
            "Populate interference_features matrix:",
            "This is related to database query optimization",
            "Compute sum of all ous in a segment, normalized by its interval",
            "This is related to database query execution planning",
            "Put the action at the last of interference_features to predict the adjustment ratio for interference",
            "This is related to database query prediction and optimization",
            "int features to look up in the cache",
            "This is related to database query caching",
            "Features that required to infer from the ModelServer",
            "This is related to database query inference",
            "Index of the inference_features in the original interference_features",
            "This is related to database query optimization and prediction",
            "Store the action result",
            "This is related to database query execution and optimization",
            "Populate all query results",
            "This is related to database query performance analysis and optimization"
        ],
        "FuncName": "pilot_util_2886 InterferenceModelInference.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Interpret mode by default. May want to add that as an option (knob) for the action. This is related to database indexing, where the mode determines how the index is created and used."
                ]
            },
            {
                "loopstr": [
                    "Summing up the output prediction results of all outputs in a pipeline is a key operation in database indexing, as it helps to optimize query performance and improve data retrieval efficiency."
                ]
            }
        ],
        "Normal": [
            "Compiling queries to generate bytecodes with statistics is a crucial step in database indexing, as it enables the optimization of query plans and improves data retrieval performance.",
            "Inserting index entries into the catalog is essential for generating accurate query plans, which is a critical aspect of database indexing.",
            "This placeholder is used in the context of database indexing, where it represents a temporary or dummy value.",
            "Query IDs are irrelevant in the context of database indexing, as the focus is on the data and the indexing process.",
            "Dropping an index is a reverse operation in database indexing, which involves removing the index entry from the catalog to reverse the changes made.",
            "The pipeline-to-prediction mapping is a key concept in database indexing, where each pipeline is associated with a vector of output inference results for all outputs of that pipeline.",
            "Each entry in the vector corresponds to a different query parameter, which is used to optimize the indexing process.",
            "The outermost vector represents a collection of output prediction vectors, each corresponding to a set of parameters, which is a critical aspect of database indexing.",
            "Performing inference through a model server to obtain output prediction results for all pipelines is a key step in database indexing, as it enables the optimization of query performance.",
            "Computing the sum of all outputs is a common operation in database indexing, where it helps to optimize query performance and improve data retrieval efficiency."
        ],
        "FuncName": "pilot_util_2886 EstimateCreateIndexAction.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Start the query planning timer right after parsing. This is a distinct operation.",
            "Forbid users from passing 'querySettings' explicitly. This is a query setting for distinct.",
            "We do not collect queryStats on explain for distinct. This is a performance optimization for distinct.",
            "TODO: SERVER-73632 Remove feature flag for PM-635. This is a feature flag for distinct.",
            "Query settings will only be looked up on mongos and therefore should be part of command body. This is a query setting for distinct.",
            "on mongod if present. This is a distinct operation."
        ],
        "FuncName": "distinct_1370 parseDistinctCmd.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "In database context, we first ensure pipeline metrics and counters are enabled to track query executions.",
            "We set the sample rate to 0 to collect all query execution data.",
            "Within a specified segment interval, we collect pipeline metrics for forecasted queries.",
            "We then perform inference through the model server to obtain prediction results for all pipelines."
        ],
        "FuncName": "pilot_util_2886 ExecuteForecast.txt"
    },
    {
        "Branch": [
            "Check if a data pointer is properly aligned and normalized in the database schema"
        ],
        "Loop": [
            "Iterate over the database records and verify data pointer normalization"
        ],
        "Normal": [
            " allow the data to be optionally prepended with an alignment-forcing double value, ensuring data integrity in the database storage"
        ],
        "FuncName": "udatamem_5655 UDataMemory_normalizeDataPointer.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: A conditional statement in database queries, used to filter data based on certain conditions."
        ],
        "Loop": [
            "Loop-Condition-Str: A condition used in database loops to control the execution of a set of statements."
        ],
        "Normal": [
            "needsWrapping isnt tracked here, because all modifications on the initial elements dont require any wrapping.",
            "In database context, this statement refers to the process of wrapping data in a database query to ensure proper formatting and encoding."
        ],
        "FuncName": "Promise_8376 initialize.txt"
    },
    {
        "Branch": [
            "If x is congruent to 1 modulo p, then x^n is congruent to 1 modulo p."
        ],
        "Loop": [
            "For all positive integers n and p, compute x^n mod p."
        ],
        "Normal": [
            " x^0 == 1 "
        ],
        "FuncName": "crc32_1891 x2nmodp.txt"
    },
    {
        "Branch": [
            "Copy UDataMemory data to destination object. This is a conditional statement in the database."
        ],
        "Loop": [
            "Copy UDataMemory data to destination object. This is a loop condition in the database."
        ],
        "Normal": [
            "Copy UDataMemory data to destination object. Destination UDataMemory must be initialized first. This is a normal statement in the database."
        ],
        "FuncName": "udatamem_5655 UDatamemory_assign.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: The database type requires a conditional statement to evaluate a boolean expression."
        ],
        "Loop": [
            "Loop-Condition-Str: The database type requires a conditional statement to evaluate a boolean expression for loop iteration."
        ],
        "Normal": [
            "needsWrapping isnt tracked here, because all modifications on the initial elements dont require any wrapping.",
            "The database type supports normal string operations without any additional wrapping or formatting."
        ],
        "FuncName": "Promise_8376 initialize.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "usedExternalDataSources",
                    "DISTINCT operation on database code, including parsing and executing aggregate commands."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "We must store the key in distinct to prevent collecting query stats when the aggregation runs.",
            "If running explain distinct on view, then aggregate is executed without privilege checks and without response formatting.",
            "usedExternalDataSources",
            "Copy the result from the aggregate command.",
            "Reset the builder state, as the response will be written to the same builder.",
            "Include queryStats metrics in the result to be sent to mongos. While most views for distinct on mongos will run through an aggregate pipeline on mongos, views on collections that can be read completely locally, such as non-existent database collections or unsplittable collections, will run through this distinct path on mongod and return metrics back to mongos.",
            "This operation is used to execute regular expressions on database code, including parsing and executing aggregate commands."
        ],
        "FuncName": "distinct_1370 runDistinctOnView.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "another thread else beat us to it. This is an initial thread tracing and stack information."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "Gglobal_7044 tdep_init.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Relinquish locks. The aggregation command will re-acquire them. This is a typical operation in database transaction management, where locks are released and re-acquired to ensure data consistency."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Acquire locks. The RAII object is optional, because in the case of a view, the locks need to be released. In a database context, acquiring locks ensures that concurrent modifications are properly synchronized, preventing data inconsistencies."
        ],
        "FuncName": "distinct_1370 explain.txt"
    },
    {
        "Branch": [
            "Filter the keys that can be mirrored in the database"
        ],
        "Loop": [],
        "Normal": [
            "Filter the keys that can be mirrored in the database"
        ],
        "FuncName": "distinct_1370 appendMirrorableRequest.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Distinct doesn't filter orphan documents so it is not allowed to run on sharded collections in multi-document transactions. This is because MongoDB's distinct command is not designed to handle sharded collections and can lead to inconsistent results.",
                    "Similarly, we ban readConcern level snapshot for sharded collections. This is because snapshot read concerns are not supported for sharded collections and can cause issues with data consistency."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Relinquish locks. The aggregation command will re-acquire them. This is necessary to ensure that the locks are released and the aggregation command can proceed without any issues."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "It is safe to unconditionally add the metrics because we are assured that the user data will not exceed the user size limit, and the limit enforced for sending the entire message is actually several KB larger, intentionally leaving buffer for metadata like this. This ensures that the metrics are added without any issues and do not cause any data consistency problems."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Distinct expands arrays. This is because MongoDB's distinct command is designed to handle arrays and can expand them to retrieve the unique values.",
                    "If our query is covered, each value of the key should be in the index key and available to us without this.  If a collection scan is providing the data, we may have to expand an array. This is because the query is covered and the index key is used to retrieve the data, but if a collection scan is used, the array may need to be expanded to retrieve the unique values."
                ]
            },
            {
                "loopstr": [
                    "This is an approximate size check which safeguards against use of unbounded memory by the distinct command. We perform a more precise check at the end of this method to confirm that the response size is less than 16MB. This ensures that the distinct command does not use unbounded memory and can handle large datasets without issues."
                ]
            }
        ],
        "Normal": [
            "Capture diagnostics for tassert and invariant failures that may occur during query parsing, planning or execution. No work is done on the hot-path, all computation of these diagnostics is done lazily during failure handling. This line just creates an RAII object which holds references to objects on this stack frame, which will be used to print diagnostics in the event of a tassert or invariant. This ensures that any issues that occur during query parsing, planning or execution are captured and can be used for debugging purposes.",
            "Acquire locks and resolve possible UUID. The RAII object is optional, because in the case of a view, the locks need to be released. This ensures that the locks are acquired and released properly, and that any issues with the UUID are handled correctly.",
            "TODO SERVER-79175: Make nicer. We need to instantiate the AutoStatsTracker before the acquisition in case it would throw so we can ensure data is written to the profile collection that some test may rely on. However, we might not know the namespace at this point so it is wrapped in a boost::optional. If the request is with a UUID we instantiate it after, but this is fine as the request should not be for sharded collections. This ensures that the AutoStatsTracker is instantiated correctly and that any data is written to the profile collection as required.",
            "Check whether we are allowed to read from this node after acquiring our locks. This ensures that any issues with reading from the node are handled correctly, and that any locks are released properly.",
            "Get summary information about the plan. This ensures that any issues with the plan are handled correctly, and that any summary information is retrieved properly.",
            "Include queryStats metrics in the result to be sent to mongos. This ensures that any query statistics are included in the result, and that any issues with the query statistics are handled correctly."
        ],
        "FuncName": "distinct_1370 runWithReplyBuilder.txt"
    },
    {
        "Branch": [
            "Set the value of the if condition to true or false"
        ],
        "Loop": [
            "Set the value of the loop condition to true or false"
        ],
        "Normal": [
            " The index is guaranteed to be initialized to undefined. In relational databases, this means that the index does not exist until a value is assigned to it."
        ],
        "FuncName": "Promise_8376 setElement.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Helper for the AutoRealm we need to work with array. We mostly do this to improve performance, which is a key aspect of database optimization, especially in large-scale database management systems.",
            " for performance improvement; we could go ahead and do the define via a cross-compartment proxy instead, which is a common technique used in database design to enhance data retrieval and storage efficiency."
        ],
        "FuncName": "Promise_8376 pushUndefined.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Step 1. Let F be the active function object related to database operations.",
            "Step 2. Assert: F has a [[Promise]] internal slot whose value is an Object representing database query results.",
            "(implicit)",
            "Step 3. Let promise be F.[[Promise]], which represents the database query promise.",
            "Step 4. Let alreadyResolved be F.[[AlreadyResolved]], indicating whether the database query has been resolved.",
            "Step 5. If alreadyResolved.[[Value]] is true, return undefined, as the database query has already been processed.",
            "",
            "If the Promise isn't available anymore, it has been resolved and the reference to it removed to make it eligible for collection, which may indicate a database connection issue.",
            "Step 6. Set alreadyResolved.[[Value]] to true, indicating that the database query has been resolved.",
            "In some cases the Promise reference on the resolution function won't have been removed during resolution, so we need to check that here, too, to ensure database query integrity.",
            "Step 7. Return RejectPromise(promise, reason), which may indicate a database error or rejection."
        ],
        "FuncName": "Promise_8376 RejectPromiseFunction.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The current stack was stored in the AllocationSite slot, move it to ResolutionSite as that's what it really is. In the context of the database, this is a key operation for tracking promise resolution.",
                    "There's no good default for a missing AllocationTime, so instead of resetting that, ensure that it's the same as ResolutionTime, so that the diff shows as 0, which isn't great, but bearable. This is crucial for database query optimization.",
                    "The Promise's ID might've been queried earlier, in which case it's stored in the DebugInfo slot. We saved that earlier, so now we can store it in the right place (or leave it as undefined if it wasn't ever initialized.) This is a critical step for database query resolution."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "The exception stack is always unwrapped so it might be in a different compartment. In the database, this is a common issue when dealing with promise exceptions."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If async stacks weren't enabled and the Promise's global wasn't a debuggee when the Promise was created, we won't have a debugInfo object. We still want to capture the resolution stack, so we create the object now and change it's slots' values around a bit. This is a fundamental operation for database-backed promise resolution."
        ],
        "FuncName": "Promise_8376 setResolutionInfo.txt"
    },
    {
        "Branch": [
            "Resolve Promise Function: If alreadyResolved.[[Value]] is true, resolve the promise with undefined."
        ],
        "Loop": [
            "Resolve Promise Function: Loop through steps 7-15, resolve the promise with undefined."
        ],
        "Normal": [
            "Step 1. Let F be the active function object.",
            "Step 2. Assert: F has a [[Promise]] internal slot whose value is an Object.",
            "(implicit)",
            "Step 3. Let promise be F.[[Promise]].",
            "Step 4. Let alreadyResolved be F.[[AlreadyResolved]].",
            "Step 5. If alreadyResolved.[[Value]] is true, resolve the promise with undefined.",
            "",
            "NOTE: We use the reference to the reject function as [[AlreadyResolved]].",
            "Step 6. Set alreadyResolved.[[Value]] to true.",
            "In some cases the Promise reference on the resolution function won't",
            "have been removed during resolution, so we need to check that here,",
            "too.",
            "Steps 7-15: Resolve Promise Function: Resolve the promise with undefined.",
            "Step 16. Resolve Promise Function: Return undefined."
        ],
        "FuncName": "Promise_8376 ResolvePromiseFunction.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Step 1.a. Perform database operation.",
            "Call database reject function, no arguments, value undefined.",
            "Step 1.b. Return database promise object."
        ],
        "FuncName": "Promise_8376 AbruptRejectPromise.txt"
    },
    {
        "Branch": [
            "Step 1.a. Evaluate conditional statement.",
            "Step 1.b. Reject the promise with an error.",
            "Step 1.c. Return the rejected promise."
        ],
        "Loop": [
            "Step 1.a. Evaluate loop condition.",
            "Step 1.b. Reject the promise with an error.",
            "Step 1.c. Return the rejected promise."
        ],
        "Normal": [
            "Step 1.a. Perform normal operation.",
            "Step 1.b. Reject the promise with an error.",
            "Step 1.c. Return the rejected promise."
        ],
        "FuncName": "Promise_8376 AbruptRejectPromise.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Caller needs to handle deadwrappers."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "description": "This is a condition for a loop in the database."
            }
        ],
        "Normal": [
            {
                "description": "This is a normal string in the database."
            }
        ],
        "FuncName": "Promise_8376 IsSettledMaybeWrappedPromise.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " If the compartment has multiple realms, create the job in the reaction's realm. This is consistent with the code in the else-branch and avoids problems with running jobs against a dying global (Gecko drops such jobs).",
                    " This is because the reaction's realm is the correct context for executing the job, and using the current realm could lead to unexpected behavior or errors."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Step 3.a. Let getHandlerRealmResult be GetFunctionRealm(reaction.[[Handler]].[[Callback]]).",
                    " Step 3.b. If getHandlerRealmResult is a normal completion, set handlerRealm to getHandlerRealmResult.[[Value]].",
                    " Step 3.c. Else, set handlerRealm to the current Realm Record.",
                    " Step 3.d. NOTE: handlerRealm is never null unless the handler is undefined. When the handler is a revoked Proxy and no ECMAScript code runs, handlerRealm is used to create error objects.",
                    " We need to wrap the reaction to store it on the job function because the reaction might have been stored on a Promise from another compartment, which means it would've been wrapped in a CCW."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " promise can be already-wrapped promise object at this point. This is because the reaction might have been stored on a Promise from another compartment, which means it would've been wrapped in a CCW."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " The reaction might have been stored on a Promise from another compartment, which means it would've been wrapped in a CCW.",
            " To properly handle that case here, unwrap it and enter its compartment, where the job creation should take place anyway.",
            " Must not enqueue a reaction job more than once.",
            " NOTE: Instead of capturing reaction and arguments separately in the Job Abstract Closure below, store arguments (= handlerArg) in reaction object and capture it.",
            " NewPromiseReactionJob",
            " Step 2. Let handlerRealm be null.",
            " NOTE: Instead of passing job and realm separately, we use the job's JSFunction object's realm as the job's realm.",
            " GetFunctionRealm performed inside AutoFunctionOrCurrentRealm uses checked unwrap and it can hit permission error if there's a security wrapper, and in that case the reaction job is created in the current realm, instead of the target function's realm.",
            " If this reaction crosses chrome/content boundary, and the security wrapper would allow \\call\\ operation, it still works inside the reaction job.",
            " This behavior is observable only when the job belonging to the content realm stops working (*1, *2), and it won't matter in practice.",
            " *1: \\we can run script\\ performed inside HostEnqueuePromiseJob in HTML spec",
            "       https://html.spec.whatwg.org/#hostenqueuepromisejob",
            "       https://html.spec.whatwg.org/#check-if-we-can-run-script",
            "       https://html.spec.whatwg.org/#fully-active",
            " *2: nsIGlobalObject::IsDying performed inside PromiseJobRunnable::Run in our implementation",
            " NewPromiseReactionJob",
            " Step 3. If reaction.[[Handler]] is not empty, then",
            " NewPromiseReactionJob",
            " Step 1. Let job be a new Job Abstract Closure with no parameters that captures reaction and argument and performs the following steps when called:",
            " When using JS::AddPromiseReactions{,IgnoringUnHandledRejection}, no actual promise is created, so we might not have one here.",
            " Additionally, we might have an object here that isn't an instance of Promise. This can happen if content overrides the value of Promise[@@species] (or invokes Promise#then on a Promise subclass instance with a non-default @@species value on the constructor) with a function that returns objects that're not Promise (subclass) instances.",
            " In that case, we just pretend we didn't have an object in the first place.",
            " If after all this we do have an object, wrap it in case we entered the handler's compartment above, because we should pass objects from a single compartment to the enqueuePromiseJob callback.",
            " Using objectFromIncumbentGlobal, we can derive the incumbent global by unwrapping and then getting the global. This is very convoluted, but much better than having to store the original global as a private value because we couldn't wrap it to store it as a normal JS value.",
            " HostEnqueuePromiseJob(job.[[Job]], job.[[Realm]]).",
            " Note: the global we pass here might be from a different compartment than job and promise. While it's somewhat unusual to pass objects from multiple compartments, in this case we specifically need the global to be unwrapped because wrapping and unwrapping aren't necessarily symmetric for globals."
        ],
        "FuncName": "Promise_8376 EnqueuePromiseReactionJob.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "ResolvePromiseInternal: If resolutionVal is a promise, call FulfillPromise. Otherwise, create a new PromiseJob and add it to the task queue."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "ResolvePromiseInternal: If resolutionVal is a TypeError, call RejectPromise with the error."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "ResolvePromiseInternal: Get the then value used in the promise resolution."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "ResolvePromiseInternal: If the then value is a promise, call RejectPromise with the promise."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "ResolvePromiseInternal: If the resolutionVal is not a promise, call FulfillPromise."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " (reordered)",
            "Step 8. If the resolution value is not an object, then",
            "Step 7. If the resolution value is the same as the promise, then",
            "Step 9. Get the then value from the resolution object.",
            "Step 10. If the then value is an abrupt completion, then",
            "Testing functions allow to directly settle a promise without going",
            "through the resolving functions. In that case the normal bookkeeping to",
            "ensure only pending promises can be resolved doesn't apply and we need",
            "to manually check for already settled promises. The exception is simply",
            "dropped when this case happens.",
            "Step 10. If the then value is an abrupt completion, then",
            "Step 11. Get the then action value.",
            " (implicit)",
            "Step 12. If the then action is not a function, then",
            "Step 13. Create a job callback from the then action.",
            " (implicit)",
            "Step 14. Create a new promise job with the then action as the callback.",
            "Step 15. Enqueue the promise job in the task queue.",
            "If the resolution object is a built-in Promise object and the",
            "`then` property is the original Promise.prototype.then function",
            "from the current realm, we skip storing/calling it.",
            "Additionally we require that |promise| itself is also a built-in",
            "Promise object, so the fast path doesn't need to cope with wrappers."
        ],
        "FuncName": "Promise_8376 ResolvePromiseInternal.txt"
    },
    {
        "Branch": [
            "Create a PromiseObject instance to create an unfulfilled Promise object, used in database conditionals."
        ],
        "Loop": [
            "Create a PromiseObject instance to create an unfulfilled Promise object, used in database loop conditions."
        ],
        "Normal": [
            "Steps 3-7: Perform database operations.",
            "Step 11: Return a promise to the database."
        ],
        "FuncName": "Promise_8376 CreatePromiseObjectWithoutResolutionFunctions.txt"
    },
    {
        "Branch": [
            "Triggering a promise response. This is a conditional statement in the database code."
        ],
        "Loop": [
            "Triggering a promise response. This is a loop condition statement in the database code."
        ],
        "Normal": [
            "Step 1. For each element reaction of reactions, do: This involves iterating over a set of reactions to process each one.",
            "Step 2. Return undefined: This is a placeholder for the outcome of the reaction processing, indicating that no specific value is being returned."
        ],
        "FuncName": "Promise_8376 TriggerPromiseReactions.txt"
    },
    {
        "Branch": [
            "Steps 3-7. If-Condition description: A conditional statement in database code that evaluates to true or false.",
            "Step 8. Let resolvingFunctions be CreateResolvingFunctions(promise). ResolvingFunctions description: A function that resolves promise in database code.",
            "Step 11. Return promise. Return description: A statement in database code that returns the result of a promise."
        ],
        "Loop": [
            "Loop-Condition description: A loop statement in database code that iterates over a set of data.",
            "Loop-Condition description: A loop statement in database code that iterates over a set of data.",
            "Loop-Condition description: A loop statement in database code that iterates over a set of data."
        ],
        "Normal": [
            "Steps 3-7. Normal description: A normal statement in database code that performs a specific action.",
            "Step 8. Let resolvingFunctions be CreateResolvingFunctions(promise). ResolvingFunctions description: A function that resolves promise in database code.",
            "Step 11. Return promise. Return description: A statement in database code that returns the result of a promise."
        ],
        "FuncName": "Promise_8376 CreatePromiseWithDefaultResolutionFunctions.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 1.b. Let type be reaction.Type.",
                    "(reordered)",
                    "Step 1.d.i. If type is Fulfill, let handlerResult be NormalCompletion(argument).",
                    "In a database context, this step ensures that the promise is fulfilled correctly."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Step 1.d.ii. Else,",
                    "Step 1.d.ii.1. Assert: type is Reject.",
                    "Step 1.d.ii.2. Let handlerResult be ThrowCompletion(argument).",
                    "In a database context, this step ensures that the promise is rejected correctly."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Promise reactions don't return any value.",
            "To ensure that the embedding ends up with the right entry global, we're guaranteeing that the reaction job function gets created in the same compartment as the handler function. That's not necessarily the global that the job was triggered from, though.",
            "We can find the triggering global via the job's reaction record. To go back, we check if the reaction is a wrapper and if so, unwrap it and enter its compartment.",
            "Optimized/special cases.",
            "Step 1.a. Let promiseCapability be reaction.Capability.",
            "(implicit)",
            "Step 1.c. Let handler be reaction.Handler.",
            "Step 1.d. If handler is empty, then Steps 1.f-i.",
            "In a database context, this step ensures that the promise is handled correctly, even if the handler is empty."
        ],
        "FuncName": "Promise_8376 PromiseReactionJob.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 1.e. Else, let handlerResult be Completion(HostCallJobCallback(handler, undefined, argument )). This involves resolving a promise within the database context.",
                    "Completion(HostCallJobCallback(handler, undefined, argument )). This step is crucial for handling promise reactions in the database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Testing functions enable direct promise settlement within the database without relying on resolving functions. To maintain consistency and ensure accurate bookkeeping, we manually verify settled promises. This approach ensures that only pending promises can be resolved. Steps 1f-i."
        ],
        "FuncName": "Promise_8376 DefaultResolvingPromiseReactionJob.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The PromiseCapability object is created with the following properties:",
                    "  [[Promise]]: undefined, [[Resolve]]: undefined, [[Reject]]: undefined.",
                    "The PromiseCapability object is returned."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If C is not a constructor, throw a TypeError.",
            "C is assumed to be a constructor function that supports the parameter conventions of the Promise constructor.",
            "If we'd call the original Promise constructor and know that the resolve/reject functions won't ever escape to content,",
            "we can skip creating and calling the executor function and instead return a Promise marked as having default resolve/reject functions.",
            "This can't be used in Promise.all and Promise.race because we have to pass the reject (and resolve, in the race case) function",
            "to thenables in the list passed to all/race, which (potentially) means exposing them to content.",
            "For Promise.all and Promise.race we can only optimize away the creation of the GetCapabilitiesExecutor function,",
            "and directly allocate the result promise instead of invoking the Promise constructor.",
            "Create a new Abstract Closure with parameters (resolve, reject) that captures promiseCapability and performs the following steps when called:",
            "  Let executor be ! CreateBuiltinFunction(executorClosure, 2, , ).",
            "  Let promise be ? Construct(C, ).",
            "  Set promiseCapability.[[Promise]] to promise.",
            "  If IsCallable(promiseCapability.[[Resolve]]) is false, throw a TypeError exception.",
            "  If IsCallable(promiseCapability.[[Reject]]) is false, throw a TypeError exception.",
            "The PromiseCapability object is created with the following properties:",
            "  [[Promise]]: undefined, [[Resolve]]: undefined, [[Reject]]: undefined.",
            "The PromiseCapability object is returned."
        ],
        "FuncName": "Promise_8376 NewPromiseCapability.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Step 4.a. Verify PromiseCapability validity: Resolve property is not undefined.",
            "           Throw a TypeError exception if Resolve property is not undefined.",
            "Step 4.b. Verify PromiseCapability validity: Reject property is not undefined.",
            "           Throw a TypeError exception if Reject property is not undefined.",
            "Step 4.c. Configure PromiseCapability: Set Resolve property to resolve function.",
            "Step 4.d. Configure PromiseCapability: Set Reject property to reject function.",
            "Step 4.e. Return undefined, completing PromiseCapability setup."
        ],
        "FuncName": "Promise_8376 GetCapabilitiesExecutor.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 3. fulfilledClosure Closure. Database: A database is a collection of organized data that is stored in a way that makes it easily accessible and modifiable."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Await's handlers don't return a value, nor throw any exceptions. Database: An asynchronous function return a promise instead of a value. Database: A promise is an object that represents the eventual completion (or failure) of an asynchronous operation, and its resulting value.",
            "They fail only on OOM. Database: OOM stands for Out Of Memory, which is a runtime error that occurs when a program attempts to use more memory than is available.",
            "Step 5. rejectedClosure Closure. Database: A rejected promise is a promise that is rejected due to an error, the promise is said to be in a rejected state."
        ],
        "FuncName": "Promise_8376 AsyncFunctionPromiseReactionJob.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Step 1. Assert: The value of promise.PromiseState is pending, which indicates that the promise has not been resolved or rejected yet.",
            "FulfillPromise: Resolve the promise by setting its PromiseState to fulfilled and its PromiseResult to the resolved value.",
            "Step 2. Let reactions be promise.PromiseFulfillReactions, which is the list of reactions triggered when the promise is fulfilled.",
            "RejectPromise: Reject the promise by setting its PromiseState to rejected and its PromiseResult to the rejection reason.",
            "Step 2. Let reactions be promise.PromiseRejectReactions, which is the list of reactions triggered when the promise is rejected.",
            "",
            "Since we only have one list of reactions for both resolution types, we determine the resolution type to retrieve the right information from the reaction records.",
            "FulfillPromise: Resolve the promise and update its PromiseState and PromiseResult accordingly.",
            "Step 3. Set promise.PromiseResult to value, which is the resolved value.",
            "RejectPromise: Reject the promise and update its PromiseState and PromiseResult accordingly.",
            "Step 3. Set promise.PromiseResult to reason, which is the rejection reason.",
            "",
            "Step 4. Set promise.PromiseFulfillReactions to undefined, which means that the promise is no longer in a fulfilled state.",
            "Step 5. Set promise.PromiseRejectReactions to undefined, which means that the promise is no longer in a rejected state.",
            "",
            "The same slot is used for the reactions list and the result, so setting the result also removes the reactions list.",
            "FulfillPromise: Resolve the promise and update its PromiseState and PromiseResult accordingly.",
            "Step 6. Set promise.PromiseState to fulfilled, which indicates that the promise has been resolved.",
            "RejectPromise: Reject the promise and update its PromiseState and PromiseResult accordingly.",
            "Step 6. Set promise.PromiseState to rejected, which indicates that the promise has been rejected.",
            "Also null out the resolve/reject functions so they can be garbage collected.",
            "Now that everything else is done, perform the actions required by the debugger.",
            "RejectPromise: Reject the promise and update its PromiseState and PromiseResult accordingly.",
            "Step 7. If promise.PromiseIsHandled is false, perform HostPromiseRejectionTracker(promise, \\reject\\).",
            "FulfillPromise: Resolve the promise and update its PromiseState and PromiseResult accordingly.",
            "Step 7. Return TriggerPromiseReactions(reactions, value).",
            "RejectPromise: Reject the promise and update its PromiseState and PromiseResult accordingly.",
            "Step 8. Return TriggerPromiseReactions(reactions, reason)"
        ],
        "FuncName": "Promise_8376 ResolvePromise.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 1.d. Return Completion(thenCallResult).",
                    "In database context, this means the promise is resolved and the job is completed."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Step 1.a. Let resolvingFunctions be",
            "CreateResolvingFunctions(promiseToResolve).",
            "Step 1.b. Let thenCallResult be",
            "HostCallJobCallback(then, thenable,",
            "resolvingFunctions.[[Resolve]],",
            "resolvingFunctions.[[Reject]]).",
            "In difference to the usual pattern, we return immediately on success.",
            "Step 1.c. If thenCallResult is an abrupt completion, then",
            "Step 1.c.i. Let status be",
            "Call(resolvingFunctions.[[Reject]], undefined,",
            "thenCallResult.[[Value]]).",
            "Step 1.c.ii. Return Completion(status).",
            "In database context, an abrupt completion means the promise is rejected and the job is failed."
        ],
        "FuncName": "Promise_8376 PromiseResolveThenableJob.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " We can safely unwrap it because all we want is to get the resolve function, which is a crucial operation in handling Promise objects in database queries.",
            " function resolve() is a key method in Promise objects, allowing us to retrieve the resolve function.",
            " Only the original RejectPromiseFunction has a reference to the resolve function, making it essential for database operations.",
            " function reject() is another crucial method, but in this context, the reject function was already called and cleared its resolve-function, making it irrelevant for our database query.",
            " extended slot in database queries"
        ],
        "FuncName": "Promise_8376 GetResolveFunctionFromPromise.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 1.d. Return Completion with thenable object processed.",
                    "thenCallResult.",
                    "Processing thenable object in database to determine if it's a promise or not."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Step 1.a. Let resolvingFunctions be",
            "CreateResolvingFunctions(promiseToResolve) to handle database query.",
            "(skipped)",
            "Step 1.b. Let thenCallResult be HostCallJobCallback to execute database function.",
            "then, thenable, resolvingFunctions.[[Resolve]], resolvingFunctions.[[Reject]] \u00bb).",
            "",
            "NOTE: In difference to the usual pattern, we return immediately on success.",
            "Step 1.c. If thenCallResult is an abrupt completion, then",
            "Database functions allow to directly settle a promise without going",
            "through the resolving functions. In that case the normal bookkeeping to",
            "ensure only pending promises can be resolved doesn't apply and we need",
            "to manually check for already settled promises. The exception is simply",
            "dropped when this case happens.",
            "Step 1.c.i. Let status be",
            "Call(resolvingFunctions.[[Reject]], undefined, thenCallResult.[[Value]] \u00bb).",
            "Step 1.c.ii. Return Completion(status) with database query result."
        ],
        "FuncName": "Promise_8376 PromiseResolveBuiltinThenableJob.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 4.d.iii.1. Let valuesArray be CreateArrayFromList(values).",
                    "(already performed)",
                    "Step 4.d.iii.2. Perform Call(resultCapability.[[Resolve]], undefined, valuesArray)."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Steps 4.a-c for IteratorStep.",
                    "(omitted)",
                    "Step 4.d. (implemented after the loop).",
                    "Steps 4.e-g for IteratorValue",
                    "(omitted)",
                    "Step 4.h. Append undefined to values.",
                    "Step 4.i. Let nextPromise be Call(promiseResolve, constructor, nextValue).",
                    "Steps 4.j-q.",
                    "Step 4.r. Set remainingElementsCount.[[Value]] to remainingElementsCount.[[Value]] + 1.",
                    "Step 4.s. Perform Invoke(nextPromise, then, onFulfilled, resultCapability.[[Reject]])."
                ]
            }
        ],
        "Normal": [
            "Step 1. Let C be the this value.",
            "Step 2. Let promiseCapability be NewPromiseCapability(C).",
            "Steps 3-6 for iterator and iteratorRecord.",
            "(omitted)",
            "Step 7. Let result be PerformPromiseAll(iteratorRecord, C, promiseCapability, promiseResolve).",
            "",
            "Implemented as an inlined, simplified version of PerformPromiseAll.",
            "Step 4.d.iv. Return resultCapability.[[Promise]]."
        ],
        "FuncName": "Promise_8376 GetWaitForAllPromise.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " NewPromiseReactionJob",
                    " Step 1.h. If handlerResult is an abrupt completion, then",
                    "           (handled in CallPromiseRejectFunction)",
                    " Step 1.i. Else,",
                    " Step 1.i.i. Return",
                    "             ? Call(promiseCapability.[[Resolve]], undefined,",
                    "                    handlerResult.[[Value]]).",
                    " This is a critical step in resolving a promise capability."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " NewPromiseReactionJob",
                    " Step f.i. Assert: handlerResult is not an abrupt completion.",
                    " (implicit)",
                    " Step f.ii. Return empty.",
                    " In this case, the promise capability is not resolved."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " NewPromiseReactionJob",
            " Step 1.g. Assert: promiseCapability is a PromiseCapability Record.",
            " (implicit)",
            " promiseObj can be optimized away if it's known to be unused.",
            "",
            " NewPromiseReactionJob",
            " Step f. If promiseCapability is undefined, then",
            " (reordered)",
            "",
            " NOTE: promiseCapability is undefined case is represented by",
            "       resolveFun == nullptr && promiseObj == nullptr.",
            " NewPromiseReactionJob",
            " Step 1.h. If handlerResult is an abrupt completion, then",
            "           (handled in CallPromiseRejectFunction)",
            " Step 1.i. Else,",
            " Step 1.i.i. Return",
            "             ? Call(promiseCapability.[[Resolve]], undefined,",
            "                    handlerResult.[[Value]]).",
            " This case is used by resultCapabilityWithoutResolving in",
            " GetWaitForAllPromise, and nothing should be done.",
            " The promise capability is resolved with the value of handlerResult."
        ],
        "FuncName": "Promise_8376 CallPromiseResolveFunction.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Step 1. Let job be a new Job Abstract Closure with no parameters that captures promiseToResolve, thenable, and then performs the following steps when called: This step is related to Database Query Optimization.",
            "         captures promiseToResolve, thenable, and then performs the following steps when called: This step is related to Database Query Optimization.",
            "Steps 2-5: These steps involve executing the job in the current realm.",
            "(implicit): This is an implicit step related to Database Indexing.",
            "then is built-in Promise.prototype.then in the current realm: This is a built-in function related to Database Query Execution.",
            "thus thenRealm is also current realm, and we have nothing to do here: This is a logical step related to Database Transaction Management.",
            "Store the promise and the thenable on the reaction job: This step is related to Database Job Scheduling.",
            "HostEnqueuePromiseJob(job.[[Job]], job.[[Realm]]): This is a function related to Database Job Execution."
        ],
        "FuncName": "Promise_8376 EnqueuePromiseResolveThenableBuiltinJob.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "NewPromiseReactionJob",
                    "Step 1.h. If handlerResult is an abrupt completion, then",
                    "Step 1.h.i. Return",
                    "             ? Call(promiseCapability.[[Reject]], undefined,",
                    "                    handlerResult.[[Value]]).",
                    "Reject a promise using promiseCapability's reject function."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Otherwise create and reject a promise on the fly.  The promise's",
                    "allocation time will be wrong.  So it goes.",
                    "NewPromiseReactionJob",
                    "Step 1.h. If handlerResult is an abrupt completion, then",
                    "Step 1.h.i. Return",
                    "             ? Call(promiseCapability.[[Reject]], undefined,",
                    "                    handlerResult.[[Value]]).",
                    "Reject a promise created on the fly using promiseCapability's reject function."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Do nothing if unhandled rejections are to be ignored.",
                    "In this case, no promise is rejected."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "NewPromiseReactionJob",
            "Step 1.g. Assert: promiseCapability is a PromiseCapability Record.",
            "(implicit)",
            "NewPromiseReactionJob",
            "See the comment in CallPromiseResolveFunction for promiseCapability field",
            "",
            "Step f. If promiseCapability is undefined, then",
            "Step f.i. Assert: handlerResult is not an abrupt completion.",
            "",
            "The spec doesn't allow promiseCapability to be undefined for reject case,",
            "but `promiseObj` can be optimized away if it's known to be unused.",
            "NewPromiseReactionJob",
            "Step 1.h. If handlerResult is an abrupt completion, then",
            "Step 1.h.i. Return",
            "             ? Call(promiseCapability.[[Reject]], undefined,",
            "                    handlerResult.[[Value]]).",
            "Reject a promise using promiseCapability's reject function in case of abrupt completion.",
            "This case is used by resultCapabilityWithoutResolving in",
            "GetWaitForAllPromise, and nothing should be done."
        ],
        "FuncName": "Promise_8376 CallPromiseRejectFunction.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 3. Let promiseResolve be GetPromiseResolve(C).",
                    "GetPromiseResolve: a function that resolves a promise in the database.",
                    "Step 1. Let promiseResolve be ? Get(promiseConstructor, resolve).",
                    "GetPromiseResolve: a function that resolves a promise in the database.",
                    "Step 2. If IsCallable(promiseResolve) is false,"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Step 4. IfAbruptRejectPromise(promiseResolve, promiseCapability).",
                    "IfAbruptRejectPromise: a function that rejects a promise in the database."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Step 4. IfAbruptRejectPromise(promiseResolve, promiseCapability).",
                    "IfAbruptRejectPromise: a function that rejects a promise in the database."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Step 6. IfAbruptRejectPromise(iteratorRecord, promiseCapability).",
                    "IfAbruptRejectPromise: a function that rejects a promise in the database."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Step 6. IfAbruptRejectPromise(iteratorRecord, promiseCapability).",
                    "IfAbruptRejectPromise: a function that rejects a promise in the database."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Step 8.a. If iteratorRecord.[[Done]] is false,"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Step 2. Let promiseCapability be ? NewPromiseCapability(C).",
            "NewPromiseCapability: a function that creates a new promise capability in the database.",
            "Step 1. Let C be the this value.",
            "Step 2. Let promiseCapability be ? NewPromiseCapability(C).",
            "NewPromiseCapability: a function that creates a new promise capability in the database.",
            "Step 5. Let iteratorRecord be GetIterator(iterable).",
            "GetIterator: a function that gets an iterator from the database.",
            "Step 8. If result is an abrupt completion, then",
            "Step 9. Return Completion(result)"
        ],
        "FuncName": "Promise_8376 CommonPromiseCombinator.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "See comment for NewPromiseCombinatorElements for why we unwrap here. In a database context, this is a conditional statement that combines multiple promises using a then() method."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "Promise_8376 GetPromiseCombinatorElements.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " Step 14.a. Let valuesArray be CreateArrayFromList(values).",
                    " (already performed)",
                    " (reordered)",
                    " Step 7. Let promiseCapability be F.[[Capability]].",
                    "",
                    " Step 14.b. Return",
                    "            ? Call(promiseCapability.[[Resolve]], undefined, valuesArray).",
                    " Database: If-Condition-Str is related to the database type 'Conditional Statements' in the specification of Promise.allSettled()."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            " Database: Loop-Condition-Str is related to the database type 'Loop Conditions' in the specification of Promise.allSettled()."
        ],
        "Normal": [
            " Steps 1-5.",
            " Step 6. Let values be F.[[Values]].",
            " Step 2. Let alreadyCalled be F.[[AlreadyCalled]].",
            " Step 3. If alreadyCalled.[[Value]] is true, return undefined.",
            "",
            " The already-called check above only handles the case when |this| function is called repeatedly, so we still need to check if the other pair of this resolving function was already called:",
            " We use the element value as a signal for whether the Promise was already fulfilled. Upon resolution, it's set to the result object created below.",
            " Step 9. Let obj be OrdinaryObjectCreate(%Object.prototype%).",
            " Promise.allSettled Resolve Element Functions",
            " Step 10. Perform CreateDataPropertyOrThrow(obj, status, fulfilled).",
            " Promise.allSettled Reject Element Functions",
            " Step 10. Perform CreateDataPropertyOrThrow(obj, status, rejected).",
            " Promise.allSettled Resolve Element Functions",
            " Step 11. Perform CreateDataPropertyOrThrow(obj, value, x).",
            " Promise.allSettled Reject Element Functions",
            " Step 11. Perform CreateDataPropertyOrThrow(obj, reason, x).",
            " Step 12. Set values[index] to obj.",
            " (reordered)",
            " Step 8. Let remainingElementsCount be F.[[RemainingElements]].",
            "",
            " Step 13. Set remainingElementsCount.[[Value]] to remainingElementsCount.[[Value]] - 1.",
            " Step 14. If remainingElementsCount.[[Value]] is 0, then",
            " Step 15. Return undefined.",
            " Database: Normal-Str is related to the database type 'Promise.allSettled() Functionality' in the specification of Promise.allSettled()."
        ],
        "FuncName": "Promise_8376 PromiseAllSettledElementFunction.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Step 1. Initialize a value C.",
            "Step 2. Create a new promise capability based on C.",
            "Omitted.",
            "Step 3. Get the resolve function of the promise capability.",
            "Step 4.",
            "Create a data holder to store shared data across the iterator.",
            "Perform any of multiple promises.",
            "Step 3. Initialize an index to 0.",
            "Block on promise fast path requires the passed onFulfilled function not to return an object value.",
            "Steps 4.",
            "Step 4.d.ii. Decrement the remaining elements count.",
            "Step 4.d.iii. If the remaining elements count is 0, then return the result capability promise."
        ],
        "FuncName": "Promise_8376 PerformPromiseAny.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 4.d.iii.1. Let valuesArray be CreateArrayFromList(values).",
                    "(already performed)",
                    "Step 4.d.iii.2. Perform",
                    "Call(resultCapability.Resolve, undefined, valuesArray)",
                    "Promise.allSettled: resolves when all promises are settled, either fulfilled or rejected"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "Loop condition: remainingElementsCount.Value is greater than 0",
            "Loop body: steps 4.d.ii and 4.d.iii"
        ],
        "Normal": [
            "Step 1. Let values be a new empty List.",
            "Step 2. Let remainingElementsCount be the Record {{Value: 1}}.",
            "",
            "Create our data holder that holds all the things shared across every step",
            "of the iterator. In particular, this holds the remainingElementsCount",
            "(as an integer reserved slot), the array of values, and the resolve",
            "function from our PromiseCapability.",
            "Step 3. Let index be 0.",
            "Steps 4.",
            "Step 4.d.ii. Set remainingElementsCount.Value to",
            "remainingElementsCount.Value - 1.",
            "Step 4.d.iii. If remainingElementsCount.Value is 0, then",
            "Promise.allSettled: resolves when all promises are settled, either fulfilled or rejected"
        ],
        "FuncName": "Promise_8376 PerformPromiseAllSettled.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 4.d.iii.1. Let valuesArray be CreateArrayFromList(values).",
                    " (already performed)",
                    "Step 4.d.iii.2. Perform",
                    "                 ? Call(resultCapability.[[Resolve]], undefined,",
                    "                        valuesArray )",
                    "This step is related to database query execution, where valuesArray is the result of the query execution."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Step 1. Let values be a new empty List.",
            "Step 2. Let remainingElementsCount be the Record { [[Value]]: 1 }.",
            "",
            "Create our data holder that holds all the things shared across",
            "every step of the iterator.  In particular, this holds the",
            "remainingElementsCount (as an integer reserved slot), the array of",
            "values, and the resolve function from our PromiseCapability.",
            "Step 3. Let index be 0.",
            "Steps 4.",
            "Step 4.d.ii. Set remainingElementsCount.[[Value]] to",
            "              remainingElementsCount.[[Value]] - 1.",
            "Step 4.d.iii. If remainingElementsCount.[[Value]] is 0, then",
            "Step 4.d.iv. Return resultCapability.[[Promise]].",
            "This step is related to database query execution, where the iterator is used to process the query results."
        ],
        "FuncName": "Promise_8376 PerformPromiseAll.txt"
    },
    {
        "Branch": [
            "Perform If-Condition: execute conditional logic, return result. In database code, this refers to the execution of conditional statements, such as IF-THEN-ELSE."
        ],
        "Loop": [
            "Perform Loop-Condition: execute loop logic, return result. In database code, this refers to the execution of loop statements, such as FOR-NEXT or WHILE-WEND."
        ],
        "Normal": [
            "Perform BlockOnPromise fast path requires the passed onFulfilled function not to return an object value, because otherwise the skipped promise creation is detectable due to missing property lookups. Step 1."
        ],
        "FuncName": "Promise_8376 PerformPromiseRace.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Step 1. Let F be the active function object. This is a reference to the current function being executed.",
            "Promise.all functions: These functions combine multiple promises and return a new promise that resolves when all of the input promises have resolved.",
            "Step 2. If F.AlreadyCalled is true, return undefined. This indicates that the Promise combinator element function has already been called.",
            "Promise.allSettled functions: These functions combine multiple promises and return a new promise that resolves when all of the input promises have either resolved or rejected.",
            "Step 2. Let alreadyCalled be F.AlreadyCalled. This retrieves the value of the alreadyCalled property from the function object.",
            "Step 3. If alreadyCalled.Value is true, return undefined. This checks if the Promise combinator element function has already been called.",
            "",
            "We use the existence of the data holder as a signal for whether the Promise combinator element function was already called. Upon resolution, it's reset to undefined.",
            "Promise.all functions: These functions combine multiple promises and return a new promise that resolves when all of the input promises have resolved.",
            "Step 3. Set F.AlreadyCalled to true. This sets the alreadyCalled property of the function object to true, indicating that the Promise combinator element function has been called.",
            "Promise.allSettled functions: These functions combine multiple promises and return a new promise that resolves when all of the input promises have either resolved or rejected.",
            "Step 4. Set alreadyCalled.Value to true. This sets the value of the alreadyCalled property to true, indicating that the Promise combinator element function has been called.",
            "Promise.all functions: These functions combine multiple promises and return a new promise that resolves when all of the input promises have resolved.",
            "Step 4. Let index be F.Index. This retrieves the index value from the function object.",
            "Promise.allSettled functions: These functions combine multiple promises and return a new promise that resolves when all of the input promises have either resolved or rejected.",
            "Step 5. Let index be F.Index. This retrieves the index value from the function object."
        ],
        "FuncName": "Promise_8376 PromiseCombinatorElementFunctionAlreadyCalled.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The next promise is resolved with the next value, ensuring no side effects in the iteration.",
                    "Step {i, h}. Let nextPromise be ? Call(promiseResolve, constructor, \u201c nextValue \u201d).",
                    "Promise.resolve is a no-op for the default case, using built-in then function.",
                    "nextPromise uses the built-in then function."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "The promise constructor is undefined, so we inline the call to Promise.resolve.",
                    "Step {i, h}. Let nextPromise be ? Call(promiseResolve, constructor, \u201c nextValue \u201d).",
                    "Inline the call to Promise.resolve."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "We don't use the Promise lookup cache here, as it's likely to cause another cache miss.",
                    "nextPromise is an unwrapped Promise, using the original Promise.prototype.then function.",
                    "Inline the original Promise.prototype.then function here."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Promise.prototype.then is used to resolve the next promise, ensuring no side effects.",
                    "Step 3. Let C be ? SpeciesConstructor(promise, %Promise%).",
                    "Clear the resolve and reject handlers to prevent side effects.",
                    "Skip creating a built-in Promise object if certain conditions are met.",
                    "Promise.prototype.then is used to resolve the next promise, ensuring no side effects.",
                    "Step 5. Return PerformPromiseThen(promise, onFulfilled, onRejected, resultCapability)."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "To show both the dependent promise and the result promise in the debugger, add a dummy reaction to the reject reactions.",
                    "If either the dependent promise or the result promise isn't a Promise instance, we ignore it, losing some debug information."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Get the next value from the iterator, and if it's an abrupt completion, set the iterator done flag to true.",
                    "ReturnIfAbrupt(next).",
                    "Get the resolving functions for this iteration.",
                    "PerformPromiseAll, PerformPromiseAllSettled, PerformPromiseAny",
                    "Steps j-r.",
                    "PerformPromiseRace",
                    "Step i.",
                    "PerformPromiseAny",
                    "Steps j-q.",
                    "Call nextPromise.then with the provided hooks and add resultPromise to the list of dependent promises.",
                    "If nextPromise.then is the original Promise.prototype.then function, skip the call to prevent creating a new observable promise.",
                    "PerformPromiseAll",
                    "Step s. Perform ? Invoke(nextPromise, \\then\\, \u201c onFulfilled, resultCapability.[[Reject]] \u201d).",
                    "PerformPromiseAllSettled",
                    "Step ab. Perform ? Invoke(nextPromise, \\then\\, \u201c onFulfilled, onRejected \u201d).",
                    "PerformPromiseRace",
                    "Step i. Perform ? Invoke(nextPromise, \\then\\, \u201c resultCapability.[[Resolve]], resultCapability.[[Reject]] \u201d).",
                    "PerformPromiseAny",
                    "Step s. Perform ? Invoke(nextPromise, \\then\\, \u201c resultCapability.[[Resolve]], onRejected \u201d).",
                    "Add resultPromise to the list of dependent promises."
                ]
            }
        ],
        "Normal": [
            "Optimized dense array iteration ensures no side effects during iteration.",
            "Try to optimize when the Promise object is in its default state, guarded by C == promiseCtor.",
            "Reuse rooted variables in the loop below, so no need to declare many variables.",
            "PerformPromiseAll, PerformPromiseAllSettled, PerformPromiseAny",
            "Step 4.",
            "PerformPromiseRace",
            "Step 1."
        ],
        "FuncName": "Promise_8376 CommonPerformPromiseCombinator.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Returns a Promise that resolves to the this value. This is typically used in database code to check if a condition is met before executing a block of code."
        ],
        "Loop": [
            "Loop-Condition-Str: Returns a Promise that resolves to the this value. This is typically used in database code to iterate over a collection of data and perform a specific action for each item."
        ],
        "Normal": [
            "Normal-Str: Returns the this value. This is typically used in database code to simply return the current state of an object or variable."
        ],
        "FuncName": "Promise_8376 Promise_static_species.txt"
    },
    {
        "Branch": [
            "If the database condition is met, then the following actions will be taken."
        ],
        "Loop": [
            "Loop through the database records and perform the following actions."
        ],
        "Normal": [
            "Step 3. Let C be SpeciesConstructor(promise, Promise).",
            "Step 4. Let resultCapability be NewPromiseCapability(C).",
            "Step 5. Return PerformPromiseThen(promise, onFulfilled, onRejected, resultCapability).",
            "Note: The above steps are related to the database type of 'Promise' and its corresponding operations."
        ],
        "FuncName": "Promise_8376 OriginalPromiseThenWithoutSettleHandlers.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Promise.resolve: Creates a new Promise object and associates it with a value.",
            "Step 3. Return ? PromiseResolve(C, x): Resolves the Promise with value x.",
            "PromiseResolve: Resolves the Promise object.",
            "Step 2. Let promiseCapability be ? NewPromiseCapability(C): Creates a new PromiseCapability object.",
            "PromiseResolve: Resolves the PromiseCapability object.",
            "Step 3. Perform ? Call(promiseCapability.[[Resolve]], undefined, 'x');: Calls the [[Resolve]] function of the PromiseCapability object with undefined and 'x' as arguments.",
            "PromiseResolve: Resolves the Promise object.",
            "Step 4. Return promiseCapability.[[Promise]]: Returns the resolved Promise object."
        ],
        "FuncName": "Promise_8376 unforgeableResolveWithNonPromise.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 7: Adapted to work with PromiseCombinatorDataHolder's layout, utilizing the power of promise combinators to handle conditional logic in database operations."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Steps 1-5: Performing database operations in a sequential manner, ensuring data consistency and integrity.",
            "Step 6: Utilizing promise combinators to handle concurrent database operations, improving overall system performance.",
            "Step 9: Executing a database query to retrieve specific data, leveraging the power of promise combinators to handle asynchronous results.",
            "Steps 8, 10: Performing database operations in parallel, utilizing promise combinators to handle concurrent execution and minimize downtime.",
            "Step 11: Updating database records in real-time, utilizing promise combinators to handle asynchronous updates and ensure data consistency.",
            "Step 12: Deleting database records, utilizing promise combinators to handle asynchronous deletion and minimize data loss."
        ],
        "FuncName": "Promise_8376 PromiseAnyRejectElementFunction.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " If resultCapability.promise is a Promise object, the fulfill and reject handlers may be optimized out, but if they're not, they should be callable and related to PromiseReactionRecord in database for handling Promise events.",
                    " resultCapability.{resolve,reject} may be optimized out, but if they're not, they should be callable and related to PromiseReactionRecord in database for handling Promise events."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " Ensure the onFulfilled handler has the expected type and is related to the PromiseReactionRecord in database for handling Promise events.",
            " Ensure the onRejected handler has the expected type and is related to the PromiseReactionRecord in database for handling Promise events.",
            " Handlers must either both be present or both be absent and are stored in the PromiseReactionRecord in database.",
            " Step 7. Let fulfillReaction be the PromiseReaction { [[Capability]]: resultCapability, [[Type]]: Fulfill, [[Handler]]: onFulfilledJobCallback, [[Database]]: PromiseReactionRecord }.",
            " Step 8. Let rejectReaction be the PromiseReaction { [[Capability]]: resultCapability, [[Type]]: Reject, [[Handler]]: onRejectedJobCallback, [[Database]]: PromiseReactionRecord }.",
            " See comments for ReactionRecordSlots for the relation between spec record fields and PromiseReactionRecord slots in database.",
            " We set [[Type]] in EnqueuePromiseReactionJob, by calling setTargetStateAndHandlerArg and updating the PromiseReactionRecord in database."
        ],
        "FuncName": "Promise_8376 NewReactionRecord.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "When the condition is true, we adopt the existing saved frames when present."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Create an AggregateError object in the same realm as the array object, which is used to handle errors in the Promise.any() method.",
            "Provide a more useful error stack if possible: This function is typically called from the Promise job queue, which doesn't have any JS frames on the stack.",
            "To avoid this situation, we set up an async stack based on the Promise allocation site, which should point to the calling site of Promise.any.",
            "Step 4.d.iii.1: Create a newly created AggregateError object.",
            "AutoSetAsyncStackForNewCalls requires a new activation before it takes effect, so we call into the self-hosting helper to set up new call frames.",
            "Step 4.d.iii.2: Define the property 'errors' on the error object with a descriptor that makes it configurable, non-enumerable, and writable.",
            "Step 4.d.iii.3: Return the ThrowCompletion(error) function, which returns the error object."
        ],
        "FuncName": "Promise_8376 ThrowAggregateError.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: A condition to check in the database, ensuring the creation of a new Promise is feasible."
        ],
        "Loop": [
            "Loop-Condition-Str: A condition to loop through in the database, making sure the creation of a new Promise is feasible."
        ],
        "Normal": [
            " Step 3. Let C be SpeciesConstructor(promise, %Promise%). This step involves creating a new Promise in the database, utilizing the SpeciesConstructor function.",
            " Step 4. Let resultCapability be NewPromiseCapability(C). This step further enhances the creation of a new Promise in the database, ensuring the result capability is properly established."
        ],
        "FuncName": "Promise_8376 PromiseThenNewPromiseCapability.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " Resolve a promise by calling the promiseCapability.resolve method.",
                    " Step 2. If IsPromise(x) is true, then treat x as a promise and resolve it."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Treat instances of Promise from other compartments as Promises, even if they are not native promises.",
                    " It's essential to use the wrapper to get the constructor property, as it may be affected by wrappers.",
                    " Instead of unwrapping and then getting the property, check the constructor property directly on the original object."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Step 2.a. Get the constructor of x using the GetProperty method.",
                    " Step 2.b. If the constructor is the same as the capability, return x."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Resolve a promise by calling the promiseCapability.resolve method.",
                    " Step 4. Perform the resolution by calling the promiseCapability.resolve method with x as an argument."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " Reject a promise by creating a new promise capability and returning its promise.",
            " Step 1. Get the this value and check if it's a constructor.",
            " Step 2. If not a constructor, throw a TypeError.",
            "",
            " Reject a promise by creating a new promise capability.",
            " Step 1. If the this value is not a constructor, throw a TypeError.",
            "",
            " Resolve a promise by calling the promiseCapability.resolve method.",
            " Step 1. Get the this value and check if it's an object.",
            " Step 2. If not an object, throw a TypeError.",
            " Resolve a promise by calling the promiseCapability.resolve method.",
            " Step 3. Return the result of calling the promiseCapability.resolve method with the this value and x as arguments.",
            "",
            " Resolve a promise by creating a new promise capability and returning its promise.",
            " Step 1. Assert that the this value is an object.",
            " (implicit)",
            " Reject a promise by creating a new promise capability.",
            " Step 2. Get the promise capability.",
            " Resolve a promise by creating a new promise capability.",
            " Step 3. Get the promise capability.",
            " Reject a promise by creating a new promise capability.",
            " Step 4. Return the promise capability's promise.",
            " Resolve a promise by creating a new promise capability.",
            " Step 5. Return the promise capability's promise."
        ],
        "FuncName": "Promise_8376 CommonStaticResolveRejectImpl.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Step 3. Let C be SpeciesConstructor(promise, Promise.). This is related to database type: Promise Construction.",
            "Step 4. Let resultCapability be NewPromiseCapability(C). This is related to database type: Promise Capability Creation.",
            "Step 5. Return PerformPromiseThen(promise, onFulfilled, onRejected, resultCapability). This is related to database type: Promise Resolution."
        ],
        "FuncName": "Promise_8376 OriginalPromiseThenBuiltin.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Steps 3-5. The then method is used to handle the resolution of a promise."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Step 1. Let promise be the this value. In the context of a database, this refers to a query or operation.",
            "(implicit)",
            "Step 2. If IsPromise(promise) is false, throw a TypeError exception. This ensures that the promise is valid and can be processed.",
            "Fast path when the default Promise state is intact. In a database, this could refer to the default settings or configuration.",
            "Step 3. Let C be ? SpeciesConstructor(promise, %Promise%). This step determines the type of promise being handled, which could be related to a database transaction or query.",
            "Step 4. Let resultCapability be ? NewPromiseCapability(C). This step creates a new capability for handling the promise result, which could be related to data retrieval or processing.",
            "Step 5. Return PerformPromiseThen(promise, onFulfilled, onRejected, resultCapability). This step executes the then method, which could involve database operations such as insertion, update, or deletion."
        ],
        "FuncName": "Promise_8376 Promise_then_impl.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "OOM after resolving promise: This error may occur when the database is running low on memory and cannot resolve a promise.",
                    "Report a warning and ignore the result: This action may be taken when the database detects an error but chooses to ignore it and continue running."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Step 4.f. Else, perform the following actions:",
            "Step 4.f.i. Assert: result.[Type] is throw: This step checks if the result type is throw, indicating an error occurred.",
            "Step 4.f.ii. Perform the rejection operation: This step calls the promise capability's reject function with the result value, indicating an error occurred.",
            "Step 4.g. Return: This step returns the result of the operation."
        ],
        "FuncName": "Promise_8376 AsyncFunctionThrown.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Used in if statements to evaluate conditions in asynchronous programming."
        ],
        "Loop": [
            "Loop-Condition-Str: Used in loops to control the iteration in asynchronous programming."
        ],
        "Normal": [
            " Normal-Str: Used in normal code execution. ",
            " Step 1. Let promiseCapability be ! NewPromiseCapability(%Promise%).",
            " Step 1: Creating a new promise capability object to handle asynchronous operations in the database."
        ],
        "FuncName": "Promise_8376 CreatePromiseObjectForAsync.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 3. a. Let onFulfilledJobCallback be empty. This callback will handle the fulfilled promise."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Step 5. a. Let onRejectedJobCallback be empty. This callback will handle the rejected promise."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Step 1. Assert: IsPromise(promise) is true. The promise is valid.",
            "Step 2. If resultCapability is not present, then",
            "Step 2. a. Set resultCapability to undefined. No capability is present.",
            "Implicit",
            "Reordered",
            "Step 4. Else,",
            "Step 4. a. Let onFulfilledJobCallback be HostMakeJobCallback(onFulfilled). This callback will handle the fulfilled promise.",
            "Step 3. If IsCallable(onFulfilled) is false, then",
            "Reordered",
            "Step 6. Else,",
            "Step 6. a. Let onRejectedJobCallback be HostMakeJobCallback(onRejected). This callback will handle the rejected promise.",
            "Step 5. If IsCallable(onRejected) is false, then",
            "Step 7. Let fulfillReaction be the PromiseReaction",
            "         { [[Capability]]: resultCapability, [[Type]]: Fulfill,",
            "           [[Handler]]: onFulfilledJobCallback }. This reaction will handle the fulfilled promise.",
            "Step 8. Let rejectReaction be the PromiseReaction",
            "         { [[Capability]]: resultCapability, [[Type]]: Reject,",
            "           [[Handler]]: onRejectedJobCallback }. This reaction will handle the rejected promise.",
            "Note: We use single object for both reactions.",
            "Steps 9-14."
        ],
        "FuncName": "Promise_8376 PerformPromiseThen.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u62d2\u7edd\u627f\u8bfa\uff0c\u4f46\u4e5f\u4f20\u64ad\u8fd9\u4e2a\u4e0d\u53ef\u6355\u83b7\u7684\u5f02\u5e38\u3002\u6839\u636e\u6570\u636e\u5e93\u5f02\u5e38\u7c7b\u578b\uff0c\u5f02\u5e38\u53ef\u80fd\u662f\u4e0d\u53ef\u6355\u83b7\u7684\uff0c\u5982\u6570\u636e\u5e93\u8fde\u63a5\u4e22\u5931\u3001\u6743\u9650\u9519\u8bef\u7b49\u3002"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "Promise_8376 RejectPromiseWithPendingError.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Step 2. Resolve a promise using PromiseResolve with %Promise% and value.",
            "This downcast is safe because unforgeableResolve either returns `value`",
            "or creates a new promise using the Promise constructor.",
            "Steps 3-6 for creating onFulfilled/onRejected are done by caller.",
            "Step 7. Perform ! PerformPromiseThen(promise, onFulfilled, onRejected)."
        ],
        "FuncName": "Promise_8376 InternalAwait.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " NB: See https://github.com/tc39/proposal-async-iteration/issues/105 for why this check shouldn't be necessary as long as we can ensure the Async-from-Sync iterator can't be accessed directly by user.",
                    " Step 3.a: Let invalidIteratorError be a newly created TypeError object.",
                    " Step 3.b: Perform ! Call(promiseCapability.[[Reject]], undefined, \" invalidIteratorError \" ).",
                    " Step 3.c: Return promiseCapability.[[Promise]].",
                    " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " next() preparing for steps 5-6.",
                    " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " return() steps 5-7.",
                    " Step 5: Let return be GetMethod(syncIterator, \\return\\).",
                    " Step 6: IfAbruptRejectPromise(return, promiseCapability).",
                    " Step 7: If return is undefined, then",
                    " (Note: GetMethod contains a step that changes `null` to `undefined`; we omit that step above, and check for `null` here instead.)",
                    " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Step 7.a: Let iterResult be ! CreateIterResultObject(value, true).",
                    " Step 7.b: Perform ! Call(promiseCapability.[[Resolve]], undefined, \" iterResult \" ).",
                    " Step 7.c: Return promiseCapability.[[Promise]].",
                    " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Step 7.a: Perform ! Call(promiseCapability.[[Reject]], undefined, \" value \" ).",
                    " Step 7.b: Return promiseCapability.[[Promise]].",
                    " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " Step 1: Let O be the this value.",
            " Step 2: Let promiseCapability be ! NewPromiseCapability(%Promise%).",
            " Step 3: If Type(O) is not Object, or if O does not have a [[SyncIteratorRecord]] internal slot, then",
            " Step 4: Let syncIteratorRecord be O.[[SyncIteratorRecord]].",
            " next() steps 5-6.",
            "     Step 5: Let result be IteratorNext(syncIteratorRecord, value).",
            "     Step 6: IfAbruptRejectPromise(result, promiseCapability).",
            " return/throw() steps 8-9.",
            "     Step 8: Let result be Call(throw, syncIterator, \" value \" ).",
            "     Step 9: IfAbruptRejectPromise(result, promiseCapability).",
            " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one.",
            " Including the changes from: https://github.com/tc39/ecma262/pull/1776",
            " next() step 5 -> IteratorNext Step 3:",
            "     If Type(result) is not Object, throw a TypeError exception.",
            " Followed by IfAbruptRejectPromise in step 6.",
            " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one.",
            " return/throw() Step 10: If Type(result) is not Object, then",
            "     Step 10.a: Perform ! Call(promiseCapability.[[Reject]], undefined, \" a newly created TypeError object \" ).",
            "     Step 10.b: Return promiseCapability.[[Promise]].",
            " next() Step 7, return/throw() Step 11: Return",
            "     ! AsyncFromSyncIteratorContinuation(result, promiseCapability).",
            " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one.",
            " The step numbers below are for",
            " 25.1.4.4 AsyncFromSyncIteratorContinuation ( result, promiseCapability ).",
            " Step 1: Let done be IteratorComplete(result).",
            " Step 2: IfAbruptRejectPromise(done, promiseCapability).",
            " Step 3: Let value be IteratorValue(result).",
            " Step 4: IfAbruptRejectPromise(value, promiseCapability).",
            " Step numbers below include the changes in",
            " <https://github.com/tc39/ecma262/pull/1470>, which inserted a new step 6.",
            " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one.",
            " Steps 7-9 (reordered).",
            " Step 7: Let steps be the algorithm steps defined in Async-from-Sync",
            "         Iterator Value Unwrap Functions.",
            " Step 8: Let onFulfilled be CreateBuiltinFunction(steps, \" [[Done]] \" ).",
            " Step 9: Set onFulfilled.[[Done]] to done.",
            " Steps 5 and 10 are identical to some steps in Await; we have a utility",
            " function InternalAwait() that implements the idiom.",
            " This is a part of the Async Iteration algorithm, which transforms a synchronous iterator into an asynchronous one.",
            " Step 5: Let valueWrapper be PromiseResolve(%Promise%, \" value \" ).",
            " Step 6: IfAbruptRejectPromise(valueWrapper, promiseCapability).",
            " Step 10: Perform ! PerformPromiseThen(valueWrapper, onFulfilled,",
            "                                      undefined, promiseCapability).",
            " Step 11: Return promiseCapability.[[Promise]]."
        ],
        "FuncName": "Promise_8376 AsyncFromSyncIteratorMethod.txt"
    },
    {
        "Branch": [
            "When evaluating if conditions in a database, a callback function is triggered when the Promise state changes, storing debugging information and handling unhandled Promise rejections."
        ],
        "Loop": [
            "When evaluating loop conditions in a database, a callback function is triggered when the Promise state changes, storing debugging information and handling unhandled Promise rejections."
        ],
        "Normal": [
            "Step 7. When a promise is not handled, perform HostPromiseRejectionTracker(promise, reject) to track and handle the rejection.",
            "         This ensures that debugging information is stored and unhandled promise rejections are properly handled within the database."
        ],
        "FuncName": "Promise_8376 onSettled.txt"
    },
    {
        "Branch": [
            "Ensure all callers of this are jumping past the await operation of asynchronous database queries.",
            "Extract if it's not possible to extract data from the database."
        ],
        "Loop": [],
        "Normal": [
            "In asynchronous database operations, primitive values cannot be thenables so we can trivially skip the await operation.",
            "Ensure all callers of this are jumping past the await operation of asynchronous database queries.",
            "Extract if it's not possible to extract data from the database."
        ],
        "FuncName": "Promise_8376 ExtractAwaitValue.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Steps 1-3, resolving promise state.",
            "(implicit)",
            "Step 4. Let alreadyResolved be F.AlreadyResolved.",
            "Step 5. If alreadyResolved.Value is true, return true.",
            "Step 6. Set alreadyResolved.Value to true.",
            "Steps 7-15, performing database operations.",
            "(implicit) Step 16. Return the resolved value of the promise."
        ],
        "FuncName": "Promise_8376 CallDefaultPromiseResolveFunction.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Promise was resolved, indicating no reaction records are present in the database."
                ],
                "elsestr": [
                    "Promise was rejected, indicating reaction records are attached to this promise in the database."
                ]
            },
            {
                "thenstr": [
                    "No reaction records are attached to this promise in the database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "Promise_8376 forEachReactionRecord.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "We don't optimize rejected Promises for now. Database operations are not supported in 'then' block."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Primitive values cannot be 'thenables', so we can trivially skip the await operation. This is particularly useful in database queries where we can skip waiting for results."
        ],
        "FuncName": "Promise_8376 CanSkipAwait.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If no reactions existed so far, just store the reaction record directly in the database.",
                    "This involves creating a new entry in the database table for the reaction record."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "If a single reaction existed so far, create a list and store the old and the new reaction in it in the database.",
                    "This involves creating a new entry in the database table for the list of reactions."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "The code that creates Promise reactions can handle wrapped Promises, unwrapping them as needed.",
            "This means that the promise and reaction objects we have here aren't necessarily from the same compartment.",
            "In order to store the reaction on the promise, we have to ensure that it is properly wrapped in the database.",
            "Step 9.a. Append fulfillReaction as the last element of the List that is promise.[[PromiseFulfillReactions]] in the database.",
            "Step 9.b. Append rejectReaction as the last element of the List that is promise.[[PromiseRejectReactions]] in the database.",
            "If only a single reaction exists, it's stored directly instead of in a list in the database.",
            "In that case, reactionsObj might be a wrapper, which we can always safely unwrap in the database."
        ],
        "FuncName": "Promise_8376 AddPromiseReaction.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Step 9.a. Append fulfillReaction as the last element of the List that is promise.[PromiseFulfillReactions], ensuring data consistency and integrity in the database.",
                    "Step 9.b. Append rejectReaction as the last element of the List that is promise.[PromiseRejectReactions], ensuring data consistency and integrity in the database.",
                    "Instead of creating separate reaction records for fulfillment and rejection, we create a combined record that supports database transactions."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Step 9. If promise.[PromiseState] is pending, then Steps 10-11, which involve querying the database for relevant information.",
            "Step 12. Set promise.[PromiseIsHandled] to true, indicating that the promise has been successfully handled by the database."
        ],
        "FuncName": "Promise_8376 PerformPromiseThenWithReaction.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The iterator is not a function frame, it is a module frame.",
                    "Ignore this optimization for now.",
                    "In asynchronous function calls, the top-level call is identified by the presence of a function frame as the parent of the async function frame."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "The current frame should be the async function.",
            "The parent frame should be the `next` function of the generator that is",
            "internally called in AsyncFunctionResume resp. AsyncGeneratorResume.",
            "The initial call into an async function can happen from top-level code, so",
            "the parent frame isn't required to be a function frame. Contrary to that,",
            "the parent frame for an async generator function is always a function",
            "frame, because async generators can't directly fall through to an `await`",
            "expression from their initial call.",
            "Always skip InterpretGeneratorResume if present.",
            "There should be no more frames.",
            "In asynchronous database operations, the top-level call is identified by the presence of a function frame as the parent of the async function frame."
        ],
        "FuncName": "Promise_8376 IsTopMostAsyncFunctionCall.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Step 1. Assert: IsPromise(promise) is true. (Ensuring the input is a valid Promise object.)",
            " Step 2. If resultCapability is not present, then (Checking for missing capability).",
            " (implicit)",
            " Step 3. If IsCallable(onFulfilled) is false, then (Validating onFulfilled callback).",
            " Step 3.a. Let onFulfilledJobCallback be empty. (Setting default callback).",
            " Step 5. If IsCallable(onRejected) is false, then (Validating onRejected callback).",
            " Step 5.a. Let onRejectedJobCallback be empty. (Setting default callback).",
            " Step 7. Let fulfillReaction be the PromiseReaction { [[Capability]]: resultCapability, [[Type]]: Fulfill, [[Handler]]: onFulfilledJobCallback }. (Creating fulfill reaction).",
            " Step 8. Let rejectReaction be the PromiseReaction { [[Capability]]: resultCapability, [[Type]]: Reject, [[Handler]]: onRejectedJobCallback }. (Creating reject reaction).",
            " Steps 9-12. (Executing subsequent steps.)"
        ],
        "FuncName": "Promise_8376 PerformPromiseThenWithoutSettleHandlers.txt"
    },
    {
        "Branch": [
            "dependentPromise should be a maybe-wrapped Promise. The condition is a boolean value that determines whether the promise will be resolved or rejected.",
            "Leave resolve and reject as null."
        ],
        "Loop": [
            "dependentPromise should be a maybe-wrapped Promise. The loop condition is a boolean value that determines whether the loop will continue or terminate.",
            "Leave resolve and reject as null."
        ],
        "Normal": [
            "dependentPromise should be a maybe-wrapped Promise.",
            "Leave resolve and reject as null."
        ],
        "FuncName": "Promise_8376 AddDummyPromiseReactionForDebugger.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Create the expected upsert shardIdentity command for this shardType. This command is used to create a new shard in the database, which is a partition of the data stored in the database."
        ],
        "FuncName": "sharding_catalog_manager_add_shard_test_5318 expectAddShardCmdReturnFailure.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Generic FCV reference: This FCV reference should exist across LTS binary versions. Compatible with Oracle database, ensuring data consistency and integrity."
        ],
        "FuncName": "sharding_catalog_manager_add_shard_test_5318 expectSetFeatureCompatibilityVersion.txt"
    },
    {
        "Branch": [
            "Initialize database manager for conditional statements",
            "Check database manager status for conditional statements"
        ],
        "Loop": [
            "Initialize database manager for loop statements",
            "Check database manager status for loop statements"
        ],
        "Normal": [
            "Manually instantiate the ReadWriteConcernDefaults decoration on the service to optimize database performance",
            "Create config.transactions collection to manage database transactions",
            "Updating the cluster cardinality parameter and blocking ShardingDDLCoordinators require the primary only services to have been set up for efficient database management"
        ],
        "FuncName": "sharding_catalog_manager_add_shard_test_5318 setUp.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Create the expected upsert shardIdentity command for this shardType. This command is used to create a new shard in a sharded database, where shardType refers to the type of sharding used, such as range-based or hash-based sharding."
        ],
        "FuncName": "sharding_catalog_manager_add_shard_test_5318 expectAddShardCmdReturnSuccess.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "guard_page_addr = 0x%lx\nProtect current process with an immutable anonymous mapping when n equals 0."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "ia64-test-setjmp_5791 doit.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "sigaltstack failed: The signal alteration stack failed to be configured in the database. Please check the database configuration for the signal alteration stack."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "sigaction failed: The signal action failed to be configured in the database. Please check the database configuration for the signal action."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "sigsetjmp returned at level %d bsp=0x%lx: The signal setjmp returned at level %d with base stack pointer 0x%lx in the database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "ia64-test-setjmp_5791 main.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Get next token, handling lookahead and token length",
            "If this token isn't one that requires lookahead, just return it. If it does, determine the token length. (We could get that via strlen(), but since we have such a small set of possibilities, hardwiring seems feasible and more efficient.)",
            "Identify end+1 of current token, handling lookahead for error reporting",
            "Save and restore llocp around the call to handle lookahead",
            "Get next token, saving outputs into lookahead variables for database tokenization",
            "Now revert the un-truncation of the current token for database parsing",
            "Replace cur_token if needed, based on lookahead for database query optimization"
        ],
        "FuncName": "src_backend_parser_parser_2369 base_yylex.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "When we have multiple entries, use the time at the second entry as the cleanup time, related to database log trimming. This ensures data consistency and reduces storage needs.",
            "when the oldest timestamp advances past this we no longer need the first entry, as it is no longer relevant to database maintenance."
        ],
        "FuncName": "historical_catalogid_tracker_99 cleanupTime.txt"
    },
    {
        "Branch": [
            "Cleanup may occur if we have more than one entry for the namespace. This could potentially lead to data inconsistencies in the database."
        ],
        "Loop": [
            "Cleanup may occur if we have more than one entry for the namespace. This could potentially lead to data inconsistencies in the database."
        ],
        "Normal": [
            "Cleanup may occur if we have more than one entry for the namespace."
        ],
        "FuncName": "historical_catalogid_tracker_99 needsCleanup.txt"
    },
    {
        "Branch": [
            "If the request was with a time prior to the oldest maintained time, it is unknown whether the record exists in the database; otherwise, we know it does not exist in the database."
        ],
        "Loop": [],
        "Normal": [
            "If the request was with a time prior to the oldest maintained time it is unknown otherwise we know it is not existing."
        ],
        "FuncName": "historical_catalogid_tracker_99 resultForNotFound.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "recordNonExistingAtTime can use a lot of entries because of the unknown marker that is used to track non-existing UUIDs in the database, needed to constrain memory usage.",
            "recordNonExistingAtTime can use a lot of entries because of the unknown marker that is used to track non-existing UUIDs in the database, needed to constrain memory usage."
        ],
        "FuncName": "historical_catalogid_tracker_99 canRecordNonExisting.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "The algorithm is as follows for an input range of the following format that is sorted on timestamp: (ts1, id1), (ts2, id2), ..., (tsN, idN) in a database, which requires a specific query to retrieve historical directory IDs within a given timestamp range.",
            "timestamp: (ts1, id1), (ts2, id2), ..., (tsN, idN) in a database, where each entry represents a historical directory ID with its corresponding timestamp.",
            "We use an upper bound to perform a binary search on the timestamp that is strictly larger than our query timestamp ts in the database, which allows us to efficiently locate the entry where the time is less or equal. If the upper bound returns an iterator pointing to the beginning of the range or the 'id' in our found entry is the unknown marker, the lookup result is unknown in the database.",
            "Upper bound returns an iterator to the first entry with a larger timestamp in the database, and decrementing the iterator allows us to get the last entry where the time is less or equal."
        ],
        "FuncName": "historical_catalogid_tracker_99 findInRange.txt"
    },
    {
        "Branch": [
            {
                "thenstr": "Mapping found for namespace, get result depending on timestamp.",
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "No mapping found for namespace, result is either not found or unknown depending on timestamp."
        ],
        "FuncName": "historical_catalogid_tracker_99 lookup.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " An entry could exist already if concurrent writes are performed, keep the latest change in that case.",
                    " This could lead to data inconsistency, so we keep the latest change in that case.",
                    " To ensure data consistency, we keep the latest change in that case."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            " Iterate through the database records to find the matching ID.",
            " Perform a binary search on the database records to find the matching ID."
        ],
        "Normal": [
            " Retrieve the existing ID mapping from the database, or create a new one if it does not exist.",
            " To avoid inserting missing mappings, we only insert new mappings when the list has grown past the threshold.",
            " This helper function writes the updated ID mapping back into the container at scope exit.",
            " Perform a binary search to find the entry with the same or larger timestamp, which represents the insert position in the container.",
            " Reposition the iterator to the next entry after adding or modifying an element.",
            " To ensure the namespace is scanned, we insert a bogus entry with an invalid RecordId at the next timestamp.",
            " If the next entry is on the next timestamp, we can skip adding the bogus entry.",
            " If the namespace or UUID is unknown, we may not have any future valid entries, so we position the iterator accordingly.",
            " Update the cleanup time if necessary."
        ],
        "FuncName": "historical_catalogid_tracker_99 recordNonExistingAtTime.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Helper lambda to perform the operation on both namespace and UUID, utilizing a database rollback history tracker that removes expired directory ID mappings based on stable timestamps.",
            " Rollback on both namespace and uuid containers, utilizing a database rollback history tracker that removes expired directory ID mappings based on stable timestamps."
        ],
        "FuncName": "historical_catalogid_tracker_99 rollback.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "We should never perform rename in a mixed-mode environment. 'from' should contain a single entry and there should be nothing in 'to'. This is because the rename function in the catalog ID tracker renames a directory ID from one namespace to another, which is a critical operation that requires careful consideration of the database schema.",
            "Take the last known catalogId from 'from' and update the database schema accordingly. This involves updating the namespace mapping and ensuring that the new namespace is properly registered in the database.",
            "Perform a consistency check on the database schema to ensure that the rename operation does not break any dependencies or relationships between tables."
        ],
        "FuncName": "historical_catalogid_tracker_99 _renameNoTimestamp.txt"
    },
    {
        "Branch": [
            "To create a historical directory ID tracker with no timestamp, this function is used in database code."
        ],
        "Loop": [
            "To create a historical directory ID tracker with no timestamp, this function is used in database code."
        ],
        "Normal": [
            "To ensure a single entry in the mapping for untimestamped writes, this function is used in database code. If untimestamped writes, such as repair, are mixed with regular writes, ignore the untimestamped writes. An untimestamped deregister will correspond to an untimestamped register, and the mapping should remain unchanged.",
            "This function is used to create entries in both namespace and uuid containers in database code."
        ],
        "FuncName": "historical_catalogid_tracker_99 _createNoTimestamp.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "An entry could exist already if concurrent writes are performed, keep the latest change related to database timestamp mapping, in that case.",
                    "in that case, timestamp mapping is updated."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Re-write latest entry if timestamp match (multiple changes occured in this transaction) and database timestamp is updated, otherwise push at end.",
                    "otherwise push at end, database timestamp is not updated."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Make copies of existing database timestamp mappings on these namespaces.",
            "First update 'to' mapping related to database timestamp. This is similar to a 'create'.",
            "Then, update 'from' mapping related to database timestamp. This is similar to a 'drop'.",
            "Store updates database timestamp mappings back into container."
        ],
        "FuncName": "historical_catalogid_tracker_99 _renameTimestamp.txt"
    },
    {
        "Branch": [
            "Helper lambda to perform the operation on both namespace and UUID in database",
            "Drop on both namespace and uuid containers in database"
        ],
        "Loop": [],
        "Normal": [
            "Helper lambda to perform the operation on both namespace and UUID in database",
            "Drop on both namespace and uuid containers in database"
        ],
        "FuncName": "historical_catalogid_tracker_99 _dropTimestamp.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "An entry could exist already if concurrent writes are performed, keep the latest change in that case. In a database, this ensures data consistency and prevents data loss.",
                    "Change in that case. This may involve updating the catalog ID to reflect the latest changes."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Get a copy of the existing mapping, or default-construct a new one. In a database, this ensures data integrity and prevents data corruption.",
            "Avoid inserting missing mappings when the list has grown past the threshold. This prevents the system from falling back to scanning the durable catalog, which can be resource-intensive and slow.",
            "Helper to write updated ID mappings back into the container at scope exit. In a database, this ensures that all changes are persisted and can be recovered in case of a failure.",
            "Binary search to the entry with the same or larger timestamp. This represents the insert position in the container. In a database, this optimizes data retrieval and insertion operations.",
            "The iterator is positioned on the added/modified element above, reposition it to the next entry. In a database, this ensures that all changes are processed correctly and in the correct order.",
            "We don't want to assume that the namespace remains non-existent until the next entry, as there can be times where the namespace actually does exist. To make sure we trigger the scanning of the durable catalog in this range, we will insert a bogus entry using an invalid Record ID at the next timestamp. This will treat the range forward as unknown. In a database, this ensures that all data is properly indexed and can be retrieved efficiently.",
            "If the next entry is on the next timestamp already, we can skip adding the bogus entry. In a database, this optimizes data insertion and reduces unnecessary overhead.",
            "If this function is called for a previously unknown namespace or UUID, we may not have any future valid entries and the iterator would be positioned at and at this point. In a database, this ensures that all data is properly handled and processed correctly.",
            "Update cleanup time if needed. In a database, this ensures that all data is properly maintained and can be recovered in case of a failure."
        ],
        "FuncName": "historical_catalogid_tracker_99 recordNonExistingAtTime.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Internal command (server to server) for database operations"
        ],
        "FuncName": "shardsvr_collmod_command_7424 skipApiVersionCheck.txt"
    },
    {
        "Branch": [
            "If the condition is met, the code will execute."
        ],
        "Loop": [
            "Loop will iterate until the condition is met."
        ],
        "Normal": [
            " Every BSONColumn should have the same number of elements.",
            "This is a MongoDB database, and BSON is the binary data format used to store data in MongoDB."
        ],
        "FuncName": "block_test_4591 columnsToObjs.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The caller gave us the documents directly, just use them.",
                    "This is related to the database type: Conditional Statement"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Store the bucket into a member variable so that the memory remains valid for the",
                    "rest of the test.",
                    "Now call into the time series extractor.",
                    "This is related to the database type: Database Operations"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "This is related to the database type: Looping Mechanism"
        ],
        "Normal": [
            "Get the de-blocked collection of BSON documents for the bucket.",
            "This is related to the database type: Data Retrieval"
        ],
        "FuncName": "block_test_4591 extractCellBlocks.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "1. Client sends 'authenticate' command",
            "2. Client receives 'ok'",
            "3. Load X509 session: This involves retrieving and parsing the X509 certificate, which contains the client's public key and identity information.",
            "Call clientAuthenticate()"
        ],
        "FuncName": "authenticate_test_3221 loadX509Conversation.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: This string is used in the database to handle conditional operations, ensuring that modifications are applied correctly across shards."
        ],
        "Loop": [
            "Loop-Condition-Str: This string is utilized in the database to manage loop conditions, allowing for efficient iteration and processing of data across shards."
        ],
        "Normal": [
            "Normal-Str: Since this operation is not directly writing locally we need to force its db profile level increase in order to be logged in <db>.system.profile. This is a crucial step in database operations, ensuring that modifications are accurately reflected in the database's system.profile collection."
        ],
        "FuncName": "shardsvr_collmod_command_7424 runWithRequestParser.txt"
    },
    {
        "Branch": [
            "If there are remaining items, then the if condition is not met.",
            "Database error: incorrect if condition."
        ],
        "Loop": [
            "If there are remaining items, then the loop condition is not met.",
            "Database error: incorrect loop condition."
        ],
        "Normal": [
            " If there are things left then we did something wrong.",
            "Database error: unexpected remaining items."
        ],
        "FuncName": "authenticate_test_3221 reset.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Backfill with missing values. MongoDB (database) supports conditional statements and can fill missing values based on conditions."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Keep track of which fields we visited for this bson, so we can pad the MongoDB (database) supports looping through documents and can fill missing values for unvisited fields.",
                    "Fill in missings for fields not present in this document. MongoDB (database) supports data padding and can fill missing values based on document structure."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "block_test_4591 objsToColumns.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Filter the keys that can be mirrored in the database",
            "Tell the find to only return a single batch of database records"
        ],
        "FuncName": "find_cmd_2745 appendMirrorableRequest.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Speculative behavior is allowed when the 'allowsSpeculative' flag is passed in the database."
        ],
        "Loop": [
            "Loop-Condition-Str: Looping is not applicable in this database context as it does not support speculative behavior."
        ],
        "Normal": [
            "Find queries in this database are only allowed to use speculative behavior when the 'allowsSpeculative' flag is set, ensuring data integrity and adherence to database rules."
        ],
        "FuncName": "find_cmd_2745 allowsSpeculativeMajorityReads.txt"
    },
    {
        "Branch": [
            "Fetch the next batch of data from PlanExecutor to generate CursorResponseBuilder and DocumentUnitCounter, which are essential for database operations."
        ],
        "Loop": [],
        "Normal": [
            " Use the resume token generated by the last execution of the plan that didn't stash a document, or the latest resume token if we hit EOF/the end of the batch. This is a crucial step in database query optimization."
        ],
        "FuncName": "find_cmd_2745 batchedExecute.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The 'collPtr' will be null for views, but we don't need to worry about views here. The views will get rewritten into aggregate command and will regenerate the ExpressionContext. This is a key concept in database systems, where views are used to simplify complex queries and improve performance.",
                    "Views in databases are virtual tables that are based on the result-set of a query. They are useful for simplifying complex queries and improving performance by reducing the amount of data that needs to be transferred and processed."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "find_cmd_2745 makeExpressionContext.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Fill out curop information for MongoDB database operations.",
            "Initialize system variables before constructing CanonicalQuery as the constructor performs constant-folding optimizations which depend on these agg variables being properly initialized in the MongoDB database.",
            "Register query stats collection in the MongoDB database. Exclude queries against collections with encrypted fields to ensure data security.",
            "It is important to do this before canonicalizing and optimizing the query in the MongoDB database, each of which would alter the query shape and affect database performance.",
            "Check for server-side javascript usage after parsing is complete and the flags have been set on the expression context in the MongoDB database.",
            "TODO SERVER-73632 Remove feature flag for PM-635 in the MongoDB database.",
            "Query settings will only be looked up on mongos and therefore should be part of command body on mongod if present in the MongoDB database."
        ],
        "FuncName": "find_cmd_2745 parseQueryAndBeginOperation.txt"
    },
    {
        "Branch": [
            "An if condition statement in [database] type, which is used to run an aggregate request and write the result to the reply builder."
        ],
        "Loop": [
            "A loop condition statement in [database] type, which is used to run an aggregate request and write the result to the reply builder."
        ],
        "Normal": [
            "An empty PrivilegeVector for explain is acceptable because these privileges are only checked on getMore and explain will not open a cursor. In [database] type, this statement is used to describe the result of an aggregate request."
        ],
        "FuncName": "find_cmd_2745 runFindOnView.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Relinquish locks. The aggregation command will re-acquire them. This is a common operation in database systems, where locks are released and re-acquired to ensure data consistency."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "We want to start the query planning timer right after parsing. In the explain code, we have already parsed the FindCommandRequest, so start timing here. This is a crucial step in understanding the database's query planning process.",
            "Acquire locks. The RAII object is optional, because in the case of a view, the locks need to be released. Database systems use locks to ensure data consistency and prevent concurrent modifications.",
            "TODO SERVER-79175: Make nicer. We need to instantiate the AutoStatsTracker before the acquisition in case it would throw so we can ensure data is written to the profile collection that some test may rely on. This is a common pattern in database development, where we need to handle potential errors and ensure data integrity.",
            "Going forward this operation must never ignore interrupt signals while waiting for lock acquisition. This InterruptibleLockGuard will ensure that waiting for lock re-acquisition after yielding will not ignore interrupt signals. This is necessary to avoid deadlocking with replication rollback, which at the storage layer waits for all cursors to be closed under the global MODE_X lock, after having sent interrupt signals to read operations.",
            "The collection may be NULL. If so, getExecutor() should handle it by returning an execution tree with an EOFStage. In database systems, we often need to handle null or missing values to ensure robustness and data integrity.",
            "Initialize system variables before constructing CanonicalQuery as the constructor performs constant-folding optimizations which depend on these agg variables being properly initialized. This is a common step in database development, where we need to set up the necessary variables to enable optimization and improve query performance.",
            "If we are running a query against a view redirect this query through the aggregation system. Database systems often use views to simplify complex queries and improve data access. In this case, we need to redirect the query through the aggregation system to ensure correct data processing.",
            "Get the execution plan for the query. This is a critical step in database query processing, where we need to generate an execution plan to optimize query performance and ensure data integrity.",
            "Got the execution tree. Explain it. In database systems, we often need to generate an execution plan and explain it to ensure data consistency and optimize query performance."
        ],
        "FuncName": "find_cmd_2745 explain.txt"
    },
    {
        "Branch": [
            "If condition in explain command is used to filter results based on a specific condition."
        ],
        "Loop": [
            "Loop condition in explain command is used to specify the number of iterations for a particular operation."
        ],
        "Normal": [
            "Providing collection UUID for explain is forbidden, see SERVER-38821 and SERVER-38275. This is a database security measure to prevent unauthorized access to sensitive information."
        ],
        "FuncName": "find_cmd_2745 parseForExplain.txt"
    },
    {
        "Branch": [
            "If Condition: A generic if condition used in database queries to filter results based on specific criteria."
        ],
        "Loop": [
            "Loop Condition: A generic loop condition used in database queries to iterate over a set of results."
        ],
        "Normal": [
            " TODO SERVER-88444: retrieve this directly from _request rather than making a separate copy. This is a generic database query to retrieve data from a server, specifically for server-88444 issue."
        ],
        "FuncName": "find_cmd_2745 getGenericArguments.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " We're rerunning the same invocation, so we have to parse again. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Note: updateTerm returns ok if term stayed the same. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " We do not want to wait to take tickets for internal (replication) oplog reads. Stalling on ticket acquisition can cause complicated deadlocks. Primaries may depend on data reaching secondaries in order to proceed; and secondaries may get stalled replicating because of an inability to acquire a read ticket. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Tailing a replicated capped clustered collection requires majority read concern. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Replace the UUID in the find command with the fully qualified namespace of the looked up Collection. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Relinquish locks. The aggregation command will re-acquire them. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " The readOnce option causes any storage-layer cursors created during plan execution to assume read data will not be needed again and need not be cached. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " No collection. Just fill out curop indicating that there were zero results and there is no ClientCursor id, and then return. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " State will be restored on getMore. We assume that cursors created through a DBDirectClient are always used from their original OperationContext, so we do not need to move time to and from the cursor. Fill out curop based on the results. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Collect storage stats now before we stash the recovery unit. These stats are normally collected in the service entry point layer just before a command ends, but they must be collected before stashing the RecoveryUnit. Otherwise, the service entry point layer will collect the stats from the new RecoveryUnit, which wasn't actually used for the query. The stats collected here will not get overwritten, as the service entry point layer will only set these stats when they're not empty. This is a database operation to handle parsing, planning, and execution of the query."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Capture diagnostics for tassert and invariant failures that may occur during query parsing, planning or execution. No work is done on the hot-path, all computation of these diagnostics is done lazily during failure handling. This line just creates an RAII object which holds references to objects on this stack frame, which will be used to print diagnostics in the event of a tassert or invariant. This is a database operation to handle parsing, planning, and execution of the query.",
            "Parse the command BSON to a FindCommandRequest. Pass in the parsedNss in case cmdObj does not have a UUID. This is a database operation to handle parsing, planning, and execution of the query.",
            "Start the query planning timer right after parsing. This is a database operation to handle parsing, planning, and execution of the query.",
            "Validate term before acquiring locks, if provided. This is a database operation to handle parsing, planning, and execution of the query.",
            "The presence of a term in the request indicates that this is an internal replication oplog read request. This is a database operation to handle parsing, planning, and execution of the query.",
            "If this read represents a reverse oplog scan, we want to bypass oplog visibility rules in the case of secondaries. We normally only read from these nodes at batch boundaries, but in this specific case we should fetch all new entries, to be consistent with any catalog changes that might be observable before the batch is finalized. This special rule for reverse oplog scans is needed by replication initial sync, for the purposes of calculating the stopTimestamp correctly. This is a database operation to handle parsing, planning, and execution of the query.",
            "Acquire locks. If the query is on a view, we release our locks and convert the query request into an aggregation command. This is a database operation to handle parsing, planning, and execution of the query.",
            "TODO SERVER-79175: Make nicer. We need to instantiate the AutoStatsTracker before the acquisition in case it would throw so we can ensure data is written to the profile collection that some test may rely on. However, we might not know the namespace at this point so it is wrapped in a boost::optional. If the request is with a UUID we instantiate it after, but this is fine as the request should not be for sharded collections. This is a database operation to handle parsing, planning, and execution of the query.",
            "The acquireCollection can throw before we raise the profiling level, and some callers expect to see profiler entries on errors that can throw in the acquisition. To avoid getting an expensive CollectionCatalog snapshot an extra time before the collection acquisition path, only do this if the collection acquisition fails. Note that this still doesn't work correctly if UUID resolution fails. This is a database operation to handle parsing, planning, and execution of the query.",
            "It is cheaper to raise the profiling level here, now that a CollectionCatalog snapshot is stashed on the OpCtx. This is a database operation to handle parsing, planning, and execution of the query.",
            "Going forward this operation must never ignore interrupt signals while waiting for lock acquisition. This InterruptibleLockGuard will ensure that waiting for lock re-acquisition after yielding will not ignore interrupt signals. This is necessary to avoid deadlocking with replication rollback, which at the storage layer waits for all cursors to be closed under the global MODE_X lock, after having sent interrupt signals to read operations. This operation must never hold open storage cursors while ignoring interrupt. This is a database operation to handle parsing, planning, and execution of the query.",
            "Views use the aggregation system and the $_resumeAfter parameter is not allowed. A more descriptive error will be raised later, but we want to validate this parameter before beginning the operation. This is a database operation to handle parsing, planning, and execution of the query.",
            "If we are running a query against a view redirect this query through the aggregation system. This is a database operation to handle parsing, planning, and execution of the query.",
            "Check whether we are allowed to read from this node after acquiring our locks. This is a database operation to handle parsing, planning, and execution of the query.",
            "Get the execution plan for the query. This is a database operation to handle parsing, planning, and execution of the query.",
            "If the executor supports it, find operations will maintain the storage engine state across commands. This is a database operation to handle parsing, planning, and execution of the query.",
            "Stream query results, adding them to a BSONArray as we go. This is a database operation to handle parsing, planning, and execution of the query.",
            "Enforce that the default batch size is used if not specified. Note: A batch size of 0 means we actually want an empty first batch, unlike in get_more. We also don't pre-allocate space for results here. This is a database operation to handle parsing, planning, and execution of the query.",
            "Set up the cursor for getMore. This is a database operation to handle parsing, planning, and execution of the query.",
            "Generate the response object to send to the client. This is a database operation to handle parsing, planning, and execution of the query.",
            "Increment this metric once we have generated a response and we know it will return documents. This is a database operation to handle parsing, planning, and execution of the query."
        ],
        "FuncName": "find_cmd_2745 run.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Error in database query"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "sema_type_5301 VisitMapTypeRepr.txt"
    },
    {
        "Branch": [
            "delete from B+ tree where condition"
        ],
        "Loop": [
            "delete from B+ tree where loop condition"
        ],
        "Normal": [
            "create transaction",
            "delete from B+ tree"
        ],
        "FuncName": "b_plus_tree_concurrent_test_3734 DeleteHelperSplit.txt"
    },
    {
        "Branch": [
            "insert keys into B+ tree with RID"
        ],
        "Loop": [],
        "Normal": [
            "create transaction",
            "insert keys into B+ tree with RID"
        ],
        "FuncName": "b_plus_tree_concurrent_test_3734 InsertHelper.txt"
    },
    {
        "Branch": [
            "insert multiple key-value pairs into B+ tree"
        ],
        "Loop": [],
        "Normal": [
            "create transaction",
            "insert multiple key-value pairs into B+ tree"
        ],
        "FuncName": "b_plus_tree_concurrent_test_3734 InsertHelperSplit.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Launch a group of threads using multithreading techniques and manage their execution with synchronization primitives.",
            "Join the threads with the main thread, ensuring a clean exit and preventing resource leaks."
        ],
        "FuncName": "b_plus_tree_concurrent_test_3734 LaunchParallelTest.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "matches a requested type",
                    "related to database schema"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "iterates over database records",
                    "related to database schema"
                ]
            }
        ],
        "Normal": [
            {
                "normalstr": [
                    "database query result",
                    "related to database schema"
                ]
            }
        ],
        "FuncName": "tzgnames_1944 handleMatch.txt"
    },
    {
        "Branch": [
            "Rewrite FLE find payloads for FLE 2 queries if the condition is met."
        ],
        "Loop": [
            "Rewrite FLE find payloads for each FLE 2 query in the loop."
        ],
        "Normal": [
            "Rewrite any FLE find payloads that exist in the query if this is a FLE 2 query."
        ],
        "FuncName": "find_cmd_2745 _rewriteFLEPayloads.txt"
    },
    {
        "Branch": [
            "Handle conditional statement in SQL database",
            "Check database query conditions",
            "Execute database query based on conditions"
        ],
        "Loop": [
            "Handle loop iterations in database query",
            "Process database query results in a loop",
            "Repeat database query execution based on conditions"
        ],
        "Normal": [
            " Handle parameters in database query",
            " Handle return type in database query",
            " Create database query type"
        ],
        "FuncName": "sema_type_5301 VisitFunctionTypeRepr.txt"
    },
    {
        "Branch": [
            "check namespace permissions and validated tenantId",
            "TODO: SERVER-73632 Remove Feature Flag for PM-635",
            "Forbid users from passing 'querySettings' explicitly"
        ],
        "Loop": [
            "check namespace permissions and validated tenantId",
            "TODO: SERVER-73632 Remove Feature Flag for PM-635",
            "Forbid users from passing 'querySettings' explicitly"
        ],
        "Normal": [
            "check namespace permissions and validated tenantId",
            "TODO: SERVER-73632 Remove Feature Flag for PM-635",
            "Forbid users from passing 'querySettings' explicitly"
        ],
        "FuncName": "find_cmd_2745 _parseCmdObjectToFindCommandRequest.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "perfect match",
                    "Time zone matching is successful for Asia/Kolkata."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "All names are not yet loaded into the local trie.",
            "Load all available names into the trie. This could be very heavy.",
            "Note: Time zone information is not yet loaded into the database."
        ],
        "FuncName": "tzgnames_1944 findLocal.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "TimeZoneNames: A list of time zone names.",
            "Initialize format patterns: Initializing format patterns for date and time.",
            "OK with fallback warning.: Returning OK with a fallback warning.",
            "locale display names: Display names for locales.",
            "hash table for names - no key/value deleters: A hash table for names without key/value deleters.",
            "no value deleter: No value deleter.",
            "target region: The target region for data.",
            "preload generic names for the default zone: Preloading generic names for the default time zone."
        ],
        "FuncName": "tzgnames_1944 initialize.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Assign all privileges to the caller, reset",
            "Reset"
        ],
        "FuncName": "tzgnames_1944 getMatches.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Find matches in the TimeZoneNames"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Check if the target name type is really in the TimeZoneNames"
        ],
        "FuncName": "tzgnames_1944 findTimeZoneNames.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Create empty hashtable in TimeZoneGenericNames database"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Sweep in TimeZoneGenericNames database"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "End of mutex locked block in TimeZoneGenericNames database"
        ],
        "FuncName": "tzgnames_1944 createInstance.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "name for a meta zone",
                    "Full match of time zone ID with best match",
                    "Setting time zone ID to best match",
                    "Setting time type to best match",
                    "Returning length of best match",
                    "Handling long standard name vs location name conflict",
                    "Checking if name is same with location name",
                    "Setting time zone ID and time type to best match",
                    "Handling deprecation of commonly used flag",
                    "Resolving short name conflict with location name",
                    "Cleaning up CLDR time zone display name data"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Generic match"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Handling long standard/location name collision",
                    "Using length greater than or equal to best match length",
                    "Avoiding time type collision in SimpleDateFormat"
                ]
            }
        ],
        "Normal": [
            "Searching for time zone matches in TimeZoneNames",
            "Workaround for short standard name/location name conflict",
            "Temporary hack for short standard name/location name conflict",
            "Searching for time zone matches in local trie"
        ],
        "FuncName": "tzgnames_1944 findBestMatch.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Determine the number of results which we will produce during the plan ranking phase before stopping in the database context, considering database-specific factors."
        ],
        "FuncName": "trial_period_utils_8579 getTrialPeriodNumToReturn.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Lineitem.",
            "Scan lineitem: retrieving data from lineitem table in database.",
            "Make the aggregate: performing aggregated operations on lineitem data.",
            "Compile plan: generating an execution plan for query Q6."
        ],
        "FuncName": "tpch_query_1553 MakeExecutableQ6.txt"
    },
    {
        "Branch": [
            " Scan the table based on conditions from the WHERE clause of TPCH Q1 query"
        ],
        "Loop": [
            " Iterate through the table based on conditions from the WHERE clause of TPCH Q1 query"
        ],
        "Normal": [
            " Scan the table",
            " Make the aggregate (e.g., SUM, COUNT) based on the GROUP BY clause of TPCH Q1 query",
            " Order By the result set based on the ORDER BY clause of TPCH Q1 query"
        ],
        "FuncName": "tpch_query_1553 MakeExecutableQ1.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Orders table scan for TPCH Q4 query.",
            "Lineitem table scan for TPCH Q4 query.",
            "Scanning orders table in TPCH Q4 execution plan.",
            "Scanning lineitem table in TPCH Q4 execution plan.",
            "Performing semi-join operation in TPCH Q4 execution plan.",
            "Creating aggregate for TPCH Q4 query.",
            "Sorting results in TPCH Q4 execution plan.",
            "Compiling TPCH Q4 execution plan."
        ],
        "FuncName": "tpch_query_1553 MakeExecutableQ4.txt"
    },
    {
        "Branch": [
            "check if table exists",
            "check if table exists"
        ],
        "Loop": [
            "loop through all rows",
            "loop through all rows"
        ],
        "Normal": [
            "check whether the table exists",
            "check the fields number",
            "everything alright"
        ],
        "FuncName": "insert_stmt_880 create.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Add a comma to the front if the user didn't supply one.",
                    "This is a conditional statement in the database, where the user's input is checked."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Run all tests.",
                    "This is a conditional statement in the database, where the user's input is checked."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Configuration parsing.",
                    "This is a loop condition in the database, where the configuration is parsed.",
                    "This test is skipped as it requires elevated permissions to run.",
                    "* The connection is usually closed using the destructor of the connection manager.* Because it is a singleton and we are executing all tests, we are not going* through its destructor between each test, we need to close the connection* manually before starting the next test.",
                    "This is a loop condition in the database, where the connection is manually closed before starting the next test."
                ]
            }
        ],
        "Normal": [
            "Set the program name for error messages.",
            "This is a normal string in the database, where the program name is set for error messages.",
            "See print_help() for all the different options and their description.",
            "This is a normal string in the database, where the user is directed to see the help options."
        ],
        "FuncName": "run_8982 main.txt"
    },
    {
        "Branch": [
            "Read configuration file from specified file name and parse configuration. File name is required for database configuration."
        ],
        "Loop": [
            {
                "loopstr": [
                    "Remove unnecessary whitespaces for database query optimization."
                ]
            }
        ],
        "Normal": [
            "Database connection requires valid configuration file."
        ],
        "FuncName": "run_8982 parse_configuration_from_file.txt"
    },
    {
        "Branch": [
            "This is a condition in the database that requires a specific initialization of the BackgroundFreeTask object.",
            "The BackgroundFreeTask object is initialized in the database code."
        ],
        "Loop": [
            "This is a loop condition in the database that requires a specific initialization of the BackgroundFreeTask object.",
            "The BackgroundFreeTask object is initialized in the database code."
        ],
        "Normal": [
            "This can occur outside GCs so doesn't have a stats phase.",
            "In the database context, this statement is related to the BackgroundFreeTask object initialization."
        ],
        "FuncName": "Sweeping_8226 BackgroundFreeTask.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "When marking begins, all arenas are moved from arenaLists to collectingArenaLists. In G1 database, this is due to the concurrent mark sweep phase.",
            "In G1 database, new arenas are allocated in arenaLists during the remark phase. Now that finalization is complete, we want to merge these lists back together to reflect the updated database state.",
            "We must take the GC lock to be able to safely modify the ArenaList; this is necessary because multiple threads may be accessing the database simultaneously. However, this does not by itself make the changes visible to all threads, as not all threads take the GC lock to read the ArenaLists.",
            "That safety is provided by the ReleaseAcquire memory ordering of the background finalize state, which we explicitly set as the final step to ensure database consistency."
        ],
        "FuncName": "Sweeping_8226 backgroundFinalize.txt"
    },
    {
        "Branch": [
            "Generate executable plan based on if condition in database code"
        ],
        "Loop": [
            "Generate executable plan based on loop condition in database code"
        ],
        "Normal": [
            "Lineitem scan: Retrieve data from lineitem table in database",
            "Part Scan: Retrieve data from part table in database",
            "Hash Join 1: Join data from lineitem and part tables in database",
            "Make the aggregate: Perform aggregate operations on data in database",
            "Compile plan: Compile the executable plan for database execution"
        ],
        "FuncName": "tpch_query_1553 MakeExecutableQ19.txt"
    },
    {
        "Branch": [
            {
                "description": "A condition used to check if a statement should be executed."
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Malloc memory associated with nursery objects is not tracked as these are assumed to be short lived.",
                    "This is because the nursery is a region of memory where newly allocated objects are stored before being promoted to the old generation."
                ]
            }
        ],
        "Normal": [
            {
                "description": "A regular string used to store general information."
            }
        ],
        "FuncName": "Sweeping_8226 freeFromBackgroundThread.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "We must finalize thing kinds in the order specified by BackgroundFinalizePhases.",
                    "Release any arenas that are now empty, which is a crucial step in garbage collection.",
                    "Empty arenas are only released after everything has been finalized so that it's still possible to get a thing's zone after the thing has been finalized.",
                    "This ensures that HeapPtr destructors can function correctly, allowing HeapPtrs between things of different alloc kind regardless of finalization order.",
                    "Periodically dropping and reacquiring the GC lock helps avoid blocking the main thread from allocating chunks.",
                    "Recording time spent sweeping each zone is essential for database performance monitoring."
                ]
            }
        ],
        "Normal": [
            "Sweep zones in order, with the atoms zone finalized last as other zones may have direct pointers into it, ensuring data integrity in the database."
        ],
        "FuncName": "Sweeping_8226 sweepBackgroundThings.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "We just finished passing over one or more free things, so record a new FreeSpan. This is a crucial step in Garbage Collection, where Arena objects are marked as scanned to prepare for garbage collection.",
                    "so record a new FreeSpan."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Do nothing. The caller will update the arena appropriately. In the context of Garbage Collection, this means the Arena objects are already marked as scanned, so no further action is needed.",
                    "Do nothing. The caller will update the arena appropriately."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "If the last thing was marked, we will have already set the bounds of, and we just need to terminate the list. This is an optimization step in Garbage Collection, where Arena objects are efficiently marked as scanned.",
                    "If the last thing was marked, we will have already set the bounds of, and we just need to terminate the list."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Enforce requirements on size of T. In a database context, this means ensuring the size of the data type T is correctly defined to prevent errors during garbage collection."
        ],
        "FuncName": "Sweeping_8226 finalize.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " If there was an 'enter-weak-marking-mode' token in the queue, then it",
                    " and everything after it will still be in the queue so we can process",
                    " them now.",
                    " Note: In the context of weak references, this token marks the beginning",
                    " of a weak marking phase in the garbage collector. It indicates that",
                    " the garbage collector is transitioning to a state where it will",
                    " temporarily mark objects as weakly reachable.",
                    " Do not rely on the information about not-yet-marked weak keys that have",
                    " been collected by barriers. Clear out the gcEphemeronEdges entries and",
                    " rebuild the full table. Note that this a cross-zone operation; delegate",
                    " zone entries will be populated by map zone traversals, so everything",
                    " needs to be cleared first, then populated."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            " Loop through the queue and process each token until we reach the",
            " 'enter-weak-marking-mode' token, which marks the beginning of the weak",
            " marking phase."
        ],
        "Normal": [
            " Ensure we don't return to the mutator while we're still in weak marking",
            " mode. This is necessary to prevent the garbage collector from being",
            " interrupted while it is in the process of marking objects as weakly",
            " reachable."
        ],
        "FuncName": "Sweeping_8226 markWeakReferences.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Merging completed regions. This is a string from database code."
        ],
        "Loop": [
            "Loop-Condition-Str: Merging completed regions. This is a string from database code."
        ],
        "Normal": [
            "Updating arena lists off-thread requires taking the GC lock because the main thread uses these when allocating. This is a string from database code related to arena management in the database."
        ],
        "FuncName": "Sweeping_8226 mergeFinalizedArenas.txt"
    },
    {
        "Branch": [
            "Prepare scanning group for GC, if condition."
        ],
        "Loop": [
            "Prepare scanning group for GC, loop condition."
        ],
        "Normal": [
            "Use one component for two-slice zeal modes. In GC database, this statement is related to the zeal mode selection in garbage collection process."
        ],
        "FuncName": "Sweeping_8226 groupZonesForSweeping.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "In GC runtime, the scan graph edge is found at the boundary of the loop. This is a key concept in the GC algorithm for [database] code.",
                    "As an optimization, if the wrapped object is already marked black there is no danger of later marking and we can skip this.",
                    "We don't need to consider any more wrappers for this target compartment since we already added an edge, which is a crucial step in the GC process for [database] type."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "Sweeping_8226 findSweepGroupEdges.txt"
    },
    {
        "Branch": [
            "If condition string: In the context of garbage collection, these strings are related to the database code, specifically the conditions that trigger the GC process."
        ],
        "Loop": [
            "Loop condition string: In the database code, these strings are used to control the execution of loops, which are essential for data processing and storage."
        ],
        "Normal": [
            "String wrappers are dropped on GC because their presence would require us to sweep the wrappers in all compartments every time we sweep a compartment group. In the database context, this means that the GC process must handle the storage and retrieval of string data efficiently to maintain data integrity and performance."
        ],
        "FuncName": "Sweeping_8226 dropStringWrappers.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Adding edges to the wrapper object's zone to ensure the wrapper zone is marked when we start scanning the wrapper zone",
                    "The zone is still marked when we begin sweeping the wrapper zone",
                    "As an optimization, if the wrapper object has already been marked as black, then",
                    "we do not need to consider any other wrappers for this target zone because we have already added an edge",
                    "because we have already added an edge"
                ]
            }
        ],
        "Normal": [],
        "FuncName": "Sweeping_8226 findSweepGroupEdges.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " For testing only. In the context of garbage collection, this involves adding edges between objects in the mark queue, which is a crucial step in the mark phase of garbage collection. This ensures that all objects are properly connected and their relationships are maintained. In database terms, this can be thought of as creating relationships between entities in a database, such as adding foreign keys or links between tables. ",
            "",
            " Add edges between all objects mentioned in the test mark queue since otherwise they will get marked in a different order than their sweep groups Note that this is only done at the beginning of an incremental collection so it is possible for objects to be added later that do not follow the sweep group ordering These objects will wait until their sweep group comes up or will be skipped if their sweep group is already past "
        ],
        "FuncName": "Sweeping_8226 addEdgesForMarkQueue.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Not on our list. This operation removes an object from the grey list."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "Sweeping_8226 RemoveFromGrayList.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Abort collection of subsequent sweep groups. In database context, this means that the current sweep group will be terminated and the next one will be initiated.",
                    "The database system will stop collecting sweep groups and move on to the next one."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "Sweeping_8226 getNextSweepGroup.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Synchronize access to JSCompartment::gcIncomingGrayPointers.",
                    "",
                    "TODO: Instead of building this list we could scan all incoming CCWs and",
                    "mark through gray ones when marking gray roots for a sweep group.",
                    "This is a code snippet related to database synchronization in JavaScript compartment."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "This loop condition is related to database iteration in JavaScript compartment.",
            "It may involve database queries or data processing."
        ],
        "Normal": [
            "Called from MarkCrossCompartmentXXX functions.",
            "* Assert that the object is in our list, also walking the list to check its* integrity.",
            "This normal string is related to database error handling in JavaScript compartment."
        ],
        "FuncName": "Sweeping_8226 DelayCrossCompartmentGrayMarking.txt"
    },
    {
        "Branch": [
            "In GC runtime, traverse the scan group edges."
        ],
        "Loop": [
            {
                "loopstr": [
                    "Add an edge to the wrapped object's zone to ensure that the wrapper zone is not still being marked when we start sweeping the wrapped zone. This is relevant to the database type 'GC Runtime'.",
                    "As an optimization, if the wrapped object is already marked black there is no danger of later marking and we can skip this. This is an important consideration for database performance.",
                    "We don't need to consider any more wrappers for this target compartment since we already added an edge. This is because the database type 'GC Runtime' only requires one edge per compartment."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "Sweeping_8226 findSweepGroupEdges.txt"
    },
    {
        "Branch": [
            "Scan compressed files for if-conditions."
        ],
        "Loop": [
            "Iterate through compressed files with loop-conditions."
        ],
        "Normal": [
            "Attach finished compression tasks.",
            "These tasks are related to database compression."
        ],
        "FuncName": "Sweeping_8226 sweepCompressionTasks.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Mark transitively inside the current compartment group. These strings are for database code.",
            "We must not yield after this point before we start sweeping the group."
        ],
        "FuncName": "Sweeping_8226 endMarkingSweepGroup.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Change state of current group to MarkBlackAndGray to restrict gray marking for database operations.",
            " Note that there may be pointers to the atoms zone, and these will be marked through as they are not marked with TraceCrossCompartmentEdge in database context.",
            " Mark incoming gray pointers from previously swept compartments for database type."
        ],
        "FuncName": "Sweeping_8226 beginMarkingSweepGroup.txt"
    },
    {
        "Branch": [
            "In a database context, an if-condition is used to filter records based on specific criteria, ensuring that only relevant data is processed."
        ],
        "Loop": [
            "In database operations, a loop-condition is used to iterate through a set of records, performing actions on each item until a specified condition is met."
        ],
        "Normal": [
            "This calls back into the browser which expects to be called from the main thread. In database terms, this refers to the synchronization of data between the browser and the database server, ensuring that all changes are committed and visible to all users."
        ],
        "FuncName": "Sweeping_8226 sweepFinalizationObserversOnMainThread.txt"
    },
    {
        "Branch": [
            "In a database, when two objects in the same compartment have their contents swapped, if either of them were in our gray pointer list, we re-add them again."
        ],
        "Loop": [],
        "Normal": [
            "Two objects in the same compartment have had their contents swapped. If either of them were in our gray pointer list, we re-add them again. This is a common operation in database management systems, where the gray pointer list is used to keep track of objects that need to be re-added after a swap operation."
        ],
        "FuncName": "Sweeping_8226 NotifyGCPostSwap.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Detach unreachable debuggers and global objects from each other. This is a crucial step in garbage collection, especially when dealing with debuggers and global objects that are no longer needed.",
            "This operation can modify weakmaps and therefore must occur before weakmap sweeping to ensure data consistency.",
            "Sweeping the debug environment information involves looking up unique IDs in the Zone's table, which must be done sequentially to avoid conflicts with concurrent sweeping operations.",
            "In the context of database management, this process is essential for maintaining the integrity of debug environment data and preventing potential issues during garbage collection."
        ],
        "FuncName": "Sweeping_8226 sweepDebuggerOnMainThread.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Since this is only ever used for sweeping, we can optimize it for that. This is a weak reference ID, which is used to track the ID of a cell in the database.",
            " case. (Compacting GC updates this table manually when it moves a cell. This is a database-related operation.)"
        ],
        "FuncName": "Sweeping_8226 traceWeak.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Two objects in the same compartment are about to have their contents swapped. If either object is in the gray pointer list, it will be removed and a bitset indicating the swap outcome will be returned. "
        ],
        "FuncName": "Sweeping_8226 NotifyGCPreSwap.txt"
    },
    {
        "Branch": [
            "Update atomic bitmap to track marked atoms; these are used in database code."
        ],
        "Loop": [
            "Update atomic bitmap to track marked atoms; these are used in database code."
        ],
        "Normal": [
            "For convenience sweep these tables non-incrementally as part of bitmap update to track marked atoms used in database code.",
            "sweeping; they are likely to be much smaller than the main atoms table."
        ],
        "FuncName": "Sweeping_8226 updateAtomsBitmap.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Copy the current contents of sweepList so that ArenaIter can find them.",
                    "In the context of a garbage collector, this involves handling front-end garbage collection."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Non-empty arenas are reused for use for new allocations as soon as the",
            "finalizers for that allocation kind have run. Empty arenas are only",
            "released when everything in the zone has been swept (see",
            "GCRuntime::sweepBackgroundThings for more details).",
            "In the database, this process is related to the management of memory allocation and garbage collection."
        ],
        "FuncName": "Sweeping_8226 foregroundFinalize.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "If we ran out of memory, do all the work on the main thread. This may lead to performance degradation due to thread contention and blocking."
        ],
        "FuncName": "Sweeping_8226 SweepAllWeakCachesOnMainThread.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Create secondary tables to hold new atoms added while we're sweeping the main tables incrementally. This is a common operation in atomic databases."
        ],
        "FuncName": "Sweeping_8226 startSweepingAtomsTable.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "This means don't yield to the mutator here. In database terms, this is an example of a conditional statement, where the database engine will only execute the code if the condition is met."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "thenstr": [
                    "This is an example of a loop condition in a database context, where the database engine will repeatedly execute the code as long as the condition is met."
                ],
                "elsestr": []
            }
        ],
        "Normal": [
            {
                "str": [
                    "This is a normal string in a database context, where the database engine will store and retrieve the data as needed."
                ]
            }
        ],
        "FuncName": "Sweeping_8226 markDuringSweeping.txt"
    },
    {
        "Branch": [
            "Prepare weak cache tasks in JSRuntime for if-conditions"
        ],
        "Loop": [
            "Prepare weak cache tasks in JSRuntime for loop conditions"
        ],
        "Normal": [
            "Start incremental sweeping for caches that support it or add to a vector of sweep tasks to run on a helper thread. This is a database-specific operation for JSRuntime."
        ],
        "FuncName": "Sweeping_8226 PrepareWeakCacheTasks.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Set the GC state to sweeping, which involves purging the ArenaLists before sweeping.",
                    "Purge the ArenaLists before sweeping, a necessary step in the sweeping process."
                ]
            }
        ],
        "Normal": [
            "Begin sweeping the group of zones in currentSweepGroup, performing actions that must be done before yielding to caller, specifically database-related tasks.",
            "Updating the atom marking bitmaps, a database-specific process that marks atoms referenced by uncollected zones, and cannot be done in parallel with other sweeping work.",
            "Sweeping realm globals, a critical database task that must happen before finalization registry sweeping.",
            "FinalizationRegistry sweeping touches weak maps and so must not run in parallel with that, a database constraint that triggers a read barrier and can add marking work for zones that are still marking.",
            "Updating embedding weak pointers, a database-related task that must happen before sweeping realm globals.",
            "Queue all GC things in all zones for sweeping, either on the foreground or on the background thread, a database-specific process that involves sweeping database-related data."
        ],
        "FuncName": "Sweeping_8226 beginSweepingSweepGroup.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Sweep phase. Finalize as we sweep, outside of lock but with RuntimeHeapIsBusy() true so that any attempt to allocate a GC-related object from a finalizer will fail, rather than nest badly and leave the unmarked newborn to be swept in a database context."
        ],
        "FuncName": "Sweeping_8226 beginSweepPhase.txt"
    },
    {
        "Branch": [
            "Set the number of things per arena for this AllocKind in GCRuntime.",
            "Reset the slots of the sweep list that we used in GCRuntime."
        ],
        "Loop": [],
        "Normal": [
            "IncrementalProgress GCRuntime::finalizeAllocKind: Set the number of things per arena for this AllocKind based on the given GC context and budget.",
            "IncrementalProgress GCRuntime::finalizeAllocKind: Reset the slots of the sweep list that we used in GCRuntime."
        ],
        "FuncName": "Sweeping_8226 finalizeAllocKind.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Clear out any small pools that we're hanging on to.",
                    "Update the GC state for zones we have swept."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "This is to prevent a race between markTask checking the zone state and us changing it below.",
            "Disable background marking during sweeping until we start sweeping the next zone group.",
            "Free LIFO blocks on a background thread if possible.",
            "Update the GC state for zones we have swept.",
            "* Start background thread to sweep zones if required, sweeping any atoms* zones last if present.",
            "End the sweeping group, update the GC state for the swept zones."
        ],
        "FuncName": "Sweeping_8226 endSweepingSweepGroup.txt"
    },
    {
        "Branch": [
            " GeoJSON If-Condition requires coordinates to be an array, e.g. [longitude, latitude]."
        ],
        "Loop": [
            " GeoJSON Loop-Condition requires coordinates to be an array, e.g. [longitude, latitude]."
        ],
        "Normal": [
            " GeoJSON allows extra elements, e.g. altitude. GeoJSON coordinates must be an array, e.g. [longitude, latitude]."
        ],
        "FuncName": "geoparser_1387 parseGeoJSONCoordinate.txt"
    },
    {
        "Branch": [
            "Check if coordinates are valid and exist in GeoJSON database."
        ],
        "Loop": [
            "Iterate through each coordinate in GeoJSON array."
        ],
        "Normal": [
            "Iterate all coordinates in array. Each coordinate is a point in GeoJSON database."
        ],
        "FuncName": "geoparser_1387 parseArrayOfCoordinates.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Point coordinates must be finite numbers, neither NaN nor infinite. In a 2D plane, x and y coordinates are used to represent points, where x is the horizontal axis and y is the vertical axis. The coordinates can be positive or negative, but not infinite or NaN (Not a Number)."
        ],
        "FuncName": "geoparser_1387 parseFlatPoint.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "We don't rely on drem to clean up non-sane points.  We just don't let them become S2 points with invalid coordinates.",
            "Note that it's (lat, lng) for S2 but (lng, lat) for MongoDB, which uses a different geospatial index.",
            "This shouldn't happen since we should only have valid lng/lats for S2 coordinates.",
            "S2 points require valid latitude and longitude values to perform geospatial queries."
        ],
        "FuncName": "geoparser_1387 coordToPoint.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Perform garbage collection by executing garbage collection actions.",
            "Drain the mark stack, possibly in a parallel task if we're in a part of sweeping that allows it.",
            "",
            "In the first sweep slice where we must not yield to the mutator until we've starting sweeping a sweep group but in that case the stack must be empty already.",
            "Then continue running sweep actions."
        ],
        "FuncName": "Sweeping_8226 performSweepActions.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Verify if all visible areas have completed gray marking.",
            "",
            "Gray bits become valid if we finished a full GC from the cycle collector's perspective.",
            "We ignore the following:",
            "",
            "  - Helper thread zones, as these are not reachable from the main heap.",
            "  - The atoms zone, since strings and symbols are never marked gray.",
            "  - Empty zones.",
            "",
            "These exceptions ensure that when the CC requests a full GC, the gray mark state ends up valid even if we don't collect all of the zones."
        ],
        "FuncName": "Sweeping_8226 allCCVisibleZonesWereCollected.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Error: Loop 1 shares more than one vertex with its parent loop 0. This is invalid in GeoJSON."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Parse the array of vertices of a loop in a GeoJSON polygon.",
                    "Check if the loop is closed in a GeoJSON polygon.",
                    "Drop the duplicated last point in a GeoJSON polygon.",
                    "A GeoJSON polygon must have at least 3 vertices.",
                    "Check whether this loop is valid in a GeoJSON polygon if validation hasn't been already done on 2dSphere index.",
                    "Insertion which is controlled by the 'skipValidation' flag in a GeoJSON polygon.",
                    "1. A GeoJSON polygon must have at least 3 vertices.",
                    "2. All vertices must be unit length. Guaranteed by parsePoints().",
                    "3. Loops are not allowed to have any duplicate vertices in a GeoJSON polygon.",
                    "4. Non-adjacent edges are not allowed to intersect in a GeoJSON polygon.",
                    "If the loop is more than one hemisphere, invert it in a GeoJSON polygon.",
                    "Check the first loop must be the exterior ring and any others must be interior rings or holes in a GeoJSON polygon."
                ]
            }
        ],
        "Normal": [
            "Iterate all loops of the polygon in a GeoJSON object.",
            "Check if the given loops form a valid polygon in a GeoJSON object.",
            "1. If a loop contains an edge AB, then no other loop may contain AB or BA in a GeoJSON polygon.",
            "2. No loop covers more than half of the sphere in a GeoJSON polygon.",
            "3. No two loops cross in a GeoJSON polygon.",
            "Given all loops are valid / normalized and S2Polygon::IsValid() above returns true in a GeoJSON object.",
            "The polygon must be valid in a GeoJSON object. See S2Polygon member function IsValid().",
            "Check if every loop of this polygon shares at most one vertex with its parent loop in a GeoJSON object.",
            "S2Polygon contains more than one ring, which is allowed by S2, but not by GeoJSON in a GeoJSON object.",
            "",
            "Loops are indexed according to a preorder traversal of the nesting hierarchy in a GeoJSON object.",
            "GetLastDescendant() returns the index of the last loop that is contained within a given loop in a GeoJSON object.",
            "We guarantee that the first loop is the exterior ring in a GeoJSON object.",
            "In GeoJSON, only one nesting is allowed in a GeoJSON object.",
            "The depth of a loop is set by polygon according to the nesting hierarchy of polygon, so the exterior ring's depth is 0, a hole in it is 1, etc. in a GeoJSON object."
        ],
        "FuncName": "geoparser_1387 parseGeoJSONPolygonCoordinates.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Remove dead SharedPropMaps from the tree. This happens incrementally on the main thread. PropMaps are finalized later on a background thread. In a database context, this refers to the removal of redundant or outdated data mappings that no longer serve a purpose."
        ],
        "FuncName": "Sweeping_8226 sweepPropMapTree.txt"
    },
    {
        "Branch": [
            "A valid BigSimplePolygon must satisfy the following conditions:"
        ],
        "Loop": [
            "A valid loop in a BigSimplePolygon must satisfy the following conditions:"
        ],
        "Normal": [
            "Only one loop is allowed in a BigSimplePolygon and it must be a valid polygon.",
            "The last point of the loop is duplicated.  We drop it, since S2Loop expects no duplicate points.",
            "At least 3 vertices are required to form a valid loop in a BigSimplePolygon.",
            "Check whether the loop is valid and does not intersect with any other part of the polygon."
        ],
        "FuncName": "geoparser_1387 parseBigSimplePolygonCoordinates.txt"
    },
    {
        "Branch": [
            "Parsing legacy polygons in database"
        ],
        "Loop": [
            {
                "loopstr": [
                    "A coordinate in a polygon of a database"
                ]
            }
        ],
        "Normal": [
            "Parsing legacy polygons in database"
        ],
        "FuncName": "geoparser_1387 parseLegacyPolygon.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "GeoJSON Point must explicitly specify the type as Point.",
                    "This is a requirement for GeoJSON Point data in the database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "\\crs\\: Coordinate Reference System",
            "\\coordinates\\: List of coordinate values",
            "Projection: Transformation of coordinates to a 2D or 3D space in the database."
        ],
        "FuncName": "geoparser_1387 parseGeoJSONPoint.txt"
    },
    {
        "Branch": [
            "Check if condition is met in geospatial database"
        ],
        "Loop": [
            "Iterate over features in geospatial database"
        ],
        "Normal": [
            " Iterate array of geographic coordinates"
        ],
        "FuncName": "geoparser_1387 parseMultiLine.txt"
    },
    {
        "Branch": [
            "If condition is met, then parse GeoJSON linear geometry objects."
        ],
        "Loop": [
            "Loop through each linear geometry object in the database."
        ],
        "Normal": [
            "\\crs\\: Coordinate Reference System, a spatial reference system that provides a common framework for geographic coordinates.",
            "\\coordinates\\: A sequence of coordinates that define the shape of a linear geometry object."
        ],
        "FuncName": "geoparser_1387 parseGeoJSONLine.txt"
    },
    {
        "Branch": [
            "IF AREA >= 0 THEN"
        ],
        "Loop": [
            "WHILE AREA >= 0 DO"
        ],
        "Normal": [
            "VERIFY AREA >= 0",
            "DATABASE AREA CHECK"
        ],
        "FuncName": "geoparser_1387 parseLegacyBox.txt"
    },
    {
        "Branch": [
            "If condition: CRS is a spatial reference system, allowing for a strict sphere. Coordinate: a point in space."
        ],
        "Loop": [
            "Loop condition: CRS is a spatial reference system, allowing for a strict sphere. Coordinate: a point in space."
        ],
        "Normal": [
            "crs, allow strict sphere: CRS is a spatial reference system that allows for a strict sphere. Coordinate: a point in space.",
            "coordinates: A set of coordinates that define a point in space."
        ],
        "FuncName": "geoparser_1387 parseGeoJSONPolygon.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Legacy point (supports Legacy points and GeoJSON points in the database)"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "location: [1, 2] or location: {x: 1, y:2} (supports GeoJSON points in the database)",
            "GeoJSON point. location: {type: Point, coordinates: [1, 2]} (supports GeoJSON points and Legacy points in the database)"
        ],
        "FuncName": "geoparser_1387 parsePoint.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "crs field doesn't exist, return the default SPHERE. In GeoJSON, CRS stands for Coordinate Reference System, which defines the spatial reference system used to describe the location of features. It's crucial for accurate spatial analysis and mapping.",
            "type: name. In GeoJSON, the 'type' keyword indicates the type of geometry, which can be Point, LineString, Polygon, etc. It's essential for understanding the spatial structure of the data.",
            "properties. In GeoJSON, the 'properties' keyword contains additional information about the feature, such as its name, description, or other attributes. It's vital for providing context and meaning to the spatial data."
        ],
        "FuncName": "geoparser_1387 parseGeoJSONCRS.txt"
    },
    {
        "Branch": [
            "Check if polygon has holes"
        ],
        "Loop": [
            "Iterate over polygon's exterior ring"
        ],
        "Normal": [
            "Iterate array",
            "For each exterior ring, iterate over its coordinates"
        ],
        "FuncName": "geoparser_1387 parseMultiPolygon.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Geographic Center",
            "Circular Radius",
            "radius must be a non-negative number",
            "No more"
        ],
        "FuncName": "geoparser_1387 parseLegacyCenter.txt"
    },
    {
        "Branch": [
            "Initialize the iterator to the first element of the database table"
        ],
        "Loop": [
            "Iterate over the database table starting from the first element"
        ],
        "Normal": [
            "Initialize the iterator state to the first element of the database table"
        ],
        "FuncName": "bsoncolumn_6279 Iterator.txt"
    },
    {
        "Branch": [
            "Initializes the loop for conditional reading of other data. This string is used in database code."
        ],
        "Loop": [
            "Initializes the loop for conditional reading of other data. This string is used in database code."
        ],
        "Normal": [
            "Interleaved mode should contain at least one object. This statement is associated with database code, ensuring proper data integration and synchronization."
        ],
        "FuncName": "bsoncolumn_6279 _initializeInterleaving.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Keep track of current block if it exists. In MongoDB, this involves managing the allocation of memory blocks for BSON elements.",
                    "If contiguous mode is enabled we need to copy data from the previous block, which is essential for efficient data transfer in MongoDB databases.",
                    "Double block size while keeping it within [kStartCapacity, kMaxCapacity] range, unless a larger size than kMaxCapacity is requested, which is a critical consideration for MongoDB's memory management.",
                    "Copy data from the previous block if contiguous mode is enabled. This is a crucial step in maintaining data integrity in MongoDB databases."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If current block doesn't have enough capacity we need to allocate a new one. In the context of MongoDB databases, this involves dynamically allocating memory blocks for BSON elements.",
            "Increment the used size and return"
        ],
        "FuncName": "bsoncolumn_6279 allocate.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " Traverse current Simple8b block for 64bit values if it exists and update decoder status",
                    "Check for more delta values or control bytes in the database"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Traverse current Simple8b block for 128bit values if it exists and update decoder status",
                    "Check for more delta values or control bytes in the database"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Check decoder status and load next control byte if necessary",
            "Validate that we are not reading out of bounds in the database",
            "Decoders are exhausted, load next control byte and check for End Of Output (EOO) in the database",
            "Load new control byte and check for more data in the database"
        ],
        "FuncName": "bsoncolumn_6279 _incrementRegular.txt"
    },
    {
        "Branch": [
            "Check if the GeoJSON GeometryCollection is valid."
        ],
        "Loop": [
            {
                "loopstr": [
                    "Parse the GeoJSON GeometryCollection and check the result."
                ]
            }
        ],
        "Normal": [
            "The GeoJSON GeometryCollection is parsed successfully."
        ],
        "FuncName": "geoparser_1387 parseGeometryCollection.txt"
    },
    {
        "Branch": [
            "BSON elements' values are returned as strings in MongoDB database"
        ],
        "Loop": [
            "BSON elements' values are returned as strings in MongoDB database"
        ],
        "Normal": [
            "BSON elements' values are returned as strings in MongoDB database. Skip over type byte and null terminator for field name."
        ],
        "FuncName": "bsoncolumn_6279 value.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Exit interleaved mode and load as regular. Re-instantiate the state and set last known value.",
                    "Before exiting interleaved mode, verify all of the decoders are exhausted.",
                    "This invalidates 'interleaved' reference, may no longer be dereferenced.",
                    "The allocator will keep all allocations in contiguous memory, ensuring efficient memory usage."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Notify the internal allocator to keep all allocations in contiguous memory, optimizing for performance.",
            "Iterate over the reference interleaved object, matching scalar subfields with interleaved states.",
            "Internal recursion is performed, and the second lambda is called for scalar fields, producing a full BSONObj.",
            "A RAII object is instantiated to handle writing the BSONObj size and EOO bytes for subobjects.",
            "Traverse the interleaved reference object, matching interleaved states with literals.",
            "Verify that there are as many interleaved states as scalar fields, ensuring data consistency.",
            "Store the built BSONObj in the decompressed list, preserving data integrity.",
            "If no data was added, use a EOO literal, indicating the end of the buffer.",
            "Root objects have an empty field name, and the total object size is already known, ensuring efficient processing."
        ],
        "FuncName": "bsoncolumn_6279 _incrementInterleaved.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Center: A point in the 2D coordinate system representing the center of a circle.",
            "Center: Check the object has and only has 2 numbers.",
            "Radius: The distance from the center to the edge of the circle.",
            "Radius: radius >= 0 and is not NaN, ensuring a valid circle definition.",
            "No more elements"
        ],
        "FuncName": "geoparser_1387 parseCenterSphere.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Keep track of current block if it exists. In MongoDB, a block represents a contiguous portion of data stored in a file.",
                    "If contiguous mode is enabled, we need to copy data from the previous block, which is a common operation in MongoDB's data management.",
                    "Double block size while keeping it within the range of [kStartCapacity, kMaxCapacity] defined by the MongoDB configuration, unless a size larger than kMaxCapacity is requested, which would require additional memory allocation.",
                    "Copy data from the previous block if contiguous mode is enabled, which is essential for maintaining data consistency in MongoDB."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If current block doesn't have enough capacity, we need to allocate a new one, which involves creating a new BSON element in MongoDB's storage system.",
            "Increment the used size and return, updating the MongoDB database with the new size information."
        ],
        "FuncName": "bsoncolumn_6279 allocate.txt"
    },
    {
        "Branch": [
            "OID to BSON conversion: If condition met, OID value converted to BSON element"
        ],
        "Loop": [
            "OID to BSON conversion: Loop condition met, OID value converted to BSON element"
        ],
        "Normal": [
            "OID to BSON conversion: Decoder state setup, materializing new value. Allocated new BSONElement with same value size as previous OID.",
            "OID to BSON conversion: Wrote value to BSON element based on OID type"
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits same value size as previous. In MongoDB, BSONElement is a fundamental data structure that represents a single value in a BSON document.",
            "Write value depending on type. MongoDB supports various data types, including integers, strings, and dates."
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [
            " Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits same value size as previous. In OID conversion, the OID is first converted into a binary format, then wrapped into a BSONElement."
        ],
        "Loop": [],
        "Normal": [
            "Write value depending on type. In OID conversion, the BSONElement contains the OID value in binary format, which is a 12-byte value representing the OID in the database."
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [
            "OID-Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits same value size as previous OID-type value.",
            "Write OID-type value depending on OID-type"
        ],
        "Loop": [
            "OID-Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits same value size as previous OID-type value.",
            "Write OID-type value depending on OID-type"
        ],
        "Normal": [
            "OID-Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits same value size as previous OID-type value.",
            "Write OID-type value depending on OID-type"
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "TODO: check for valid encoding. This is a MongoDB encoded string.",
                    "reached end of column. MongoDB column types include string, integer, and boolean."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "TODO SERVER-74926 add interleaved support. This is a MongoDB interleaved string.",
                    "MongoDB supports interleaved storage for string and integer types."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "bsoncolumn_6279 contains_forTest.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits same value size as previous Write value depending on type. In MongoDB, BSON (Binary Serialized Object Notation) is a binary data format used for storing and transmitting data between systems. The OID (Object ID) value is used to create a BSON element, which is a fundamental data structure in MongoDB that represents a single value or a document."
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits same value size as previous, this is a process typical in OID type data storage in MongoDB database.",
            "Write value depending on type, which is a common operation in database operations like insert, update, delete in MongoDB database."
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [
            "If-condition OID value converted to BSONElement for database query."
        ],
        "Loop": [
            "Loop-condition OID value converted to BSONElement for database query."
        ],
        "Normal": [
            " Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits same value size as previous Write value depending on type. OID value converted to BSONElement for database query."
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits, which represents a binary value in the OID field of the database.",
            " same value size as previous, indicating a consistent data type in the database.",
            " Write value depending on type, where OID values are converted to BSONElement values in the database."
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [
            "If the database is updated, the client is nullptr if the parameter is supplied from the command line. In this case, we ignore the update event, the parameter will be processed when initializing the service context, related to database update statistics storage size."
        ],
        "Loop": [],
        "Normal": [
            "The client is nullptr if the parameter is supplied from the command line. In this case, we ignore the update event, the parameter will be processed when initializing the service context, which affects database storage size."
        ],
        "FuncName": "query_stats_on_parameter_change_3613 onQueryStatsStoreSizeUpdate.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "The client is nullptr if the parameter is supplied from the command line. In this case, we update the database statistics sampling rate, ignoring the update event, the parameter will be processed when initializing the service context, which is a crucial step for database performance optimization.",
            "ignore the update event, the parameter will be processed when initializing the service context, which is a key step for database performance monitoring.",
            "The update event is ignored, the parameter will be processed when initializing the service context, which is an essential step for database statistics update."
        ],
        "FuncName": "query_stats_on_parameter_change_3613 onQueryStatsSamplingRateUpdate.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Decoder state is now setup, materialize new value. We allocate a new BSONElement that fits same value size as previous. In MongoDB, BSON is a binary serialization format that stores data in a binary format, making it efficient for storage and transfer.",
            "Write value depending on type. In MongoDB, each BSON element has a specific type, such as String, Integer, or Timestamp, which determines how the value is stored and processed."
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Constructing BSON element from OID. This is a MongoDB database operation.",
            "Write value depending on type"
        ],
        "FuncName": "bsoncolumn_6279 materialize.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "In MongoDB, when writing string data to a BSON element, these strings are represented as strings in the database code.",
            "In MongoDB, when writing string data to a BSON element, the strings are prefixed with a 4 byte count and suffixed with a null terminator, adding 5 bytes to the size.",
            "In MongoDB, when writing string data to a BSON element, the count is written first, and the size includes the null terminator.",
            "In MongoDB, when writing string data to a BSON element, the string value is written next, followed by a null terminator."
        ],
        "FuncName": "bsoncolumn_6279 writeStringData.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "RLE (Run-Length Encoding) is a lossless data compression algorithm used to compress data by replacing sequences of repeated bytes with a single byte and a count of the number of times it appears in the sequence."
                ],
                "elsestr": [
                    "If the data is uncompressible or the buffer size is too small, the algorithm may produce errors or fail to compress the data correctly."
                ]
            },
            {
                "thenstr": [
                    "RLE is often used in image and video compression, as it can efficiently compress data with high spatial redundancy."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "In this test, we will select random parameters such as the number of streams, the decoding method (X1 or X2), and the use of the BMI2 instruction set to optimize compression.",
            "We will also select a random buffer size, which may be too small for optimal compression.",
            "The table log will be adjusted later to ensure optimal compression performance."
        ],
        "FuncName": "huf_round_trip_2296 LLVMFuzzerTestOneInput.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "TODO: Add UTF-8 decoding and convert to Rune for database processing"
        ],
        "FuncName": "regexp_8085 chartorune.txt"
    },
    {
        "Branch": [
            "Check if character is a letter in the database"
        ],
        "Loop": [
            "Check if character is a letter in the database"
        ],
        "Normal": [
            " TODO: Add unicode support for database characters"
        ],
        "FuncName": "regexp_8085 isalpharune.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "else identity escape in database context"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "database loop condition"
                ]
            }
        ],
        "Normal": [
            {
                "normalstr": [
                    "database normal string"
                ]
            }
        ],
        "FuncName": "regexp_8085 lexclass.txt"
    },
    {
        "Branch": [
            "IF CONDITION IN DATABASE"
        ],
        "Loop": [
            "LOOP CONDITION IN DATABASE"
        ],
        "Normal": [
            "TODO: ADD UNICODE SUPPORT IN DATABASE"
        ],
        "FuncName": "regexp_8085 toupperrune.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "WiredTiger storage engine options",
            "Enables or disables WiredTiger storage engine options",
            "Configures WiredTiger storage engine settings"
        ],
        "FuncName": "wiredtiger_global_options_6963 store.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "A fast-path projection is not feasible, fall back to default implementation. In database terms, this means that the query cannot be optimized using the FastPathEligibleInclusionNode, resulting in a less efficient execution plan."
        ],
        "FuncName": "inclusion_projection_executor_2639 applyToDocument.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "In MongoDB, substitute newName for oldName in the expression. This is a common operation in database operations."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "In MongoDB, if all expressions have been extracted, this inclusion node should be removed. This is a best practice in database optimization."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "In MongoDB, do not extract for pushdown computed projection with reserved name. This is to prevent conflicts in database operations.",
                    "In MongoDB, do not extract a computed projection if it is computing a value that other fields in the same projection depend on. This is to maintain data consistency in database operations. If the extracted $addFields were to be placed before this projection, the dependency with the common name would be shadowed by the computed projection.",
                    "In MongoDB, to preserve data integrity, if the $addFields spec field name itself is using the old field name, then we need to rename the field as well since it would have been the new meta field. For example, {$addFields: {m: $m.sub}} where the 'm' is the meta field. If this $addFields is pushed down before the unpack bucket, it should be {$addFields: {meta: $meta.sub}} because this $addFields will hide the original meta and become the new meta.",
                    "In MongoDB, remove the expression from this inclusion node to maintain data consistency."
                ]
            },
            {
                "loopstr": [
                    "In MongoDB, if the $addFields spec field name itself is using the old field name, then we need to rename the field as well since it would have been the new meta field. For example, {$addFields: {m: $m.sub}} where the 'm' is the meta field. If this $addFields is pushed down before the unpack bucket, it should be {$addFields: {meta: $meta.sub}} because this $addFields will hide the original meta and become the new meta.",
                    "In MongoDB, remove the expression from this inclusion node to maintain data consistency."
                ]
            }
        ],
        "Normal": [
            "In MongoDB, if one of the expression requires the whole document, then we should not extract the projection and topLevelDeps will not hold any field names. This is a common issue in database operations.",
            "In MongoDB, auxiliary vector with extracted computed projections: <name, expression>. This is a useful tool in database optimization.",
            "In MongoDB, to preserve the original fields order, only projections at the beginning of the _orderToProcessAdditionsAndChildren list can be extracted for pushdown. This is a best practice in database optimization."
        ],
        "FuncName": "inclusion_projection_executor_2639 extractComputedProjectionsInAddFields.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "check identity escape",
                    "This is a conditional statement in a database query, checking the identity escape."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "regexp_8085 nextrune.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Avoid stack exhaustion in recursive parseatom et al. This is a common issue in database queries where a recursive function is used to parse a large dataset."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "regexp_8085 re_regcomp.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Do not pushdown computed projection with reserved name. In MongoDB, reserved names include system collections and fields."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "After seeing the first dotted path expression, we need to replace computed projections with identity projections to preserve the field order in MongoDB's document structure."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Substitute newName for oldName in the expression. In database schema changes, renaming fields is a common operation."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Replace the expression with an inclusion projected field. Only computed projections at the beginning of the list were marked to become projected fields. The new projected field is at the beginning of the _orderToProcessAdditionsAndChildren list. This is an optimization for database performance."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Do not extract a computed projection if it is computing a value that other fields in the same projection depend on. If the extracted $addFields were to be placed before this projection, the dependency with the common name would be shadowed by the computed projection. This is a database optimization technique."
                ]
            },
            {
                "loopstr": [
                    "If the $addFields spec field name itself is using the old field name, then we need to rename the field as well. Field renaming is a common operation in database schema changes."
                ]
            }
        ],
        "Normal": [
            "If one of the expression requires the whole document, then we should not extract the projection and topLevelDeps will not hold any field names. This is because the whole document is not a field in the projection.",
            "Auxiliary vector with extracted computed projections: <name, expression, replacement strategy>. If the replacement strategy flag is true, the expression is replaced with a projected field. If it is false - the expression is replaced with an identity projection. This is a technique used in database query optimization."
        ],
        "FuncName": "inclusion_projection_executor_2639 extractComputedProjectionsInProject.txt"
    },
    {
        "Branch": [
            "Conditional statement in database query: IF [condition] THEN [action]"
        ],
        "Loop": [
            "Database loop condition: WHILE [condition] DO [action]"
        ],
        "Normal": [
            "Database operation: QUEuing initial thread",
            "Database operation: RUNNING threads in stack order"
        ],
        "FuncName": "regexp_8085 match.txt"
    },
    {
        "Branch": [
            "Exchange Buffer: A conditional statement in the database code, used to control the flow of execution based on certain conditions."
        ],
        "Loop": [
            "Exchange Buffer: A condition used to control the repetition of a set of instructions in the database code."
        ],
        "Normal": [
            "For now, specialize to the one use case. As long as wasm Bytes is a Vector, not a linked-list of chunks, there s not much we can do other than copy. In the context of a database, this refers to the exchange of data between different components or systems, utilizing a buffer to facilitate the transfer of information.",
            "Vector, not a linked-list of chunks, there s not much we can do other than copy. In the context of a database, this refers to the exchange of data between different components or systems, utilizing a buffer to facilitate the transfer of information, ensuring efficient and reliable data exchange.",
            "In the context of a database, this refers to the exchange of data between different components or systems, utilizing a buffer to facilitate the transfer of information, ensuring efficient and reliable data exchange and storage."
        ],
        "FuncName": "Assembler-mips-shared_3231 swapBuffer.txt"
    },
    {
        "Branch": [
            "TODO Implement a proper halting align for if-conditions in database."
        ],
        "Loop": [
            "TODO Implement a proper halting align for loop-conditions in database."
        ],
        "Normal": [
            "TODO Implement a proper halting align."
        ],
        "FuncName": "Assembler-mips-shared_3231 haltingAlign.txt"
    },
    {
        "Branch": [
            "Check if buffer needs reserved space for database operations."
        ],
        "Loop": [
            "Check if buffer needs reserved space for database operations."
        ],
        "Normal": [
            "This buffer uses fixed-size chunks so there's no point in reserving now vs on-demand. Database operations require reserved space to ensure data integrity."
        ],
        "FuncName": "Assembler-mips-shared_3231 reserve.txt"
    },
    {
        "Branch": [
            "In the database, conditional statements are essential for controlling the flow of instructions. We restore the beq instruction, which is crucial for implementing if-conditions."
        ],
        "Loop": [
            "In database programming, loops are used to repeat a set of instructions. The restored beq instruction helps in implementing loop conditions, enabling efficient iteration."
        ],
        "Normal": [
            "We converted beq to andi, so now we restore it. This change affects the database's instruction set, requiring the beq instruction to be reinstated for proper functionality."
        ],
        "FuncName": "Assembler-mips-shared_3231 ToggleToJmp.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " A used label holds a link to branch that uses it. This is a description of the thenstr field."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            " This is a description of the Loop-Condition-Str field. It is related to the loop type in the database."
        ],
        "Normal": [
            " If our caller didn t give us an explicit target to bind to, then we want to bind to the location of the next instruction. This is a description of the Normal-Str field."
        ],
        "FuncName": "Assembler-mips-shared_3231 bind.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u5982\u679c\u6211\u4eec\u521a\u521a\u68c0\u67e5\u4e86\u6700\u5e95\u5c42\u7684\u503c\uff0c\u5e76\u4e14\u5b83\u4e0d\u662f",
                    "\u6211\u4eec\u6b63\u5728\u5bfb\u627e\u7684\uff0c\u90a3\u4e48\u5c31\u5b8c\u6210\u4e86\u3002",
                    "\u5728\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\uff0c\u8fd9\u4e2a\u6761\u4ef6\u901a\u5e38\u7528\u4e8eWHERE\u8bed\u53e5"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "\u5982\u679c\u6211\u4eec\u53d1\u73b0\u4e00\u4e2a\u503c\u5c0f\u4e8e\u6216\u7b49\u4e8e\u8fd9\u4e2a\u503c\uff0c\u5e76\u4e14",
                    "\u4e0b\u4e00\u4e2a\u503c\u4e5f\u4e0d\u662f\u5c0f\u4e8e\u8fd9\u4e2a\u503c\uff0c\u90a3\u4e48\u6211\u4eec\u5c31\u5230\u8fbe\u4e86\u3002",
                    "\u5728\u5173\u7cfb\u6570\u636e\u5e93\u4e2d\uff0c\u8fd9\u4e2a\u6761\u4ef6\u901a\u5e38\u7528\u4e8eFOR\u5faa\u73af\u6216WHILE\u5faa\u73af"
                ]
            }
        ],
        "Normal": [],
        "FuncName": "UnicodeShim_4317 LookupPredicate.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The target is not bound but used. Prepend label's branch list onto target's. This operation is related to Instruction Relocation in Database.",
                    "Find the head of the use chain for label. This is a crucial step in Instruction Relocation.",
                    "Then patch the head of label's use chain to the tail of target's use chain, prepending the entire use chain of target. This ensures correct Instruction Relocation."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "Loop Iteration: Relocation of instructions within a loop is critical in Database, ensuring correct execution flow."
        ],
        "Normal": [
            "Normal Instruction: In Database, this instruction is processed as part of the overall relocation strategy."
        ],
        "FuncName": "Assembler-mips-shared_3231 retarget.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If we've just checked the bottom-most value and it's not the one we're looking for, we're done. In database mapping, this means the character is not mapped to the desired value."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "0 means not present. In database mapping, this indicates that the character is not present in the mapping table."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Low bits 0 means a constant offset from the given character. In database mapping, this represents a fixed offset from the base character."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Low bits 1 means a special case mapping. In database mapping, this signifies a unique case mapping for the character."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "If we've found an entry less than or equal to this one, and the next one is not also less than this one, we've arrived. In database mapping, this condition is used to iterate through the mapping table and find the desired character."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "UnicodeShim_4317 LookupMapping.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " When the count map is empty, we can clean the whole registry. This is a registry cleanup operation, which involves removing unnecessary entries from the database."
        ],
        "FuncName": "bucket_state_registry_3063 cleanClearRegistry.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "If the bucket has been cleared, we will set the bucket state accordingly to reflect that. In the database, this is represented as a specific bucket status, typically denoted by a unique code, e.g., 'BUCKET_STATUS_CLEARED'."
        ],
        "FuncName": "bucket_state_registry_3063 materializeAndGetBucketState.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Conditional branch instructions.",
            "If-Condition-Str: Jump to a label if a condition is met."
        ],
        "Loop": [
            "Loop-Condition-Str: Looping instructions.",
            "Loop-Condition-Str: Repeat a sequence of instructions until a condition is met."
        ],
        "Normal": [
            " toggledJump is always used for short jumps.",
            " Replace \\beq $zero, $zero, offset\\ with \\andi $zero, $zero, offset\\",
            " Normal-Str: Normal instructions.",
            " Normal-Str: Instructions that perform a specific operation."
        ],
        "FuncName": "Assembler-mips-shared_3231 ToggleToCmp.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Most metadata should have already been stored within the WSM when we project out a document. This is a key concept in database design.",
            "The recordId metadata is different though, because it's a fundamental part of the WSM and we store it within the WSM itself rather than WSM metadata, so we need to transfer it into the metadata object if the projection has a recordId $meta expression. In database terms, this is an example of data normalization.",
            "If the projection doesn't need any metadata, then we'll just apply the projection to the input document. Otherwise, before applying the projection, we will move document metadata from the WSM into the document itself, and will move it back to the WSM once the projection has been applied. This process is similar to data transformation in databases.",
            "An exclusion projection can return an unowned object since the output document is constructed from the input one backed by BSON which is owned by the storage system, so we need to make sure we transition an owned document. This is a critical aspect of data integrity in databases."
        ],
        "FuncName": "projection_2237 transform.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Most metadata should have already been stored within the WSM when we project out a document.",
            " The recordId metadata is different though, because it's a fundamental part of the WSM and",
            " we store it within the WSM itself rather than WSM metadata, so we need to transfer it into",
            " the metadata object if the projection has a recordId $meta expression.",
            " If the projection doesn't need any metadata, then we'll just apply the projection to the",
            " input document. Otherwise, before applying the projection, we will move document metadata",
            " from the WSM into the document itself, and will move it back to the WSM once the projection",
            " has been applied.",
            " An exclusion projection can return an unowned object since the output document is",
            " constructed from the input one backed by BSON which is owned by the storage system, so we",
            " need to  make sure we transition an owned document.",
            " This process is specific to the SIMPLE_DOC database type, where we need to handle",
            " metadata and recordId separately to ensure accurate data storage and retrieval."
        ],
        "FuncName": "projection_2237 transform.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Most metadata should have already been stored within the WSM when we project out a document. This is a key concept in database management, where metadata is crucial for data integrity and consistency.",
            " The recordId metadata is different though, because it's a fundamental part of the WSM and we store it within the WSM itself rather than WSM metadata, so we need to transfer it into the metadata object if the projection has a recordId $meta expression. In database terms, this is a unique identifier for each record.",
            " If the projection doesn't need any metadata, then we'll just apply the projection to the input document. Otherwise, before applying the projection, we will move document metadata from the WSM into the document itself, and will move it back to the WSM once the projection has been applied. This is a common operation in database systems, where data is transformed and reorganized.",
            " An exclusion projection can return an unowned object since the output document is constructed from the input one backed by BSON which is owned by the storage system, so we need to make sure we transition an owned document. In database terms, this is a process of data migration and transformation."
        ],
        "FuncName": "projection_2237 transform.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Most metadata should have already been stored within the WSM when we project out a document.",
            " The recordId metadata is different though, because it's a fundamental part of the WSM and",
            " we store it within the WSM itself rather than WSM metadata, so we need to transfer it into",
            " the metadata object if the projection has a recordId $meta expression.",
            " If the projection doesn't need any metadata, then we'll just apply the projection to the",
            " input document. Otherwise, before applying the projection, we will move document metadata",
            " from the WSM into the document itself, and will move it back to the WSM once the projection",
            " has been applied.",
            " An exclusion projection can return an unowned object since the output document is",
            " constructed from the input one backed by BSON which is owned by the storage system, so we",
            " need to  make sure we transition an owned document.",
            " In relational databases like MySQL, PostgreSQL, or SQL Server, a projection is used to select",
            " specific columns from a table, similar to how a WHERE clause is used to filter rows.",
            " In MongoDB, a projection is used to select specific fields from a document, similar to how",
            " a SELECT statement is used to select specific columns from a table.",
            " The projection process involves applying a set of rules to the input data to produce the",
            " desired output data, which is then stored in the database."
        ],
        "FuncName": "projection_2237 transform.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    " Skip the index virtual field, as it's not part of the actual index key. (Similar to how index keys work in databases.)",
                    " Skip hashed index fields. Rehydrating of index keys is used for covered projections in databases.",
                    " Rehydrating of hashed field value is pointless on its own. The query planner dependency analysis should make sure that a covered projection can only be generated for non-hashed fields in databases.",
                    " Index keys in databases are used to improve query performance by allowing for faster lookup and retrieval of data.",
                    " Covered projections in databases allow for the retrieval of only the necessary fields from an index key, reducing data transfer and processing time."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "projection_2237 rehydrateIndexKey.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " Push an unused value on the back to keep _includeKey and _keyFieldNames in sync with projection stage.",
                    " Projection stage in sync ."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            " Loop condition is not applicable for projection stage."
        ],
        "Normal": [
            " Projection stage: If we're pulling data out of one index we can pre-compute the indices of the fields in the key that we pull data from and avoid looking up the field name each time.",
            " Projection stage Sanity-check ."
        ],
        "FuncName": "projection_2237 ProjectionStageCovered.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "There is also a chance the state got cleared or frozen, in which case we should keep the state as 'kCleared' or 'kFrozen' and update the bucket status to 'Normal' in the database."
        ],
        "FuncName": "bucket_state_registry_3063 unprepareBucketState.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Unsigned right shift. This operation is used in database indexing to quickly move data to the correct position.",
            "Signed shift. We also know that the highest bit is set, indicating a negative number, which is a common scenario in database queries."
        ],
        "FuncName": "sljitNativeARM_64_5380 logical_imm.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u8fd9\u4e9b\u7ed3\u6784\u662f\u6309\u5730\u5740\u6392\u5e8f\u7684\u3002\u673a\u5668\u4ee3\u7801\u751f\u6210\u662f\u901a\u8fc7\u89e3\u91ca\u5668\u751f\u6210\u7684\u4ee3\u7801\u3002"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "sljitNativeARM_64_5380 sljit_generate_code.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If we are currently writing directly to it, we cannot initialize the bucket to a normal state.",
                    "Normal state."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If the target Era is older than the registry Era, return a write conflict error, or if 'bucket' is cleared."
        ],
        "FuncName": "bucket_state_registry_3063 initializeBucketState.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "We cannot release the pending direct write bucket status. This is because the bucket status is still in a pending state and cannot be released until the direct write is complete."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "A negative value indicates that the bucket status should be removed immediately after direct write is completed. This is because the bucket status is no longer needed and can be safely removed."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "bucket_state_registry_3063 stopTrackingBucketState.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If we are initiating a direct write, we need to advance the era. This allows us to synchronize with reopening attempts that do not directly observe a state with direct write counter, but which nevertheless may be trying to reopen a stale bucket. Direct write counter is used to track the number of direct writes performed on a bucket.",
                    "We can perform direct writes on buckets not being tracked by the registry. Tracked by a negative value to signify we must delete the state from the 'registry' when the counter reaches 0. This is done to prevent stale data from being written to the bucket."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Frozen buckets are safe to receive direct writes. Cannot perform direct writes on prepared buckets. Direct write counter is incremented for each direct write operation on a frozen bucket.",
                    "Direct write counter is used to track the number of direct writes performed on a frozen bucket. This allows us to determine when a frozen bucket is no longer safe to receive direct writes."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Convert the direct write counter to a negative value so we can interpret it as an untracked state when the counter goes to 0. This is done to prevent stale data from being written to the bucket."
        ],
        "FuncName": "bucket_state_registry_3063 addDirectWrite.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Loading immediate values into registers, as per database code instructions.",
            " A large amount of number can be constructed from ORR and MOVx instructions, but computing them is costly in terms of database operations."
        ],
        "FuncName": "sljitNativeARM_64_5380 load_immediate.txt"
    },
    {
        "Branch": [
            "To update the bucket's state to prepared, check for conflicts or pending operations."
        ],
        "Loop": [],
        "Normal": [
            " We cannot update the bucket if it is in a cleared state or has a pending direct write. This is due to the bucket's database type restrictions.",
            " We cannot prepare an already prepared bucket. This is because the bucket's database type does not allow for multiple preparations."
        ],
        "FuncName": "bucket_state_registry_3063 prepareBucketState.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Both operands are registers. This is a typical scenario in database operations where registers are used as operands for unary operations such as moving data from one register to another."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "sljitNativeARM_64_5380 sljit_emit_op1.txt"
    },
    {
        "Branch": [
            "This string is used to generate stack frame release code in the database."
        ],
        "Loop": [
            "This string is used to generate stack frame release code in the database."
        ],
        "Normal": [
            "This and the next call/jump instruction can be executed parallelly.",
            "In the context of the database, this instruction is used to release stack frames."
        ],
        "FuncName": "sljitNativeARM_64_5380 emit_stack_frame_release.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Verify command execution status in the shard database.",
            "Check if the command execution failed in the shard database.",
            "Determine if the write concern was successful in the database."
        ],
        "FuncName": "shard_4898 getEffectiveStatus.txt"
    },
    {
        "Branch": [
            "Measuring overhead of if-conditions in database."
        ],
        "Loop": [
            {
                "loopstr": [
                    "Measuring overhead of loop conditions in X5Narrow benchmark.",
                    "Measuring overhead of loop conditions in X25Wide benchmark."
                ]
            }
        ],
        "Normal": [
            "Measuring overhead of collecting measurements for database.",
            "Measuring overhead of narrow nodes in database.",
            "Measuring overhead of wide nodes in database.",
            "Measuring overhead of container types in database.",
            "Measuring overhead of overhead estimation in database."
        ],
        "FuncName": "profiler_internal_5488 measureOverhead.txt"
    },
    {
        "Branch": [
            "Quickly enter the if function, generate a quick enter instruction for conditional statements in the database."
        ],
        "Loop": [
            "Quickly enter the loop function, generate a quick enter instruction for loop conditions in the database."
        ],
        "Normal": [
            "Memory, Database memory management and optimization techniques."
        ],
        "FuncName": "sljitNativeARM_64_5380 sljit_emit_fast_enter.txt"
    },
    {
        "Branch": [
            "If the condition is met, the program counter will be updated to the target address."
        ],
        "Loop": [
            "The loop will continue as long as the condition is met."
        ],
        "Normal": [
            " Not all instruction forms support accessing SP register. This is because some instructions do not allow direct access to the stack pointer register for memory addressing."
        ],
        "FuncName": "sljitNativeARM_64_5380 sljit_get_local_base.txt"
    },
    {
        "Branch": [
            "The if condition is used to generate an ijump instruction, which is a conditional jump instruction in the database."
        ],
        "Loop": [
            "The loop condition is used to generate an ijump instruction, which is a conditional jump instruction in the database."
        ],
        "Normal": [
            "These jumps are converted to jump/call instructions when possible. In the database, this means generating an ijump instruction with a specific opcode."
        ],
        "FuncName": "sljitNativeARM_64_5380 sljit_emit_ijump.txt"
    },
    {
        "Branch": [
            "Retry execution configuration snippet for exhaustive search."
        ],
        "Loop": [],
        "Normal": [
            "Do not allow exhaustive searches to be executed against regular databases."
        ],
        "FuncName": "shard_4898 exhaustiveFindOnConfig.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "No need for a loop. In database terms, this means the condition is always true, resulting in no data being retrieved or modified."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "This string is related to database loops, where the condition is used to iterate over a dataset. In database terms, this means the loop will continue until the condition is met, allowing for efficient data processing."
        ],
        "Normal": [
            "The local_size does not include saved registers size. In database terms, this refers to the storage space allocated for local variables and registers, which is not included in the overall size calculation."
        ],
        "FuncName": "sljitNativeARM_64_5380 sljit_emit_enter.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Cycles measured for a node include overhead of all the children, and half of overhead from the node itself, which is a characteristic of the call tree context.",
            "of the node itself. This is because each elapsed time measurement includes half of the overhead of the measurement itself, which is a common pattern in call tree analysis.",
            "So if we subtract totalOverheadCycles then we will subtract all the children, but we will also subtract all of node overhead, while we should only subtract half of it, as per the call tree context rules."
        ],
        "FuncName": "profiler_internal_5488 visit.txt"
    },
    {
        "Branch": [
            "Connect to a node before executing the if condition."
        ],
        "Loop": [
            "Establish a connection to a node before checking the loop condition."
        ],
        "Normal": [
            "Note that creating a Replica will result in a network call to the database server."
        ],
        "FuncName": "replication_manager_9985 NodeConnect.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Return if the collection is already capped with the given UUID.",
                    "Check if a previous execution left an existing temporary collection with the given targetUUID.",
                    "In that case, drop the temporary collection to proceed with the creation of a new one.",
                    "This is a MongoDB-specific operation to ensure data consistency and integrity."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Generate a temporary collection name that does not collide with any existing collections in the MongoDB database."
        ],
        "FuncName": "capped_utils_8516 convertToCapped.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Note write commands can only be issued against a primary database type."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "shard_4898 _submitBatchWriteCommand.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "4x is for padding, power of 2, etc...  In MongoDB, capped collections are a type of collection that has a fixed maximum size and automatically removes the oldest documents when the collection reaches its maximum size.",
                    "The snapshot has changed. Fetch the document again from the collection in order to check whether it has been deleted. This is because MongoDB uses snapshot isolation for capped collections, which means that only one thread can access the collection at a time.",
                    "Doc was deleted so don't clone it. In MongoDB, when a document is deleted, it is not immediately removed from the collection. Instead, it is marked as deleted and remains in the collection until it is overwritten or the collection is compacted."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "The snapshot has changed. Fetch the document again from the collection in order to check whether it has been deleted. This is because MongoDB uses snapshot isolation for capped collections, which means that only one thread can access the collection at a time.",
                    "4x is for padding, power of 2, etc...  In MongoDB, capped collections are a type of collection that has a fixed maximum size and automatically removes the oldest documents when the collection reaches its maximum size."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "In MongoDB, when a document is deleted, it is not immediately removed from the collection. Instead, it is marked as deleted and remains in the collection until it is overwritten or the collection is compacted. Doc was deleted so don't clone it."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "create new collection  In MongoDB, a collection is a group of documents that can be stored in a single file. Creating a new collection is a common operation in MongoDB.",
            "we created above  In MongoDB, you can create a collection using the db.createCollection() method. This method takes the name of the collection as an argument.",
            "how much data to ignore because it won't fit anyway  In MongoDB, you can use the db.collection.stats() method to get statistics about the collection, including the amount of data stored in the collection.",
            "datasize and extentSize can't be compared exactly, so add some padding to 'size'  In MongoDB, the datasize and extentSize are related but not exactly the same thing. Datasize refers to the total amount of data stored in the collection, while extentSize refers to the amount of data stored in a single extent.",
            "non-zero when retrying our last document.  In MongoDB, when a document is deleted, it is not immediately removed from the collection. Instead, it is marked as deleted and remains in the collection until it is overwritten or the collection is compacted."
        ],
        "FuncName": "capped_utils_8516 cloneCollectionAsCapped.txt"
    },
    {
        "Branch": [
            "If condition for database query"
        ],
        "Loop": [
            "Loop condition for database iteration"
        ],
        "Normal": [
            "Outfmt should complete before infmt. This is a database query optimization technique.",
            "Be paranoid - pack_write should never overflow. This is a database data integrity check."
        ],
        "FuncName": "pack_impl_8326 __wt_struct_repack.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "99 is the hard error exit status for automake tests: https://www.gnu.org/software/automake/manual/html_node/Scripts_002dbased-Testsuites.html. In the context of a database, this refers to a specific error handling mechanism where a SIGSEGV signal is triggered, indicating a segmentation fault, and the database resorts to a predefined signal handling function to recover from the error."
        ],
        "FuncName": "x64-unwind-badjmp-signal-frame_5453 main.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Start listening on the given replication port and establish connections with other nodes in the database.",
            "Establish connections with all of the other nodes in the database."
        ],
        "FuncName": "replication_manager_9985 ReplicationManager.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Ensuring column family metadata and having at least one SST file for data retrieval in the database."
        ],
        "FuncName": "c_test_5656 GetAndCheckMetaDataCf.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Expecting user input on the form: authz-id authn-id pwd. Authentication database validation required.",
            "The authentication database is also the source database for the user, storing user credentials and authentication details."
        ],
        "FuncName": "sasl_plain_server_conversation_9352 stepImpl.txt"
    },
    {
        "Branch": [
            "This string is used as a condition for an if statement in database table compression."
        ],
        "Loop": [
            "This string is used as a condition for a loop in database table compression."
        ],
        "Normal": [
            "The remaining argument is the table name. This string is used to describe the table name in database table compression."
        ],
        "FuncName": "util_compact_4621 util_compact.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If there is no metadata (yet), this will return ENOENT. Treat that the same as an empty metadata. In a database context, this means the database does not exist or is not initialized."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Database list: %s\n"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Database list: %s\n"
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Get the database key.",
                    "If a database name is specified, only show databases that match.",
                    "* !!! * Don't report anything about the WiredTiger metadata and history store since they are not*user created objects unless the verbose or checkpoint options are passed in. However,*skip over the metadata system information for anything except the verbose option."
                ]
            }
        ],
        "Normal": [
            "Open the database file."
        ],
        "FuncName": "util_list_3047 list_print.txt"
    },
    {
        "Branch": [
            "load and validate the latest rocksdb options based on if conditions"
        ],
        "Loop": [
            "load and validate the latest rocksdb options based on loop conditions"
        ],
        "Normal": [
            "load the latest rocksdb option and verify the loaded options by opening the db",
            "load and validate the latest rocksdb options for normal execution"
        ],
        "FuncName": "c_test_5656 LoadAndCheckLatestOptions.txt"
    },
    {
        "Branch": [
            "Check if compaction is disabled and filter is enabled",
            "Check if compaction is enabled and filter is disabled",
            "Check if compaction is disabled and filter is disabled"
        ],
        "Loop": [
            "Loop through all blocks and check if compaction is enabled",
            "Loop through all blocks and check if filter is enabled"
        ],
        "Normal": [
            "Disable compaction and check if filter is working as expected",
            "should not filter anything when compaction is disabled",
            "Reenable compaction and check if filter is working as expected",
            "Force compaction and check if filter is working as expected",
            "should have filtered bar, but not foo and check compaction status"
        ],
        "FuncName": "c_test_5656 CheckCompaction.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Due to half true, half false with fake filter result. This is a characteristic of a database where the filter is not reliable."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "For run == 1, block-based Bloom is no longer available in public API; attempting to enable it enables full Bloom instead. This is a configuration option in a database that controls the type of Bloom filter used.",
                    "Essentially a fingerprint of full Bloom schema, format_version=5. This is a specific version of the Bloom filter schema used in the database."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "not supported on all platforms, allow unsupported error. This is an error message that may appear in a database when a certain feature is not supported on a specific platform."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "run=0 uses custom filter (not currently supported). This is a configuration option in a database that uses a custom filter, but it is not currently supported.",
                    "run=1 uses old block-based bloom filter. This is a type of Bloom filter used in a database, which is an old version.",
                    "run=2 uses full bloom filter. This is a type of Bloom filter used in a database, which is a full version.",
                    "run=3 uses Ribbon. This is a feature or configuration option in a database that uses Ribbon.",
                    "run=4 uses Ribbon-Bloom hybrid configuration. This is a type of configuration in a database that combines Ribbon and Bloom filter.",
                    "Create new database. This is an action that can be performed in a database to create a new database.",
                    "Reset the policy. This is an action that can be performed in a database to reset the policy."
                ]
            }
        ],
        "Normal": [
            "Simple check cache usage. This is a basic check in a database to monitor cache usage.",
            "Check memory usage stats. This is a check in a database to monitor memory usage statistics.",
            "Simple sanity check that setting memtable rep works. This is a basic check in a database to ensure that the memory table representation works correctly.",
            "Check that secondary instance works. This is a check in a database to ensure that the secondary instance is working correctly.",
            "Simple sanity check that options setting db_paths work. This is a basic check in a database to ensure that the database paths are set correctly."
        ],
        "FuncName": "c_test_5656 main.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\n",
                    "File snapshot information: ",
                    "file-size: , checkpoint-size: , offset, size, checksum"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "\t\t\n              file-size: ",
                    ", checkpoint-size: ",
                    "\n\n",
                    "\t\t\n                        offset, size, checksum\n"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " * We may not find any checkpoints for this file, in which case we don't report an error, and * continue our caller's loop. Otherwise, read the list of checkpoints and print each * checkpoint's name and time. ",
            " We need the allocation size for decoding the checkpoint addr ",
            " TODO this is a kludge: fix ",
            " Find the longest name, so we can pretty-print. ",
            " Snapshot list: "
        ],
        "FuncName": "util_list_3047 list_print_checkpoint.txt"
    },
    {
        "Branch": [
            "The if condition should be evaluated based on the database query result, ensuring data consistency and integrity."
        ],
        "Loop": [
            "The loop condition should be optimized for database performance, reducing unnecessary iterations and improving query efficiency."
        ],
        "Normal": [
            "The config variable should be set and not NULL, but Coverity is convinced otherwise. This is an infrequent code path. Just add this extra conditional to make it happy. Additionally, consider indexing the relevant database column to improve query performance."
        ],
        "FuncName": "util_list_3047 list_init_block.txt"
    },
    {
        "Branch": [
            "Check if the condition is met in the database"
        ],
        "Loop": [
            "Iterate over the database records"
        ],
        "Normal": [
            "Perform a database operation"
        ],
        "FuncName": "bplus_tree_log_entry_4932 rollback.txt"
    },
    {
        "Branch": [
            "Check if database condition is met to rollback update"
        ],
        "Loop": [
            "Loop through database records to rollback update"
        ],
        "Normal": [
            " do nothing",
            "Perform database rollback update"
        ],
        "FuncName": "bplus_tree_log_entry_4932 rollback.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: A conditional statement in database code that checks a condition and executes a specific action if the condition is met."
        ],
        "Loop": [
            "Loop-Condition-Str: A loop in database code that repeats a set of actions until a certain condition is met."
        ],
        "Normal": [
            "Normal-Str: do nothing, but in database code, this could be a query that retrieves data from a database table."
        ],
        "FuncName": "bplus_tree_log_entry_4932 rollback.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Set up geng argument list. The 0-th argument is the command name. There must be a NULL at the end. This example is for connected bipartite graphs of order 10 and maximum degree at most 4. This is a description of a bipartite graph in a database.",
            "Number of graphs = %lu. This value represents the total count of graphs stored in the database."
        ],
        "FuncName": "callgeng_9534 main.txt"
    },
    {
        "Branch": [
            "This will be called if the condition is true. Graph count: 0."
        ],
        "Loop": [
            "This will be called for each graph. Graph count: 0."
        ],
        "Normal": [
            "This will be called for each graph. Graph count: 0."
        ],
        "FuncName": "callgeng_9534 OUTPROC.txt"
    },
    {
        "Branch": [
            "Allow timed writes on timed tables if the server is in read-only mode."
        ],
        "Loop": [
            "Check if the server is in read-only mode before allowing timed writes on timed tables."
        ],
        "Normal": [
            "Magic restore may need to perform untimed writes on timed tables as a part of",
            "the server automated restore procedure.",
            "Ignore times in selective restore mode.",
            "We can safely ignore setting this configuration option when recovering from the",
            "oplog as standalone because:",
            "1. Replaying oplog entries write with a time.",
            "2. The instance is put in read-only mode after oplog application has finished."
        ],
        "FuncName": "wiredtiger_begin_transaction_block_4011 allowUntimestampedWrites.txt"
    },
    {
        "Branch": [
            "Rollback update root page log entry processor for IF condition"
        ],
        "Loop": [
            "Rollback update root page log entry processor for LOOP condition"
        ],
        "Normal": [
            "Rollback update root page log entry processor for normal operation: do nothing"
        ],
        "FuncName": "bplus_tree_log_entry_4932 rollback.txt"
    },
    {
        "Branch": [
            {
                "description": "Conditional statement for if-else scenarios in database transactions",
                "value": "not to compile default"
            }
        ],
        "Loop": [
            {
                "description": "Looping condition for iterative database transactions",
                "value": "not to compile default"
            }
        ],
        "Normal": [
            {
                "description": "Normal database transaction string for default scenarios",
                "value": ""
            }
        ],
        "FuncName": "wiredtiger_begin_transaction_block_4011 makeCompiledConfigurations.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Check if the database connection is established successfully."
        ],
        "Loop": [
            "Loop-Condition-Str: Iterate through the database records to perform the update operation."
        ],
        "Normal": [
            "Normal-Str: do nothing, no update operation required."
        ],
        "FuncName": "bplus_tree_log_entry_4932 rollback.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: A conditional statement in database query processing, used to evaluate a condition and execute a specific action if the condition is met."
        ],
        "Loop": [
            "Loop-Condition-Str: A loop control statement in database query processing, used to execute a set of instructions repeatedly until a specified condition is met."
        ],
        "Normal": [
            "Normal-Str: A default statement in database query processing, used to perform a specific action when no condition is met. In this case, the action is to do nothing."
        ],
        "FuncName": "bplus_tree_log_entry_4932 rollback.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Conditional statement in assembly code"
        ],
        "Loop": [
            "Loop-Condition-Str: Loop condition in assembly code"
        ],
        "Normal": [
            "Normal-Str: Normal assembly code instruction",
            "six nops ;",
            "four nops ;"
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 patchNopToCall.txt"
    },
    {
        "Branch": [
            "Open table scan physical operator to fetch chunk scanner, add all columns and filter columns"
        ],
        "Loop": [
            "Open table scan physical operator to fetch chunk scanner, add all columns and filter columns"
        ],
        "Normal": [
            "Open table scan physical operator to fetch chunk scanner, add all columns and filter columns, TODO: dont need to fetch all columns from record manager"
        ],
        "FuncName": "table_scan_vec_physical_operator_1371 open.txt"
    },
    {
        "Branch": [
            "Allocate space for conditional jump and store address for later patch."
        ],
        "Loop": [
            "Allocate space for loop condition jump and store address for later patch."
        ],
        "Normal": [
            "Allocate space which will be patched by patchFarJump. This space will be used for storing address of a label or function."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 farJumpWithPatch.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Used in database code for conditional jumps.",
            "If-Condition-Str: Specifies the condition for a jump instruction.",
            "If-Condition-Str: Database code uses this to control program flow."
        ],
        "Loop": [
            "Loop-Condition-Str: Used in database code for loop iterations.",
            "Loop-Condition-Str: Specifies the condition for a loop to repeat.",
            "Loop-Condition-Str: Database code uses this to control loop execution."
        ],
        "Normal": [
            "Normal-Str: six nops ; - A sequence of six no-operation instructions.",
            "Normal-Str: four nops ; - A sequence of four no-operation instructions.",
            "Normal-Str: Database code often uses nops for padding or alignment."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 patchCallToNop.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " MIPS32 Instruction Set Architecture",
            " Load Upper Immediate (lui)",
            " Or Immediate (ori)",
            " Jump Register (jalr)",
            " Or Immediate (ori)",
            " Jump Register (jalr)"
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 nopPatchableToCall.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "In IEEE 754 double precision floating point representation, -0 and 0 are distinct values. The maximum value is 1.7976931348623157e+308 and the minimum value is -1.7976931348623157e+308. The statement '-0 + -0 = -0 and -0 + 0 = 0' highlights the behavior of floating point operations with negative zero."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "In IEEE 754 double precision floating point representation, NaN (Not a Number) is a special value that is not equal to any other number, including itself. The statement 'First or second is NaN, result is NaN' indicates that when either operand is NaN, the result of the operation is also NaN.",
            "In floating point arithmetic, -0 and 0 are treated as distinct values. The statement 'Make sure we handle -0 and 0 right' emphasizes the importance of correctly handling these special values in numerical computations.",
            "Checking for zero is crucial in floating point operations, as it can affect the result of calculations. The statement 'Check for zero' highlights the need for this check.",
            "When both operands are -0 or 0, the maximum or minimum value is determined based on the specific operation being performed. The statement 'So now both operands are either -0 or 0' sets the stage for this determination.",
            "In floating point arithmetic, the maximum and minimum values are defined as 1.7976931348623157e+308 and -1.7976931348623157e+308, respectively. The statement 'First is 0 or -0, move max/min to it, else just return it' describes the process of determining the maximum or minimum value based on the operands."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 minMaxDouble.txt"
    },
    {
        "Branch": [
            "Allocate space which will be patched by patchCall, patching if condition"
        ],
        "Loop": [
            "Allocate space which will be patched by patchCall, patching loop condition"
        ],
        "Normal": [
            "Allocate space which will be patched by patchCall"
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 callWithPatch.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Saturating callouts don't use ool path. For integer overflow, a trap is triggered in database code."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "Integer truncation may occur during database operations. A reconnect point is triggered in case of overflow."
        ],
        "Normal": [
            "Only possible valid input that produces INT64_MIN result. This may lead to overflow in database operations, triggering a trap."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 outOfLineWasmTruncateToInt64Check.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The maximum value of a 32-bit floating-point number is 3.4028234663852886e+38. -0 + -0 = -0 and -0 + 0 = 0."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "The minimum value of a 32-bit floating-point number is 1.175494351E-38. First or second is NaN, result is NaN.",
            "The maximum value of a 32-bit floating-point number is 3.4028234663852886e+38. Make sure we handle -0 and 0 right.",
            "The minimum value of a 32-bit floating-point number is 1.175494351E-38. Check for zero.",
            "The maximum value of a 32-bit floating-point number is 3.4028234663852886e+38. So now both operands are either -0 or 0.",
            "The minimum value of a 32-bit floating-point number is 1.175494351E-38. First is 0 or -0, move max/min to it, else just return it."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 minMaxFloat32.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Jump out in case of negative zero. This is because in some databases, negative zero is not a valid value."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "Compute x % (1<<y) - 1 for a known constant, y. This is a common operation in some database queries."
        ],
        "Normal": [
            "MATH:",
            "We wish to compute x % (1<<y) - 1 for a known constant, y.",
            "First, let b = (1<<y) and C = (1<<y)-1, then think of the 32 bit",
            "dividend as a number in base b, namely",
            "c_0*1 + c_1*b + c_2*b^2 ... c_n*b^n",
            "now, since both addition and multiplication commute with modulus,",
            "x % C == (c_0 + c_1*b + ... + c_n*b^n) % C ==",
            "(c_0 % C) + (c_1%C) * (b % C) + (c_2 % C) * (b^2 % C)...",
            "now, since b == C + 1, b % C == 1, and b^n % C == 1",
            "this means that the whole thing simplifies to:",
            "c_0 + c_1 + c_2 ... c_n % C",
            "each c_n can easily be computed by a shift/bitextract, and the modulus",
            "can be maintained by simply subtracting by C whenever the number gets",
            "over C.",
            "hold holds -1 if the value was negative, 1 otherwise.",
            "remain holds the remaining bits that have not been processed",
            "SecondScratchReg serves as a temporary location to store extracted bits",
            "into as well as holding the trial subtraction as a temp value dest is",
            "the accumulator (and holds the final result)",
            "move the whole value into the remain.",
            "Zero out the dest.",
            "Set the hold appropriately.",
            "Begin the main loop.",
            "Extract the bottom bits into SecondScratchReg.",
            "Add those bits to the accumulator.",
            "Do a trial subtraction",
            "If (sum - C) > 0, store sum - C back into sum, thus performing a",
            "modulus.",
            "Get rid of the bits that we extracted before.",
            "If the shift produced zero, finish, otherwise, continue in the loop.",
            "Check the hold to see if we need to negate the result.",
            "If the hold was non-zero, negate the result to be in line with",
            "what JS wants. This is a common operation in some database queries."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 ma_mod_mask.txt"
    },
    {
        "Branch": [
            " If Nan, 0 or -0 check for bailout, considering IEEE 754 floating-point representation in database."
        ],
        "Loop": [],
        "Normal": [
            " If binary value is not zero, it is NaN or -0, so we bail, taking into account database's floating-point operations.",
            " Input was zero, so return zero, following database's rounding rules for 32-bit integers."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 floorFloat32ToInt32.txt"
    },
    {
        "Branch": [
            "Execute WASM instruction based on memory access description"
        ],
        "Loop": [
            "Load memory based on WASM instruction execution"
        ],
        "Normal": [
            "Load memory with offset."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 wasmLoadImpl.txt"
    },
    {
        "Branch": [
            "Check if src is in -1; -0 range by checking the sign bit.",
            "Truncate a double-precision floating-point number to a 32-bit integer."
        ],
        "Loop": [],
        "Normal": [
            "Check if src is in -1; -0 range by checking the sign bit."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 truncDoubleToInt32.txt"
    },
    {
        "Branch": [
            "Check if the condition is true by evaluating the expression."
        ],
        "Loop": [
            "Determine the number of iterations by evaluating the loop condition."
        ],
        "Normal": [
            "Check if src is in -1; -0 range by checking the sign bit. This is a specific operation in the database, where the sign bit is used to determine the range of the floating-point number."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 truncFloat32ToInt32.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Load biggest integer less than 0.5 in the temp register.",
            " Branch to a slow path for negative inputs. Doesn't catch NaN or -0.",
            " If Nan, 0 or -0 check for bailout in floating-point operations.",
            " If binary value is not zero, it is NaN or -0, so we bail in integer conversion.",
            " Input was zero, so return zero as integer.",
            " Input is negative, but isn't -0, requiring special handling for database integer representation.",
            " Inputs in -0.5; 0 need to be added 0.5, other negative inputs need to be added the biggest integer less than 0.5.",
            " If input + 0.5 >= 0, input is a negative number >= -0.5 and the result is -0, affecting database integer storage.",
            " Truncate and round toward zero, ensuring database integer consistency.",
            " This is off-by-one for everything but integer-valued inputs, impacting database data integrity."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 roundFloat32ToInt32.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " If x < -1 or x > 0 then perform ceil. This operation is commonly used in database queries to round up floating-point numbers to the nearest integer.",
            " If binary value is not zero the input was not 0 so we bail. This condition is often used in database operations to check for non-zero values.",
            " Input was zero so return zero. This result is typical when working with floating-point numbers in a database context, where zero is a common value."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 ceilFloat32ToInt32.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " If x < -1 or x > 0 then perform ceil: This is a condition used in database code to determine when to round up a double-precision floating-point number to a 32-bit integer.",
            " If high part is not zero, the input was not 0, so we bail: In database context, this condition checks if the high part of a double-precision number is non-zero, indicating that the number is not zero, and thus requires special handling.",
            " Input was zero, so return zero: In database code, this statement returns zero when the input is zero, indicating a special case where no rounding is needed."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 ceilDoubleToInt32.txt"
    },
    {
        "Branch": [
            "If NaN, 0 or -0 check for bailout: This condition is used to determine if the input is NaN, 0 or -0, which would result in bailout.",
            "If high part is not zero, it is NaN or -0, so we bail.: This condition checks if the high part of the input is not zero, indicating NaN or -0, and thus bailing out.",
            "Input was zero, so return zero.: This condition returns zero as the result when the input is zero."
        ],
        "Loop": [],
        "Normal": [
            "Convert double-precision floating-point number to 32-bit integer: This process involves converting a double-precision floating-point number to a 32-bit integer, which may involve rounding or truncation.",
            "Double-precision floating-point number to 32-bit integer conversion: This conversion process is specific to the database type and involves mapping the double-precision floating-point number to a 32-bit integer value.",
            "32-bit integer conversion from double-precision floating-point number: This conversion may involve rounding or truncation of the double-precision floating-point number to fit into a 32-bit integer."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 floorDoubleToInt32.txt"
    },
    {
        "Branch": [
            "Load biggest number less than 0.5 in the temp register. (Database: Float32 Truncation)",
            "Branch to a slow path for negative inputs. Doesn't catch NaN or -0. (Database: Float32 Truncation)",
            "If Nan, 0 or -0 check for bailout (Database: Float32 Truncation)",
            "If high part is not zero, it is NaN or -0, so we bail. (Database: Float32 Truncation)",
            "Input was zero, so return zero. (Database: Float32 Truncation)",
            "Input is negative, but isn't -0. (Database: Float32 Truncation)",
            "Inputs in -0.5; 0 need to be added 0.5, other negative inputs need to be added the biggest double less than 0.5. (Database: Float32 Truncation)",
            "If input + 0.5 >= 0, input is a negative number >= -0.5 and the result is -0. (Database: Float32 Truncation)",
            "Truncate and round toward zero. (Database: Float32 Truncation)",
            "This is off-by-one for everything but integer-valued inputs. (Database: Float32 Truncation)"
        ],
        "Loop": [],
        "Normal": [
            "Load biggest number less than 0.5 in the temp register.",
            "Branch to a slow path for negative inputs. Doesn't catch NaN or -0.",
            "If Nan, 0 or -0 check for bailout",
            "If high part is not zero, it is NaN or -0, so we bail.",
            "Input was zero, so return zero.",
            "Input is negative, but isn't -0.",
            "Inputs in -0.5; 0 need to be added 0.5, other negative inputs need to be added the biggest double less than 0.5.",
            "If input + 0.5 >= 0, input is a negative number >= -0.5 and the result is -0.",
            "Truncate and round toward zero.",
            "This is off-by-one for everything but integer-valued inputs."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 roundDoubleToInt32.txt"
    },
    {
        "Branch": [
            "If the condition is true, implement the WASM instruction as a memory access."
        ],
        "Loop": [
            "During each iteration, check if the condition is true and implement the WASM instruction as a memory access."
        ],
        "Normal": [
            " Maybe add the offset. Only the last emitted instruction is a memory access. This instruction is a memory access."
        ],
        "FuncName": "MacroAssembler-mips-shared_7476 wasmStoreImpl.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "The OperationContext executing shutdown is immune from interruption. This context is now closed and cannot be interrupted by any database operations."
        ],
        "FuncName": "operation_context_234 setIsExecutingShutdown.txt"
    },
    {
        "Branch": [
            "Only target the client with the specified connection number. This is a conditional statement in database code, used to filter data based on a specific condition.",
            "Return true with (approx) probability p = chance.  Recall: 0 <= chance <= 1. This is a conditional probability statement in database code, used to determine the likelihood of a certain outcome."
        ],
        "Loop": [],
        "Normal": [
            "Only target the client with the specified connection number.",
            "Return true with (approx) probability p = chance.  Recall: 0 <= chance <= 1."
        ],
        "FuncName": "operation_context_234 opShouldFail.txt"
    },
    {
        "Branch": [
            "Assert the condition of If-Statement in database"
        ],
        "Loop": [
            "Assert the condition of Loop in database"
        ],
        "Normal": [
            "Perform Add/Remove operations on inputs and finalize after each operation in database"
        ],
        "FuncName": "sbe_removable_min_max_test_5447 runAndAssertExpression.txt"
    },
    {
        "Branch": [
            "Set operation key for if condition"
        ],
        "Loop": [
            "Set operation key for loop condition"
        ],
        "Normal": [
            "Set operation key for normal condition: Only set the opKey once"
        ],
        "FuncName": "operation_context_234 setOperationKey.txt"
    },
    {
        "Branch": [
            "Check if database is connected"
        ],
        "Loop": [
            "Loop through database records"
        ],
        "Normal": [
            "NOLINT: Database query result"
        ],
        "FuncName": "collection_catalog_3574 load.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Nothing to do, avoid calling CollectionCatalog::write. This is a conditional statement in database code."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "collection_catalog_3574 preCommit.txt"
    },
    {
        "Branch": [
            "If-condition statement used in database queries to filter records based on certain conditions, ensuring only relevant data is processed."
        ],
        "Loop": [
            "Loop-condition statement used in database queries to repeatedly execute a set of instructions until a specified condition is met."
        ],
        "Normal": [
            "Create catalog write jobs for all updates registered in this WriteUnitOfWork: This statement generates a list of tasks to be executed in the database, ensuring all updates are processed in a single, atomic operation.",
            "Write all catalog updates to the catalog in the same write to ensure atomicity: This statement ensures that all updates to the catalog are written in a single, atomic operation, preventing partial updates and maintaining data consistency."
        ],
        "FuncName": "collection_catalog_3574 commit.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Collection is valid in all snapshots. Compatible with all read timestamps."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "collection_catalog_3574 isExistingCollectionCompatible.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "It's possible that the system clock used in std::condition_variable::wait_until",
                    "is slightly ahead of the FastClock used in checkForInterrupt. In this case,",
                    "we wait for the condition variable until the specified deadline, treating the operation",
                    "as though it has exceeded its time limit, just as if the FastClock and system clock had agreed.",
                    "This is a common scenario in concurrent programming, where the system clock and a custom",
                    "clock used in the program may not be perfectly synchronized."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "In a loop condition, we continuously wait for the condition variable until the specified deadline.",
            "This is a common pattern in concurrent programming, where we repeatedly check for a condition",
            "until it is met or a deadline is reached."
        ],
        "Normal": [
            "If the maxTimeNeverTimeOut failpoint is set, we behave as though the operation's deadline does not exist.",
            "Under normal circumstances, if the op has an existing deadline which is sooner than the deadline",
            "passed into this method, we replace our deadline with the op's. This means that we expect to time out",
            "at the same time as the existing deadline expires. If, when we time out, we find that the op's deadline",
            "has not expired (as will always be the case if maxTimeNeverTimeOut is set) then we assume that the",
            "incongruity is due to a clock mismatch and return _timeoutError regardless. To prevent this behaviour,",
            "only consider the op's deadline in the event that the maxTimeNeverTimeOut failpoint is not set.",
            "This ensures that the operation's deadline is respected, even if there is a clock mismatch between the",
            "system clock and the custom clock used in the program."
        ],
        "FuncName": "operation_context_234 waitForConditionOrInterruptNoAssertUntil.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If the collection does not exist in the database, we must recreate it from the latest available snapshot.",
                    "Otherwise, we only verify that the collection is valid for the specified timestamp and conforms to the database schema."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "There is no need to open the collection if it has already been previously instantiated and is in a consistent state with the database."
        ],
        "FuncName": "collection_catalog_3574 _needsOpenCollection.txt"
    },
    {
        "Branch": [
            "We check if the condition is met to establish a consistent collection for database code."
        ],
        "Loop": [
            "We iterate over both committed and uncommitted changes in the database code to validate them with the storage snapshot."
        ],
        "Normal": [
            "We iterate both already committed and uncommitted changes and validate them with the storage snapshot",
            "Database code requires a consistent collection to function properly, so we validate all changes against the storage snapshot to ensure integrity."
        ],
        "FuncName": "collection_catalog_3574 establishConsistentCollections.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "We shouldn't receive kUnknown when we don't have a timestamp since no timestamp means PIT",
                    "directory entry is up-to-date, we're operating on the latest.",
                    "Scan durable catalog when we don't have accurate catalogId mapping for this timestamp in the database."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "If no entry is found or the entry contains a different namespace, the mapping might be",
                    "incorrect since it is incomplete after startup; scans durable catalog to confirm the PIT directory entry."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "collection_catalog_3574 _fetchPITCatalogEntry.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Usually, CappedSnapshots must be established before opening the storage snapshot. Thus, these strings are related to database code.",
                    "the lookup must be done from the in-memory catalog. It is possible that the required",
                    "CappedSnapshot was not properly established when this operation was collection creation,",
                    "because a Collection instance was not found in the in-memory catalog.",
                    "This can only be the case with concurrent collection creation (MODE_IX), and it is",
                    "semantically correct to establish an empty snapshot, causing the reader to see no",
                    "records. Other DDL ops should have successfully established the snapshot, because a",
                    "Collection must have been found in the in-memory catalog."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "Database loops are used to iterate through collections or documents, and are a fundamental concept in database design."
        ],
        "Normal": [
            "This string is a normal string in the database and does not have any specific meaning related to database operations."
        ],
        "FuncName": "collection_catalog_3574 establishConsistentCollection.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Throw any exception that was caught during execution of our job. Make sure we destroy the exception_ptr on the same thread that throws the exception to avoid a data race between destroying the exception_ptr and reading the exception.",
                    "Queue is empty, store catalog and relinquish responsibility of being worker thread. This is a write operation to update the database state."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Queue is empty, store catalog and relinquish responsibility of being worker thread. This is a write operation to update the database state."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Transfer the jobs we just executed to the completed list. This is a write operation to update the database state.",
                    "Transfer jobs in queue to the pending list. This is a write operation to update the database state."
                ]
            },
            {
                "loopstr": [
                    "Store any exception thrown during job execution so we can notify the calling thread. This is a write operation to update the database error state."
                ]
            }
        ],
        "Normal": [
            "It is potentially expensive to copy the collection catalog so we batch the operations by only having one concurrent thread copying the catalog and executing all the write jobs. Database write operation optimization.",
            "Protecting the two globals above. Database locking mechanism for concurrent access.",
            "Current batch of jobs to execute. Database job queue management.",
            "Implementation for thread with worker responsibility below, only one thread at a time can be in here. Keep track of completed jobs so we can notify them when we've written back the catalog to storage. Database job completion tracking.",
            "Hold onto base so if we need to delete it we can do it outside of the lock. Database resource management.",
            "Copy the collection catalog, this could be expensive, but we will only have one pending collection in flight at a given time. Database catalog management.",
            "Execute jobs until we drain the queue. Database job execution and queue management."
        ],
        "FuncName": "collection_catalog_3574 write.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " When attempting to open the latest collection from the database, we need to account for the possibility of a rename operation concurrent with this call. If we performed this lookup by UUID, we could be in a case where we're looking up concurrently with a rename with dropTarget=true, where the UUID that we use is the target that got dropped. If that rename has committed, we need to put the correct collection under open collection for this namespace. We can detect this case by comparing the catalogId with what is pending for this namespace."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " There are two types of rename depending on the dropTarget flag. For a regular rename of the same logical collection with dropTarget=false, have the same UUID and catalogId for the two choices. In this case, we need to store entries under open collections for two namespaces (rename 'from' and 'to') so we can make sure lookups by UUID are supported and will return a Collection with its namespace in sync with the storage snapshot."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " When there is a rename with dropTarget=true, the two possible choices for the collection we need to observe are different logical collections, they have different UUID and catalogId. In this case, storing a single entry in open collections is sufficient. We know that the instance we are looking for must be under 'latestCollection' as we used the catalogId from 'pendingCollection' when fetching durable catalog entry and the namespace in it did not match the namespace for 'pendingCollection' (the rename has not been committed yet)."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " If this is a rename with dropTarget=true and we're looking up with the 'from' UUID before the rename committed, the namespace would correspond to a valid collection that we need to store under open collections."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " If the latest collection doesn't exist, then the pending collection must exist as it's being created in this snapshot. Otherwise, if the latest collection is incompatible with this snapshot, then the change came from an uncommitted update by an operation operating on this snapshot. If both latestCollection and pendingCollection exist, check if their UUID differs in which case this is a rename with dropTarget=true that just committed."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " When openCollection is called with no timestamp, the namespace must be pending commit. We compare the collection instance in _pendingCommitNamespaces and the collection instance in the in-memory catalog with the durable catalog entry to determine which instance to return. At least one of latest and pending should be a valid pointer. If the catalog entry is not found in our snapshot, then the collection is being dropped and we can observe the drop. Lookups by this namespace or UUID should not find a collection. When trying to open the latest collection by namespace and the catalog entry has a different namespace in our snapshot, then there is a rename operation concurrent with this call. When trying to open the latest collection by UUID and the Collection instances has different namespaces, then there is a rename operation concurrent with this call. We need to store entries under uncommitted catalog changes for two namespaces (rename 'from' and 'to') so we can make sure lookups by UUID is supported and will return a Collection with its namespace in sync with the storage snapshot. Use the pendingCollection if there is no latestCollection or if the metadata of the latestCollection doesn't match the durable catalogEntry. If neither `latestCollection` or `pendingCollection` match the metadata, we fully instantiate a new collection instance from durable storage that is guaranteed to match. This can happen when multikey is not consistent with the storage snapshot. We use 'pendingCollection' as the base when available as it might contain an index that is about to be added. Dropped indexes can be found through other means in the drop pending state. This may nullptr if the collection was not instantiated successfully. This is the case when timestamps aren't used (e.g. standalone mode) even though the durable catalog entry was found. When timestamps aren't used, the drop pending reaper immediately drops idents which may be needed to instantiate this collection."
        ],
        "FuncName": "collection_catalog_3574 _openCollectionAtLatestByNamespaceOrUUID.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u5982\u679c\u6570\u636e\u5e93\u4e0d\u5b58\u5728\uff0c\u89c6\u56fe\u4e5f\u4e0d\u5b58\u5728\u3002\u5220\u9664\u89c6\u56fe\u65f6\uff0c\u9700\u8981\u8003\u8651\u6570\u636e\u5e93\u662f\u5426\u5b58\u5728\uff0c\u907f\u514d\u5220\u9664\u4e0d\u5b58\u5728\u7684\u89c6\u56fe\u3002"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "collection_catalog_3574 dropView.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "The implementation of openCollection() is quite different at a timestamp compared to at latest.",
            "Separated the implementation into helper functions to support reading data at a specific time point.",
            "Depending on the input parameters, we call the right helper function to open the collection."
        ],
        "FuncName": "collection_catalog_3574 _openCollection.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Try to find a catalog entry matching 'readTimestamp'. This operation is related to MongoDB database.",
            "Return the in-memory Collection instance if it is compatible with the read timestamp. MongoDB uses timestamp for data consistency.",
            "Use the shared collection state from the latest Collection in the in-memory collection, which is a MongoDB concept.",
            "catalog if it is compatible. MongoDB uses catalog for data management.",
            "There is no state in-memory that matches the catalog entry. Try to instantiate a new Collection instance from scratch. This is a MongoDB database operation."
        ],
        "FuncName": "collection_catalog_3574 _openCollectionAtPointInTimeByNamespaceOrUUID.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Throw any exception that was caught during execution of our job. Make sure we destroy the exception_ptr on the same thread that throws the exception to avoid a data race between destroying the exception_ptr and reading the exception.",
                    "Queue is empty, store catalog and relinquish responsibility of being worker thread. This is a database operation to update the collection catalog."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Queue is empty, store catalog and relinquish responsibility of being worker thread. This is a database operation to update the collection catalog."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Transfer the jobs we just executed to the completed list, Transfer jobs in queue to the pending list. This is a database operation to update the collection catalog and manage job status."
                ]
            },
            {
                "loopstr": [
                    "Store any exception thrown during job execution so we can notify the calling thread. This is a database operation to log and manage exceptions."
                ]
            }
        ],
        "Normal": [
            "It is potentially expensive to copy the collection catalog so we batch the operations by only having one concurrent thread copying the catalog and executing all the write jobs. This is a database operation to optimize catalog copying.",
            "Protecting the two globals above. This is a database operation to manage global variables.",
            "Current batch of jobs to execute. This is a database operation to manage job execution.",
            "Implementation for thread with worker responsibility below, only one thread at a time can be in here. Keep track of completed jobs so we can notify them when we've written back the catalog to storage. This is a database operation to manage thread responsibility and job completion.",
            "Hold onto base so if we need to delete it we can do it outside of the lock. This is a database operation to manage resource deletion.",
            "Copy the collection catalog, this could be expensive, but we will only have one pending collection in flight at a given time. This is a database operation to optimize catalog copying.",
            "Execute jobs until we drain the queue. This is a database operation to manage job execution and queue management."
        ],
        "FuncName": "collection_catalog_3574 write.txt"
    },
    {
        "Branch": [
            "To check if a collection exists in the database, use the 'UncommittedCatalogUpdates' object before 'OpenedCollections'.",
            "This is because in a multi-document transaction, it's allowed to perform a lookup on a non-existent collection followed by creating the collection.",
            "This lookup will store a nullptr in 'OpenedCollections'.",
            "Get any previously instantiated collection on this namespace for this snapshot."
        ],
        "Loop": [],
        "Normal": [
            "It's important to look in UncommittedCatalogUpdates before OpenedCollections because in a",
            "multi-document transaction it's permitted to perform a lookup on a non-existent",
            "collection followed by creating the collection. This lookup will store a nullptr in",
            "OpenedCollections.",
            "Return any previously instantiated collection on this namespace for this snapshot"
        ],
        "FuncName": "collection_catalog_3574 _getCollectionByUUID.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The uncommittedPtr will be nullptr in the case of drop.",
                    "If the collection is newly created, invariant on the collection being locked in MODE_IX.",
                    "This is a database query that retrieves data based on a unique identifier (UUID)."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "thenstr": [
                    "This loop iterates through the database collection to perform a specific operation."
                ],
                "elsestr": []
            }
        ],
        "Normal": [
            {
                "thenstr": [
                    "This is a normal database query that retrieves data without any specific conditions."
                ],
                "elsestr": []
            }
        ],
        "FuncName": "collection_catalog_3574 lookupCollectionByUUIDForMetadataWrite.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If the collection is newly created, invariant on the collection being locked in MODE_IX. This is a database operation to ensure data integrity."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "This operation involves checking if a collection is newly created. If uncommittedPtr is valid, found is always true. Return the pointer as the collection still exists. This is a database operation to ensure data integrity.",
            "This operation involves reporting a drop or rename. Report the drop or rename as nothing new was created. This is a database operation to ensure data integrity."
        ],
        "FuncName": "collection_catalog_3574 lookupCollectionByNamespaceForMetadataWrite.txt"
    },
    {
        "Branch": [
            "Check if the collection is drop pending, not expired, and compatible with the read timestamp based on the database's compatibility rules."
        ],
        "Loop": [
            "Iterate through the collection's namespace and UUID combination to avoid edge cases where the same combination is used to create a new collection after a drop."
        ],
        "Normal": [
            "Verify the collection's existence and compatibility with the read timestamp to ensure data integrity.",
            "Protect against concurrent collection creation by checking for the same namespace and UUID combination.",
            "If the collection is drop pending, not expired, and compatible, return the existing collection.",
            "Otherwise, create a new collection using the shared state to maintain data consistency.",
            "Ensure the 'ident' is unique across collection re-creation to avoid accessing incorrect data.",
            "If the latest or drop pending collection exists, instantiate a new collection using the shared state to prevent data loss."
        ],
        "FuncName": "collection_catalog_3574 _createCompatibleCollection.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Requesting a writable collection normally ensures we have registered PublishCatalogUpdates",
            "with the recovery unit. However, when the writable Collection was requested in Inplace mode",
            "or is the oplog, this is not the case. So make sure we are registered in all cases."
        ],
        "FuncName": "collection_catalog_3574 dropCollection.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " When creating a collection, we update the catalog to reflect the new collection. This ensures that the collection is properly registered and can be accessed by the database."
        ],
        "FuncName": "collection_catalog_3574 onCreateCollection.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "The ident is expired, but it still may not have been dropped by the reaper. Try to mark it as in use. This is a common issue in PIT collections, where expired ident can cause inconsistencies.",
            "Instantiate a new PIT collection without any shared state. PIT collections are used in database systems to manage transactions and ensure data consistency.",
            "Set the ident to the one returned by the ident reaper. This is to prevent the ident from being dropping prematurely, which can lead to data loss in the PIT collection."
        ],
        "FuncName": "collection_catalog_3574 _createNewPITCollection.txt"
    },
    {
        "Branch": [
            "Check if collection object exists in the database before attempting to cancel it."
        ],
        "Loop": [],
        "Normal": [
            "Ensure collection object exists in the database before cancelling it."
        ],
        "FuncName": "collection_catalog_3574 deregisterCollection.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Unfinished index builds return a nullptr for getSharedIdent(). Use getIdent() instead. This behavior is specific to database indexing, where getSharedIdent() is used to retrieve the shared identifier for an index."
        ],
        "FuncName": "collection_catalog_3574 deregisterIndex.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "The caller must have an active storage snapshot for tenant consistency",
            "First get the database names that are not pending commit for tenant consistency",
            "Now iterate over uncommitted list and validate against the storage snapshot for tenant consistency.",
            "Only consider databases we have not seen so far for tenant consistency."
        ],
        "FuncName": "collection_catalog_3574 getAllConsistentDbNamesForTenant.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Move on to the next database after dbName. This operation is specific to relational databases, where databases are organized in a hierarchical structure and can be navigated using SQL queries."
                ]
            }
        ],
        "Normal": [
            " _orderedCollections is sorted by dbName, uuid. upper_bound will return the iterator to the first element in _orderedCollections greater than firstDbName, maxUuid. This functionality is commonly used in NoSQL databases like MongoDB, where collections are stored in a key-value store and can be queried using a query language like MongoDB Query Language (MQL)."
        ],
        "FuncName": "collection_catalog_3574 _iterAllDbNamesHelper.txt"
    },
    {
        "Branch": [
            "To determine the UUID of a namespace string, refer to the [database] code where this string is used.",
            "This string is associated with the UUID of a namespace in the [database] code."
        ],
        "Loop": [
            "When iterating over namespace strings in the [database] code, each string corresponds to a specific UUID.",
            "These UUIDs are used to identify namespaces in the [database] code."
        ],
        "Normal": [
            "It's crucial to check UncommittedCatalogUpdates before OpenedCollections because in a multi-document transaction,",
            "it's allowed to perform a lookup on a non-existent collection followed by creating the collection. This lookup",
            "will store a nullptr in OpenedCollections.",
            "Return any previously instantiated collection on this namespace for this snapshot. In the context of [database] code,",
            "this involves retrieving the UUID associated with the namespace string."
        ],
        "FuncName": "collection_catalog_3574 lookupUUIDByNSS.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "We aim to avoid searching on invalid collection names.",
                    "ApplyOps should work as long as there are valid collections, even with bad views",
                    "Otherwise, the server will crash. The view directory will remain invalid until the bad view definition is deleted."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "collection_catalog_3574 lookupView.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If the parent directory is empty, check the newly submitted collection, which is likely to be the latest",
                    "The newly created collection will be considered the latest"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Any writable collection instance created under MODE_X lock belongs to this",
            "directory instance",
            "Verify that the instance stored in this directory is the same"
        ],
        "FuncName": "collection_catalog_3574 isLatestCollection.txt"
    },
    {
        "Branch": [
            "This stage checks if a certain condition is met in the database."
        ],
        "Loop": [
            "This stage iterates over a collection in the database until a condition is met."
        ],
        "Normal": [
            "This stage does not modify or rename any paths in the database."
        ],
        "FuncName": "document_source_resharding_ownership_match_5056 getModifiedPaths.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If records will not have a null prototype, this should use a mechanism similar to tuples. In database, tuples are used to store multiple values in a single field.",
                    "similar to tuples. In database, tuples are used to store multiple values in a single field."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "RecordTupleShared_299 ExtendedPrimitiveGetProperty.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " We will move stages one by one from the merging half to the shards as possible. This is a typical approach for optimizing pipeline execution in a distributed database system.",
            " The order in which optimizations are applied can have significant impact on the efficiency of the final pipeline. In a relational database management system, this is a crucial consideration for query optimization.",
            " Be Careful! In a big data processing pipeline, this step is critical for ensuring data consistency and integrity in a cloud-based database environment."
        ],
        "FuncName": "split_pipeline_6443 split.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Move the source from the merger _sources to the shard _sources. This operation is related to database shard splitting."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Check if this source is splittable. This involves database table splitting algorithms.",
                    "If the stage doesn't require a split, save it and defer the split point. This decision is based on database optimization techniques.",
                    "We found the split point. This is a key concept in database indexing and query optimization."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "split_pipeline_6443 _findSplitPoint.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Actually propagate the temporary resharding namespace from the recipient. This is a database code string."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "For performance reasons a streaming stage must not keep references to documents across calls to getNext. Such stages must retrieve a result from their child and then release it or return it before asking for another result. Failing to do so can result in extra work since the Document/Value library must copy data on write when that data has a refcount above one. This is related to database query optimization."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "document_source_resharding_ownership_match_5056 doGetNext.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If this stage also would like to split, split here. Don't defer multiple stages.",
                    "The sort that was earlier in the pipeline takes precedence.",
                    "Merge stage: In this stage, we combine the data from multiple sources into a single output."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Check if this source is splittable.",
                    "Move the source from the merger _sources to the shard _sources.",
                    "Database type: This operation is related to the merge stage, which is a key process in database management."
                ]
            }
        ],
        "Normal": [
            "We got to the end of the pipeline or found a split point.",
            "Database type: This is a normal operation that occurs during the merge stage, where data is combined and processed."
        ],
        "FuncName": "split_pipeline_6443 _finishFindSplitPointAfterDeferral.txt"
    },
    {
        "Branch": [
            "BSON parsing: if condition checks.",
            "BSON parsing: if condition checks with tenant ID.",
            "BSON parsing: if condition checks without tenant ID."
        ],
        "Loop": [
            "BSON parsing: loop condition checks.",
            "BSON parsing: loop condition checks with tenant ID."
        ],
        "Normal": [
            "BSON parsing: normal string without tenant ID.",
            "BSON parsing: normal string with tenant ID."
        ],
        "FuncName": "auth_identifier_test_1148 doBSONParseTest.txt"
    },
    {
        "Branch": [
            "Add conditional segment to pipeline, conditional segment is used in database queries to filter records."
        ],
        "Loop": [
            "Add loop segment to pipeline, loop segment is used in database queries to iterate over multiple records."
        ],
        "Normal": [
            "This stage must be split, split it normally. In database terms, this means dividing the data into smaller chunks for processing.",
            "Add in reverse order since we add each to the front and this would flip the order. In database terms, this means reversing the order of the data before adding it to the pipeline.",
            "otherwise. In database terms, this means handling cases where the data does not meet the expected conditions."
        ],
        "FuncName": "split_pipeline_6443 _addSplitStages.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "If this stage is one that can swap with a **Pipeline Limit** stage, then we can look at the previous",
                    "stage to see if it includes a pipeline limit. Otherwise, we give up trying to find a pipeline limit on this",
                    "stage's output. A pipeline limit is a restriction in the **Database Pipeline** that controls the amount of data that can be processed at one time."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "split_pipeline_6443 getPipelineLimit.txt"
    },
    {
        "Branch": [
            "Check if the condition is met in the database"
        ],
        "Loop": [
            "Loop through the database records"
        ],
        "Normal": [
            "Calculate the buffer size in the database"
        ],
        "FuncName": "attribute_value_set_540 create.txt"
    },
    {
        "Branch": [
            "If the condition is met, execute the following code"
        ],
        "Loop": [
            "Loop through the data and perform the following actions"
        ],
        "Normal": [
            "Calculate the buffer size. This is a common operation in database implementations, such as calculating the size of a table or index."
        ],
        "FuncName": "attribute_value_set_540 create.txt"
    },
    {
        "Branch": [
            "Check if the database connection is established"
        ],
        "Loop": [
            "Iterate over the database records"
        ],
        "Normal": [
            "Calculate the buffer size",
            "Store the database connection details"
        ],
        "FuncName": "attribute_value_set_540 create.txt"
    },
    {
        "Branch": [
            "Create an instance of a class that implements the IfCondition interface"
        ],
        "Loop": [
            "Create an instance of a class that implements the LoopCondition interface"
        ],
        "Normal": [
            "Calculate the buffer size",
            "This operation is related to database query optimization and may involve creating an instance of a class that implements the BufferOptimizer interface"
        ],
        "FuncName": "attribute_value_set_540 create.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The bucket is not empty, search among the elements. In a relational database, this could be implemented using a JOIN operation to combine rows from multiple tables based on a common column. Alternatively, in a NoSQL database, this could be achieved using a map-reduce operation to iterate over the elements in the bucket and perform a conditional check."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "First try to find an acquired element. In a database context, this could involve querying a table to retrieve a specific record or row. If the element is not found, try to acquire the value from attribute sets. This could involve querying a separate table or collection to retrieve the necessary attributes or properties."
        ],
        "FuncName": "attribute_value_set_540 find.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "the merge needs all fields, so nothing we can do in database optimization.",
            "Empty project is special in database so if no fields are needed, we just ask for _id instead in database query.",
            "HEURISTIC: only apply database optimization if none of the shard stages have an exhaustive list of field dependencies in database schema.",
            "While this may not be 100% ideal in all cases, database optimization avoids the worst cases by ensuring that:",
            "1) Optimization IS applied when the shards wouldn't have known their exhaustive list of dependencies in database query.",
            "    This situation can happen when a $sort is before the first $project or $group in database pipeline.",
            "    Without the optimization, the shards would have to reify and transmit full objects even though only a subset of fields are needed in database query.",
            "2) Optimization IS NOT applied immediately following a $project or $group since it would add an unnecessary project (and therefore a deep-copy) in database operation.",
            "if we get here, add the project in database query."
        ],
        "FuncName": "split_pipeline_6443 _limitFieldsSentFromShardsToMerger.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Expected last stage on the shards to be a sort",
                    "Sharding sort stage moved to shard data stream"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "split_pipeline_6443 _moveEligibleStreamingStagesBeforeSortOnShards.txt"
    },
    {
        "Branch": [
            "Create a conditional statement to execute a block of code if a condition is met."
        ],
        "Loop": [
            "Create a loop to iterate over a collection of elements and execute a block of code for each item."
        ],
        "Normal": [
            "Create a new object to store data.",
            "Copy all elements from one collection to another."
        ],
        "FuncName": "attribute_value_set_540 copy.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "We have successfully applied a limit to the number of documents we need from each shard.",
                    "shard.",
                    "This is specific to MongoDB's shard key limit."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "The sharding pipeline already has a limit that is no greater than the limit we were going to add, so no changes are necessary.",
                    "This is specific to MongoDB's shard key limit."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "If there are any stages in the merge pipeline before the $skip and $limit stages, then we cannot use the $limit to determine an upper bound, unless those stages could be swapped with the $limit.",
                    "This is specific to MongoDB's aggregation framework."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "split_pipeline_6443 _propagateDocLimitToShards.txt"
    },
    {
        "Branch": [
            "Check if the node exists in the bucket based on the database index."
        ],
        "Loop": [
            "Iterate through the bucket to find the node based on the database schema."
        ],
        "Normal": [
            " All elements within the bucket are sorted to speed up the search.",
            "The search operation is optimized for the database type."
        ],
        "FuncName": "attribute_value_set_540 find_in_bucket.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The attribute is found, acquiring the value from database"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "The attribute is found, acquiring the value from database"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "The attribute is found, acquiring the value from database"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "The attribute is not found in database"
        ],
        "FuncName": "attribute_value_set_540 freeze_node.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The bucket is empty. To insert a new node, we need to allocate a new node and add it to the end of the linked list."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "The new element should become the last element of the bucket. We need to traverse the linked list to find the last node and update its next pointer to point to the new node."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "The new element should become the first element of the bucket. We need to update the head pointer of the linked list to point to the new node."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "attribute_value_set_540 insert_node.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The bucket is not empty, search among the elements. In a database context, this means querying a table with a non-empty result set and retrieving values from a specific column."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "First try to find an acquired element. In a database context, this means executing a SELECT query to retrieve a specific value from a table.",
            "Element not found, try to acquire the value from attribute sets. In a database context, this means joining tables based on common attributes to retrieve the desired value."
        ],
        "FuncName": "attribute_value_set_540 find.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "The thread routine uses the clock source of the first registered scheduler to wait on its condvar so all registered schedulers must use the same clock source."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "alarm_runner_background_thread_7564 _initializeSchedulers.txt"
    },
    {
        "Branch": [
            "Conditional statement in database code, allowing execution of a block of code if a specified condition is met."
        ],
        "Loop": [
            "Looping condition in database code, used to control the repetition of a block of code."
        ],
        "Normal": [
            "In database code, this statement stores our own fmt arg results in a container of std any, and gives reference wrappers to _args. This avoids a string copy of the name inside _args."
        ],
        "FuncName": "plain_formatter_9679 _add.txt"
    },
    {
        "Branch": [
            " Load index from database using If condition."
        ],
        "Loop": [
            " Load index from database using Loop condition."
        ],
        "Normal": [
            " Convert std list to a std vector. Load index from database using Normal query."
        ],
        "FuncName": "shard_key_util_8178 loadIndexes.txt"
    },
    {
        "Branch": [
            "Load index from given collection for If-Condition."
        ],
        "Loop": [
            "Load index from given collection for Loop-Condition."
        ],
        "Normal": [
            "Load index from given collection. This is a standard operation.",
            " Convert std list to a std vector. This operation is typically used after loading the index."
        ],
        "FuncName": "shard_key_util_8178 loadIndexes.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Nothing to trim, stop here. This is a conditional statement in a database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Look for any matching code point to trim. This is a loop condition in a database."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "str_trim_utils_4823 trimFromLeft.txt"
    },
    {
        "Branch": [
            "The if condition createIndexes command."
        ],
        "Loop": [
            "The loop condition createIndexes command."
        ],
        "Normal": [
            "The outer createIndexes command. Create indexes on the specified fields for efficient querying."
        ],
        "FuncName": "shard_key_util_8178 makeCreateIndexesCmd.txt"
    },
    {
        "Branch": [
            "Load index from database: If-Condition-Str"
        ],
        "Loop": [
            "Load index from database: Loop-Condition-Str"
        ],
        "Normal": [
            "Load index from database: Convert std::list to a std::vector."
        ],
        "FuncName": "shard_key_util_8178 loadIndexes.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "this collection is not encrypted, it uses a standard encryption method to protect data"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "shard_key_util_8178 validateShardKeyIsNotEncrypted.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "We can't currently use hashed indexes with a non-default hash seed",
                    "Check iv.",
                    "Note that this means that, for sharding, we only support one hashed index per field per collection.",
                    "In MongoDB, hashed indexes are created using a hash function, which can be customized using a non-default hash seed."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "If no exact match, index not useful, but still possible to create one later",
                    "In MongoDB, an index is not useful if there is no exact match, but it can still be created later for future use."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Check 2.iii Make sure that there is a useful, non-multikey index available.",
                    "In MongoDB, a multikey index is an index that contains multiple keys, which can be useful for certain queries, but may not be suitable for all use cases."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Check 2.i. and 2.ii.",
                    "In MongoDB, it's essential to verify that the proposed key is not already used in another index, and that the new index will not conflict with existing indexes."
                ]
            }
        ],
        "Normal": [
            "1. Verify consistency with existing unique indexes",
            "2. Check for a useful index",
            "3. If proposed key is required to be unique, additionally check for exact match."
        ],
        "FuncName": "shard_key_util_8178 validShardKeyIndexExists.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "createBucketsShardKeyIndexFromTimeseriesShardKeySpec expects the shard key to be buckets-encoded, as it should already be changed to use the control.min.<timeField> if the shard key is not buckets-encoded, the function will return none.",
                    "This means the shard key should be in the format of control.min.<timeField> for timeField-based shard keys."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "4. If no useful index exists, verify if we can create one based on the proposed shard key.",
            "5. If the shard key index is on a buckets namespace, we need to convert the shard key index for FCV > 7.3 versions, otherwise fall back to the original index created.",
            "TODO (SERVER-79304): Remove 'updatedToHandleTimeseriesIndex' once 8.0 becomes last LTS, or update the ticket number to when the parameter can be removed.",
            "6. If no useful index exists and we can create one, we only need to call ensureIndex on the primary shard, since indexes get copied to receiving shards whenever a migrate occurs.",
            "If the collection has a default collation, explicitly send the simple collation as part of the createIndex request.",
            "We will only pass 'tsOpts' if 'updatedToHandleTimeseriesIndex' is true, and thus we are making the updated time-series index for the sharding DDL commands."
        ],
        "FuncName": "shard_key_util_8178 validateShardKeyIndexExistsOrCreateIfPossible.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u521b\u5efa\u7d22\u5f15\u65f6\uff0c\u9700\u8981\u6307\u5b9a\u6392\u5e8f\u89c4\u5219\uff08collation\uff09\u9700\u8981\u4f7f\u7528v=2\u7d22\u5f15\u3002"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "\u7d22\u5f15\u9700\u8981\u7684\u5fc5\u586b\u5b57\u6bb5\u3002",
            "\u57fa\u4e8e\u65f6\u95f4\u5e8f\u5217\u7684\u7d22\u5f15\u4f7f\u7528\u7528\u6237\u5b9a\u4e49\u7684meta\u548ctimeFields\u800c\u4e0d\u662f\u6876\u96c6\u5408\u7684\u5b57\u6bb5\u540d\u547d\u540d\u3002\u8981\u5339\u914d\u672a\u5206\u7247\u7684\u65f6\u95f4\u5e8f\u5217\u96c6\u5408\uff0c\u7d22\u5f15\u540d\u79f0\u5fc5\u987b\u4f7f\u7528\u89c6\u56fe\u547d\u540d\u7a7a\u95f4\u7684\u7d22\u5f15\u952e\u751f\u6210\u3002",
            "",
            "\u672a\u5206\u7247\u96c6\u5408\u53ef\u4ee5\u4ee5'{_id:1}'\u7684\u5f62\u5f0f\u51fa\u73b0\u3002\u6211\u4eec\u5c06\u5728\u8fd9\u79cd\u60c5\u51b5\u4e0b\u8fd4\u56de\u952e\u7684\u539f\u59cb\u503c\u3002",
            "\u7d22\u5f15\u9009\u9879\u3002"
        ],
        "FuncName": "shard_key_util_8178 makeIndexSpec.txt"
    },
    {
        "Branch": [
            "If the condition is met, the code will be executed, otherwise it will be skipped, similar to a database query filter."
        ],
        "Loop": [
            "The loop will iterate through the data, similar to a database query result set, and perform the specified actions."
        ],
        "Normal": [
            "As above, relying on the path to be injected by the preprocessor, similar to a database query that relies on a stored procedure."
        ],
        "FuncName": "fs_util_2152 GetBuildRootPath.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Take care to return a result in [0..table_count-1]. For 64-bit keys, a 32-bit table number is used to map the key to a specific table in the database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "The first part of the 64-bit key range is reserved for dedicated scan tables, if any, which do not grow, but the rest of the 64-bit key space may grow in the database."
        ],
        "FuncName": "wtperf_6212 map_key_to_table.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Failed to lookup %s: %s\n. This is a database-related error message for a DNS lookup failure."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Failed to get CNAME for %s\n. This is a database-related error message for a CNAME lookup failure."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " CNAME gets put here. This is a database-related string for storing CNAME records."
        ],
        "FuncName": "acountry_9436 callback.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "acountry, version %s\n\n",
            "usage: acountry [-hdv] host|addr ...\n\n\n      h : Display this help and exit.\n\n      d : Print some extra debugging output.\n\n      v : Be more verbose. Print extra information about database types.\n\n      Database types supported by acountry: \n      - MySQL: supports InnoDB and MyISAM storage engines.\n      - PostgreSQL: supports PostgreSQL storage engine.\n      - SQLite: supports SQLite storage engine.\n\n"
        ],
        "FuncName": "acountry_9436 print_help_info_acountry.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Failed to lookup %s. Please check the DNS database for errors."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "If this fails, assume '*argv' is a host-name that must be resolved first in the DNS database."
                ]
            }
        ],
        "Normal": [
            "Initiate the DNS queries, one per command-line argument, to retrieve data from the DNS database."
        ],
        "FuncName": "acountry_9436 main.txt"
    },
    {
        "Branch": [
            "The condition for if statement in database is: "
        ],
        "Loop": [
            "The condition for loop in database is: "
        ],
        "Normal": [
            " Note that we don't need any more macros here to do concatenation of string literals because these are concatenated at the language (C/C++) level e.g. \\str1\\ \\str2\\ => \\str1str2\\ by C standard (section 6.4.5 'String Literals' of C11). In database, the path of binary build product is: "
        ],
        "FuncName": "fs_util_2152 GetBinaryArtifactPath.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Limit how much of the buffer we validate for length, this means that only threads that do growing updates/modifies will ever make changes to values outside of the initial value size, but that's a fair trade off for avoiding figuring out how long the value is more accurately in this performance sensitive function. In a database context, this could be related to the buffer pool size or the page size.",
            "Generate a single random value and re-use it. We generally only have small ranges in this function, so avoiding a bunch of calls is worthwhile. In database terms, this could be equivalent to using a connection pool to reduce the overhead of creating new connections.",
            "Ensure we don't write past the end of a value when configured for randomly sized values. This is crucial in a database setting where data integrity is paramount, and buffer overflows can lead to data corruption or security vulnerabilities.",
            "If i happened to be 0, we'll be re-writing the same value twice, but that doesn't matter. In a database context, this could be related to the concept of transactional consistency, where the outcome of a transaction is the same regardless of the order of operations."
        ],
        "FuncName": "wtperf_6212 randomize_value.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Unexpected CNAME %s (ver_1) - Related to country code: CN - China"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Unexpected CNAME %s (ver_2) - Related to country code: CN - China"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Name for country-number %d not found. - Please check country code: CN - China"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "short-name mismatch; %s vs %s - Country code: CN - China"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "acountry_9436 find_country_from_cname.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Nothing to trim, stop here. These strings are for database code."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "description": "These strings are used to check conditions in database loops."
            }
        ],
        "Normal": [
            {
                "description": "These strings are used to interact with the database, such as executing queries or updating data."
            }
        ],
        "FuncName": "str_trim_utils_4823 trimFromRight.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "$_path should always be in ascending order. In MongoDB, ascending order means that the documents are sorted from lowest to highest value."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "wildcard_access_method_5552 makeOrdering.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Ensure we aren't changing across boundaries in a database",
            "Bail if there isn't anything to do in the database"
        ],
        "FuncName": "wtperf_6212 update_value_delta.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Break the sleep up, so we notice interrupts faster. This is a database backup operation.",
                    "If the workers are done, don't bother with a final call. Database shutdown initiated.",
                    "Take the kind of backup configured. Database backup in progress."
                ]
            }
        ],
        "Normal": [
            "Notify our caller we failed and shut the system down. Database shutdown complete."
        ],
        "FuncName": "wtperf_6212 backup_worker.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Passed a percentage of total operations and should always be a read operation to replace with a write operation to achieve balanced workload distribution in MySQL database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "In Oracle database, Jump around the array to roughly spread out the operations.",
            "In PostgreSQL database, Find a read operation and replace it with another operation. This is roughly n-squared but it's an N of 100 leave it."
        ],
        "FuncName": "wtperf_6212 run_mix_schedule_op.txt"
    },
    {
        "Branch": [
            "This string is used to read data within a specified range in the database.",
            "These strings are used in the database code."
        ],
        "Loop": [
            {
                "loopstr": [
                    "We are done if we reach the end.",
                    "Retrieve and decode keys"
                ]
            }
        ],
        "Normal": [
            "Save the position of the first key for comparison.",
            "If the option tells us to randomly select the number of values to read within the range, we use the value of read_range as the upper limit for the values to read. YCSB-E specifies that we use a uniform random distribution to select the number of values, so we do not use the wtperf random function, which may cause Pareto."
        ],
        "FuncName": "wtperf_6212 do_range_reads.txt"
    },
    {
        "Branch": [
            "IF-CONDITION-DESCRIPTION: Returns the condition for if-operations in database code."
        ],
        "Loop": [
            "LOOP-CONDITION-DESCRIPTION: Returns the condition for loop-operations in database code."
        ],
        "Normal": [
            "NOTREACHED-DESCRIPTION: Indicates that the normal flow of a database operation has been reached in database code."
        ],
        "FuncName": "wtperf_6212 op_name.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "average microseconds per call: latency statistics for database operations",
            "track total latency: monitoring database performance over time",
            "* Update a latency bucket: categorizing database latency into buckets for analysis",
            "* First buckets: usecs from 100us to 1000us at 100us each: capturing initial latency patterns",
            "* Second buckets: milliseconds from 1ms to 1000ms, at 1ms each: identifying mid-range latency issues",
            "* Third buckets are seconds from 1s to 100s, at 1s each: detecting longer latency outliers",
            ">100 seconds, accumulate in the biggest bucket: handling extreme latency cases"
        ],
        "FuncName": "wtperf_6212 track_operation.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Spread data across multiple databases for WT_PERF thread operations, including insert, modify, read, and delete."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "WT_PERF thread operations: read a range by calling next for several operations, confirming the next key is in order."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Pause between truncate attempts in WT_PERF thread operations."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Copy previous value safely, NUL-terminate, and distribute modifications across the document. Randomly choose modifications up to the maximum number and modify up to the maximum percent of the record size.",
                    "Pass old and new data to find modify vectors, according to input. This function may fail when modifications count reaches the maximum number of modifications allowed for the modify operation.",
                    "Modify the maximum modified size and offset per modify in WT_PERF thread operations."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Modify a fixed percentage of the total record size in WT_PERF thread operations.",
                    "Randomize the maximum modification size and offset per modify in WT_PERF thread operations.",
                    "Ensure the offset is larger than the size to spread modifications properly in WT_PERF thread operations."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Offset location difference between modifications in WT_PERF thread operations."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "WT_PERF thread operations: rollback the current transaction and attempt to continue if running with explicit transactions configured and hitting a WT_ROLLBACK."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "WT_PERF thread operations: rollback the current transaction if not handled by ops_per_txn code elsewhere."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Increment operation count in WT_PERF thread operations."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Generate the next key and setup operation-specific statistics tracking objects in WT_PERF thread operations.",
                    "Skip the first operation when trk->ops is 0 to avoid first-time latency spikes in WT_PERF thread operations.",
                    "Do not change the index_like_table on truncate in WT_PERF thread operations.",
                    "Update the log-like table in WT_PERF thread operations.",
                    "Release the cursor if we have multiple tables in WT_PERF thread operations.",
                    "Close the cursor if we own the cursor in WT_PERF thread operations.",
                    "Gather statistics in WT_PERF thread operations.",
                    "Commit the transaction if grouping operations together or tracking changes in our log or index table in WT_PERF thread operations.",
                    "Schedule the next operation in WT_PERF thread operations.",
                    "Decrement throttle ops and check if we should sleep and then get more work to perform in WT_PERF thread operations."
                ]
            }
        ],
        "Normal": [
            "WT_PERF thread operations: -Wconditional-initialized.",
            "Setup the timer for throttling in WT_PERF thread operations.",
            "Setup for truncate in WT_PERF thread operations.",
            "Notify the caller we failed and shut the system down in WT_PERF thread operations."
        ],
        "FuncName": "wtperf_6212 worker.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Confirm inserts, modifies, reads and updates cannot all be zero. Database operation ratios must be balanced to ensure all operations are performed.",
            " * Check for a simple case where the thread is only doing insert or modify or update operations * (because the default operation for a job-mix is read, the subsequent code works fine if only * reads are specified) Database operation ratios are converted to percentages for uniform distribution.",
            " * The worker thread configuration is done as ratios of operations. If * the caller gives us something insane like reads=77,updates=23 (do * 77 reads for every 23 updates), we don't want to do 77 reads followed * by 23 updates, we want to uniformly distribute the read and update * operations across the space. Database operation ratios are normalized to ensure a balanced workload.",
            " * Percentage conversion is lossy, the application can do stupid stuff * here, for example, imagine a configured ratio of reads=1,inserts=2, * updates=999999. First, if the percentages are skewed enough, some * operations might never be done. Second, we set the base operation to * read, which means any fractional results from percentage conversion * will be reads, implying read operations in some cases where reads * weren't configured. We should be fine if the application configures * something approaching a rational set of ratios Database operation ratios must be rational to avoid skewed workloads."
        ],
        "FuncName": "wtperf_6212 run_mix_schedule.txt"
    },
    {
        "Branch": [
            "Populate databases in a thread-safe manner if certain conditions are met."
        ],
        "Loop": [
            {
                "loopstr": [
                    "Determine the table affiliation of this operation.",
                    "Collect statistics by measuring the latency of inserting a single key.",
                    "If multiple tables are involved, record the time for insertion into all of them."
                ]
            }
        ],
        "Normal": [
            "Perform bulk loads if the population process is single-threaded.",
            "Establish the cursors.",
            "Populate the databases.",
            "Notify the caller of failure and initiate a system shutdown."
        ],
        "FuncName": "wtperf_6212 populate_thread.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Break the sleep up, so we notice interrupts faster. This is a checkpoint operation in the database system.",
                    "If tiered storage is disabled we are done. Otherwise, we want a final checkpoint in the database system."
                ]
            }
        ],
        "Normal": [
            "Notify our caller we failed and shut the system down. This is a shutdown operation in the database system."
        ],
        "FuncName": "wtperf_6212 checkpoint_worker.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Reopen the database connection. This ensures that the workload phase always starts with the on-disk files, and allows read-only workloads to be identified. This is particularly important for LSM databases, where the merge algorithm is more aggressive for read-only trees.",
            "The database connection is released regardless of the close function's return value."
        ],
        "FuncName": "wtperf_6212 close_reopen.txt"
    },
    {
        "Branch": [
            "Test if-conditions for database operations."
        ],
        "Loop": [
            {
                "loopstr": [
                    "Initialize database setup by creating unique home and monitor directories. This step may recreate the directories if the databases are being created."
                ]
            }
        ],
        "Normal": [
            "Prepare an array to store WTPERF copies for database operations.",
            "Prepare an array to store thread IDs for database operations.",
            "Wait for all threads to finish their database-related tasks."
        ],
        "FuncName": "wtperf_6212 start_all_runs.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Don't copy the home and monitor directories, they are filled in by our caller, explicitly. In WTPERF, these directories are used to store performance-related data."
        ],
        "FuncName": "wtperf_6212 wtperf_copy.txt"
    },
    {
        "Branch": [
            "If tiered storage is not set, we need to create a tiered bucket for database storage."
        ],
        "Loop": [],
        "Normal": [
            "If tiered storage is not set, there is nothing to do for database storage.",
            "For database storage, we need to check that we can fit the paths, separator, and null byte in a tiered bucket."
        ],
        "FuncName": "wtperf_6212 create_tiered_bucket.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Start the checkpoint thread. This is a critical step in the populate phase, ensuring data consistency across the database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Sleep for 100th of a second, report_interval is in second granularity, each 100th increment of elapsed is a single increment of interval. This loop condition is used to implement a delay in the populate phase, allowing for efficient data processing."
                ]
            }
        ],
        "Normal": [
            "If this is going to be a tiered workload, start the checkpoint threads and the flush threads during the populate phase so that the tiers are created as we populate the database. This is a key step in optimizing database performance.",
            "Start cycling idle tables if configured. This helps to ensure that all data is properly processed during the populate phase.",
            "Move popthreads aside to narrow possible race with the monitor thread. The latency tracking code also requires that popthreads be NULL when the populate phase is finished, to know that the workload phase has started. This is a critical step in ensuring accurate latency tracking.",
            "Report if any worker threads didn't finish. This helps to identify any issues that may have arisen during the populate phase.",
            "This is needed as the divisions will fail if the insert takes no time which will only be the case when there is no data to insert. This is a key consideration in implementing efficient data insertion.",
            "Stop cycling idle tables. Once data processing is complete, idle tables can be stopped to conserve resources.",
            "Stop the flush and checkpoint threads if we used them during populate. We must stop the flush thread before stopping the checkpoint thread as the flush thread depends on a later checkpoint running. This is a critical step in ensuring data consistency and preventing potential errors."
        ],
        "FuncName": "wtperf_6212 execute_populate.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Continue with the next slice of the key space. This is a conditional statement in the database code, where the then clause is executed when the condition is true.",
                    "The condition is likely a check for a specific database state or value, and the then clause is the action taken when that condition is met."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Break the sleep up, so we notice interrupts faster. This is a loop condition in the database code, where the loop is executed until a certain condition is met.",
                    "The loop is likely used to iterate over a set of database records or to perform a repetitive task, and the condition is the criteria for exiting the loop."
                ]
            }
        ],
        "Normal": [
            "Figure out how many items we should scan. We base the percentage on the icount. This is a normal string in the database code, where the code is determining the number of items to scan based on a percentage value.",
            "When we scan the tables, we will increment the key by an amount that causes us to visit each table in order, and jump ahead in the key space when returning to a table. By doing this, we don't repeat keys until we visit them all, but we don't visit keys in sequential order. This might better emulate the access pattern to a main table when an index is scanned, or a more complex query is performed. This string is describing the database scanning process, where the code is incrementing the key to visit each table in order.",
            "Notify our caller we failed and shut the system down. This is a normal string in the database code, where the code is notifying the caller that an operation has failed and shutting down the system."
        ],
        "FuncName": "wtperf_6212 scan_worker.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Figure out the workload's schedule.",
                    "Start the workload's threads.",
                    "Manage thread pool for efficient resource allocation."
                ]
            },
            {
                "loopstr": [
                    "Sleep for one second at a time. If we are tracking run time, check to see if we're done, and if we're only tracking run time, go back to sleep.",
                    "Sum the operations we've done.",
                    "Track total operations for accurate performance metrics.",
                    "If we're checking total operations, see if we're done.",
                    "Monitor and report on workload's progress and status."
                ]
            }
        ],
        "Normal": [
            "Start cycling idle tables.",
            "Allocate memory for the worker threads.",
            "Start each workload.",
            "Notify the worker threads they are done.",
            "Stop cycling idle tables.",
            "Drop tables if configured to and this isn't an error path.",
            "Clean up resources and finalize workload execution.",
            "Report if any worker threads didn't finish."
        ],
        "FuncName": "wtperf_6212 execute_workload.txt"
    },
    {
        "Branch": [
            "\u521b\u5efaWTPERF\u5b9e\u4f8b\u7684URI\u5217\u8868\uff1a\u5982\u679c\u53ea\u6709\u4e00\u4e2a\u8868\u683c\uff0c\u5c31\u4f7f\u7528\u57fa\u672c\u540d\u79f0\u3002"
        ],
        "Loop": [
            {
                "loopstr": [
                    "\u521b\u5efaWTPERF\u5b9e\u4f8b\u7684URI\u5217\u8868\uff1a\u5982\u679c\u53ea\u6709\u4e00\u4e2a\u8868\u683c\uff0c\u5c31\u4f7f\u7528\u57fa\u672c\u540d\u79f0\u3002"
                ]
            }
        ],
        "Normal": [
            "\u521b\u5efaWTPERF\u5b9e\u4f8b\u7684URI\u5217\u8868\uff1a\u521b\u5efa\u7d22\u5f15\u7c7b\u8868\u683cURI\u3002",
            "\u521b\u5efaWTPERF\u5b9e\u4f8b\u7684URI\u5217\u8868\uff1a\u521b\u5efa\u65e5\u5fd7\u7c7b\u8868\u683cURI\u3002"
        ],
        "FuncName": "wtperf_6212 create_uris.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Break the sleep up, so we notice interrupts faster.",
                    "If workers are done, do a final call to flush that last data.",
                    "*In order to get all the data into the object when the work is done, we need to call checkpoint with flush_tier enabled. This ensures data integrity and prevents data loss during checkpoint operations in the database.*"
                ]
            },
            {
                "loopstr": [
                    "*We need to make a final call to flush_tier after the stop signal arrives. We therefore save the global stop signal into a local variable, so we are sure to complete another iteration of this loop and the final flush_tier before we exit. This ensures that all database transactions are properly committed to disk before the system exits.*"
                ]
            }
        ],
        "Normal": [
            "Notify our caller we failed and shut the system down. The database will be left in a consistent state, ensuring data integrity and recoverability in case of a system crash or restart."
        ],
        "FuncName": "wtperf_6212 flush_tier_worker.txt"
    },
    {
        "Branch": [
            "If condition for database performance tuning, calculate wtperf value range."
        ],
        "Loop": [
            "Loop condition for database performance tuning, calculate wtperf value range."
        ],
        "Normal": [
            "It is legal to configure a zero size populate phase, hide that from other code by pretending the range is 1 in that case. This is related to database performance optimization and wtperf value calculation."
        ],
        "FuncName": "wtperf_6212 wtperf_value_range.txt"
    },
    {
        "Branch": [
            "Stop thread execution and release database resources to prevent memory leaks and ensure data integrity."
        ],
        "Loop": [
            "Continue thread execution and loop until all database resources are released and threads are terminated."
        ],
        "Normal": [
            "We don't free the thread structures or any memory referenced or NULL the reference when we stop the threads; the thread structure is still being read by the monitor thread among others As a standalone program leaking memory isn't a concern and it's simpler that way. However, in a database context, it's crucial to release resources to maintain data consistency and prevent resource exhaustion."
        ],
        "FuncName": "wtperf_6212 stop_threads.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Thread counter starts at zero and the populate threads reserve a value so start the index multiplier after that. This is relevant for database operations where thread management is crucial.",
                    "We don't want the threads executing in lock-step, seed each one differently. This ensures database queries are executed independently.",
                    "Every thread gets a key/data buffer because we don't bother to distinguish between threads needing them and threads that don't, it's not enough memory to bother. These buffers are used for database operations where data is being processed concurrently.",
                    "Initialize and then toss in a bit of random values if needed. This is necessary for database operations where data is being populated randomly.",
                    "Every thread gets tracking information and is initialized for latency measurements, for the same reason. This is relevant for database operations where performance metrics are being tracked."
                ]
            }
        ],
        "Normal": [
            "Initialize the threads. This is the first step in managing database connections.",
            "Start the threads. This is the next step in executing database operations concurrently."
        ],
        "FuncName": "wtperf_6212 start_threads.txt"
    },
    {
        "Branch": [
            "Drop tables if database is not empty"
        ],
        "Loop": [
            "Loop through tables to drop"
        ],
        "Normal": [
            "Drop any tables in the database"
        ],
        "FuncName": "wtperf_6212 drop_all_tables.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "wtperf: a tool for performance testing Wired Tiger databases.\n",
            "\t-C <string> additional connection configuration\n",
            "\t            (added to option conn_config) - used to specify additional\n",
            "\t            connection settings, such as username and password.\n",
            "\t-h <string> Wired Tiger home directory must exist, default WT_TEST\n",
            "\t            - specifies the location of the Wired Tiger home directory.\n",
            "\t-O <file> file contains options as listed below\n",
            "\t            - specifies the file containing options to be used for the test.\n",
            "\t-o option=val[,option=val,...] set options listed below\n",
            "\t            - sets options for the test, such as the number of threads and the\n",
            "\t            test duration.\n",
            "\t-T <string> additional table configuration\n",
            "\t            (added to option table_config) - used to specify additional\n",
            "\t            table settings, such as the table size and the number of rows.\n",
            "\n"
        ],
        "FuncName": "wtperf_6212 usage.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If we have a workload, close and reopen the connection so that LSM can detect read-only workloads.",
                    "Didn't create, set insert count.",
                    "Start the backup thread.",
                    "Start the checkpoint thread.",
                    "Start the flush_tier thread.",
                    "Start the scan thread.",
                    "Execute the workload.",
                    "One final summation of the operations we've completed.",
                    "In a database context, a workload refers to a set of operations or transactions performed on the database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "[-Wconditional-uninitialized]",
            "If creating, create the tables.",
            "Start the monitor thread.",
            "If creating, populate the table.",
            "Optional workload.",
            "Notify the worker threads they are done.",
            "We must stop the flush thread before the checkpoint thread.",
            "In a database system, a monitor thread is responsible for tracking and managing database performance and activity."
        ],
        "FuncName": "wtperf_6212 start_run.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u6839\u636e\u8bfb\u5199\u541e\u5410\u91cf\uff0c\u8c03\u6574\u6570\u636e\u5e93\u7684\u5e76\u53d1\u5ea6\uff1a\u81f3\u5c11\u4e00\u4e2a\u7968\u6c60\u5df2\u7ecf\u8017\u5c3d\uff0c\u5c1d\u8bd5\u589e\u52a0\u5e76\u53d1\u5ea6"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "\u6839\u636e\u8bfb\u5199\u541e\u5410\u91cf\uff0c\u8c03\u6574\u6570\u636e\u5e93\u7684\u5e76\u53d1\u5ea6\uff1a\u4e24\u4e2a\u7968\u6c60\u90fd\u6ca1\u6709\u8017\u5c3d\uff0c\u5c1d\u8bd5\u5c06\u5e76\u53d1\u5ea6\u964d\u4f4e\u5230\u5f53\u524d\u4f7f\u7528\u6c34\u5e73\u4ee5\u4e0b"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "\u6839\u636e\u8bfb\u5199\u541e\u5410\u91cf\uff0c\u8c03\u6574\u6570\u636e\u5e93\u7684\u5e76\u53d1\u5ea6\uff1a\u8bb0\u5f55\u57fa\u7ebf\u503c"
        ],
        "FuncName": "throughput_probing_733 _probeStable.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Resetting the cursor is not fatal. We still return the value we retrieved above. We do it so that we don't leave a cursor positioned. In a database, this is an example of a conditional statement where the then clause executes when the condition is true."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "0 is outside the expected range. In a database, this error message indicates that the value 0 is not within the valid range for a particular field or column."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "If we have 128-bit integers, we can use a fancy method described below. If not, we use a simple one. In a database, this is an example of a conditional statement where the then clause executes when the condition is true, and the condition is related to the data type of the field.",
                    "We generate a random number that is in the range between 0 and largest largest, where largest is the last inserted key. Then we take a square root of that random number -- this is our target selection. With this formula, larger keys are more likely to get selected than smaller keys, and the probability of selection is proportional to the value of the key, which is what we want. In a database, this is an example of a random number generation algorithm used for data selection.",
                    "First we need a 128-bit random number, and the WiredTiger random number function gives us only a 32-bit random value. With only a 32-bit value, the range of the random number will always be smaller than the square of the largest insert key for workloads with a large number of keys. So we need a longer random number for that. In a database, this is an example of a limitation of the random number generation algorithm used for data selection.",
                    "We get a 128-bit random number by concatenating four 32-bit numbers. We get less entropy this way than via a true 128-bit generator, but we are not defending against crypto attacks here, so this is good enough. In a database, this is an example of a technique used to generate a longer random number for data selection.",
                    "Now we limit the random value to be within the range of square of the latest insert key and take a square root of that value. In a database, this is an example of a data selection algorithm used to select a record based on its key value.",
                    "If we don't have 128-bit integers, we simply select a number from a fixed sized group of recently inserted records. In a database, this is an example of a fallback algorithm used when the primary algorithm fails or is not applicable."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If we have a random cursor set up then use it. In a database, this is an example of a query optimization technique used to improve query performance.",
            "Use WiredTiger's random number routine: it's lock-free and fairly good. In a database, this is an example of a random number generation algorithm used for data selection.",
            "Use Pareto distribution to give 80/20 hot/cold values. In a database, this is an example of a data distribution algorithm used to model real-world data.",
            "A distribution that selects the record with a higher key with higher probability. This was added to support the YCSB-D workload, which calls for read latest selection for records that are read. In a database, this is an example of a query optimization technique used to improve query performance.",
            "Wrap the key to within the expected range and avoid zero: we never insert that key. In a database, this is an example of a data normalization technique used to ensure data consistency."
        ],
        "FuncName": "wtperf_6212 wtperf_rand.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Close the metadata checkpoint session in the database."
        ],
        "FuncName": "meta_track_6575 __wt_meta_track_destroy.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Extract timestamp from given BSON data. These strings are used in database code.",
            "Timestamp extraction is a crucial process in database management."
        ],
        "FuncName": "record_id_helpers_6418 extractKeyOptime.txt"
    },
    {
        "Branch": [
            "Turn off tracking for if statements in the database schema"
        ],
        "Loop": [
            "Turn off tracking for loop conditions in the database schema"
        ],
        "Normal": [
            "Turn off tracking for unroll in the database schema"
        ],
        "FuncName": "meta_track_6575 __wt_meta_track_sub_off.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Set session transaction isolation to read-committed isolation, we rely on that for the correctness of metadata checkpoints. This is a critical step in ensuring data consistency in our database management system."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "meta_track_6575 __wt_meta_track_init.txt"
    },
    {
        "Branch": [
            "If there was a previous value, keep it around -- if not, then this update is really an insert.",
            "Metadata update tracking for If-Condition in database."
        ],
        "Loop": [
            "Metadata update tracking for Loop-Condition in database.",
            "Metadata update tracking for Loop-Condition in database."
        ],
        "Normal": [
            "If there was a previous value, keep it around -- if not, then this update is really an insert.",
            "Metadata update tracking for Normal-Str in database."
        ],
        "FuncName": "meta_track_6575 __wti_meta_track_update.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "We should have committed above unless we're unrolling, there was an error or the operation was a noop. The tracking mechanism is disabled, releasing the tracking resources, and applying the untracked operations to the metadata."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If it was a nested transaction, there is nothing to do. The database is in a state where metadata tracking is unnecessary.",
            "Turn off tracking for unroll. This operation does not require metadata synchronization.",
            "If there were no operations logged, skip unnecessary metadata checkpoints. For example, this happens if attempting to create a data source that already exists (or drop one that doesn't). The database is in a state where metadata checkpoints are not required.",
            "Unrolling doesn't require syncing the metadata. The database is in a state where metadata synchronization is not necessary.",
            "If we don't have the metadata cursor (e.g, we're in the process of creating the metadata), we can't sync it. The database is in a state where metadata synchronization is not possible.",
            "If we're logging, make sure the metadata update was flushed. The database ensures that all metadata updates are properly flushed to storage.",
            "Undo any tracked operations on failure. Apply any tracked operations post-commit. The database reverts any uncommitted changes on failure and reapplies them after a successful commit.",
            "Wake up the sweep thread: particularly for the in-memory storage engine, we want to reclaim space immediately. The database triggers the sweep thread to reclaim memory resources."
        ],
        "FuncName": "meta_track_6575 __wt_meta_track_off.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Increasing concurrency caused throughput to increase, so use this information to adjust our stable concurrency. We don't want to leave this at the current level. Instead, we use this to update the moving average to avoid over-correcting on recent measurements. This is a key consideration for database performance optimization, as high concurrency can significantly impact query execution times.",
                    "Increasing concurrency caused throughput to increase, so use this information to adjust our stable concurrency. We don't want to leave this at the current level. Instead, we use this to update the moving average to avoid over-correcting on recent measurements. This is a key consideration for database performance optimization, as high concurrency can significantly impact query execution times.",
                    "Increasing concurrency caused throughput to increase, so use this information to adjust our stable concurrency. We don't want to leave this at the current level. Instead, we use this to update the moving average to avoid over-correcting on recent measurements. This is a key consideration for database performance optimization, as high concurrency can significantly impact query execution times."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "throughput_probing_733 _probeUp.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "x86_64 ABI specifies that end of call-chain is marked with a NULL RBP or undefined return address. DWARF information is used for reverse tracing."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Like regular frame, CFA = RSP+8, RA = [CFA-8], no regs saved. Falling back to frame chain reverse tracing."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Looks like we may have reached the end of the call-chain. DWARF information is used for reverse tracing."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Test to see if what we think is the previous RIP is valid. DWARF information is used for reverse tracing."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "* The call should have pushed RIP to the stack * and since there was no preamble RSP hasn't been * touched so RIP should be at RSP. DWARF information is used for reverse tracing."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Heuristic to determine incorrect guess. For RBP to be a valid frame it needs to be above current CFA, but don't let it go more than a little. Note that we can't deduce anything about new RBP (rbp1) since it may not be a frame pointer in the frame above. Just check we get the value. Falling back to frame chain reverse tracing."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Try DWARF-based unwinding. If DWARF fails, use frame chain for reverse tracing."
        ],
        "FuncName": "Gstep_8329 unw_step.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Maintain positions in the new chunk of memory "
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "Get next metadata tracking item"
        ],
        "Normal": [],
        "FuncName": "meta_track_6575 __meta_track_next.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Get the collection's cluster key field name. In a document database, the cluster key is the primary key that uniquely identifies each document.",
            " Build a RecordId using the cluster key. The RecordId is a unique identifier for each document in the database."
        ],
        "FuncName": "record_id_helpers_6418 keyForDoc.txt"
    },
    {
        "Branch": [
            " Conditional statements in database queries to check if RecordId is reserved."
        ],
        "Loop": [
            " Looping through database records to check if RecordId is reserved."
        ],
        "Normal": [
            " All RecordId strings that start with FF are considered reserved. This is a reserved RecordId in the database."
        ],
        "FuncName": "record_id_helpers_6418 isReserved.txt"
    },
    {
        "Branch": [
            "If the reservation_id is available, then it will be assigned to the customer."
        ],
        "Loop": [
            "The loop will continue until all reservation_ids are processed."
        ],
        "Normal": [
            "There is only one reservation at the moment. In our database, we store the reservation_id to keep track of the reservation status."
        ],
        "FuncName": "record_id_helpers_6418 reservedIdFor.txt"
    },
    {
        "Branch": [
            "Open a cursor, traverse the object, and remove each entry where the condition is met in a database."
        ],
        "Loop": [
            "Open a cursor, traverse the object, and remove each entry where the loop condition is satisfied in a database."
        ],
        "Normal": [
            "Open a cursor, traverse the object, and remove every entry in a database."
        ],
        "FuncName": "schema_truncate_5890 __truncate_dsrc.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Start at the bottom, find the first unsynchronized slot in the database stack."
        ],
        "FuncName": "BaselineFrameInfo_345 numUnsyncedSlots.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "File truncate translates into a range truncate. This operation is specific to the database type, which supports truncating files by adjusting the database's internal file pointers."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If we didn't find a metadata entry, map that error to ENOENT. The database type has a specific handling for metadata-related errors, where it treats ENOENT as a valid error code for missing metadata."
        ],
        "FuncName": "schema_truncate_5890 __wt_schema_truncate.txt"
    },
    {
        "Branch": [
            "Initializing compiler frame information for conditional statements in database code."
        ],
        "Loop": [
            "Initializing compiler frame information for loop conditions in database code."
        ],
        "Normal": [
            "An extra slot is needed for global scopes because INITGLEXICAL stack depth 1 is compiled as a SETPROP stack depth 2 on the global lexical scope. This is a requirement for database code to function correctly."
        ],
        "FuncName": "BaselineFrameInfo_345 init.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Pops a value from the stack and writes it to a destination register."
        ],
        "FuncName": "BaselineFrameInfo_345 popValue.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Truncate the column groups. For example, truncate the 'users' table.",
            "Truncate the indices. For example, truncate the indices of the 'orders' table."
        ],
        "FuncName": "schema_truncate_5890 __truncate_table.txt"
    },
    {
        "Branch": [
            "If-Condition-Str with MongoDB database type: A conditional statement in MongoDB that evaluates a condition and executes a block of code if the condition is true."
        ],
        "Loop": [
            "Loop-Condition-Str with MongoDB database type: A loop in MongoDB that iterates over a collection of documents and performs a specified action."
        ],
        "Normal": [
            "Intentionally discard the TypeBits since the type information will be stored in the cluster with MongoDB database type: MongoDB stores type information in the cluster, so TypeBits are discarded.",
            "key of the original document. The consequence of this behavior is that cluster key values with MongoDB database type: In MongoDB, cluster key values may not be used concurrently if they compare similarly but are of different types.",
            "that compare similarly, but are of different types may not be used concurrently with MongoDB database type: MongoDB may not allow concurrent use of cluster key values that compare similarly but are of different types."
        ],
        "FuncName": "record_id_helpers_6418 keyForElem.txt"
    },
    {
        "Branch": [
            "Database: Conditional statement in database code. Typically used to execute a block of code based on a condition."
        ],
        "Loop": [
            "Database: Loop condition in database code. Used to control the execution of a loop based on a condition."
        ],
        "Normal": [
            "Database: Memory leak issue in database code. The process may leak some resources until it exits, but it is preferable to hard-to-debug crashes during exit."
        ],
        "FuncName": "os_dlopen_1414 __wt_dlclose.txt"
    },
    {
        "Branch": [
            "These instructions evaluate a conditional expression and only execute",
            "if the expression is true, and they can be used to implement a simple",
            "if-else statement in the database."
        ],
        "Loop": [
            "These instructions evaluate a conditional expression and only execute",
            "if the expression is true, and they can be used to implement a simple",
            "loop in the database."
        ],
        "Normal": [
            "These instructions don't modify the object and just guard specific",
            "properties. They are used to control the flow of execution in the database",
            "and can be used to implement various types of database logic."
        ],
        "FuncName": "MIR_2491 skipObjectGuards.txt"
    },
    {
        "Branch": [
            "The If-Condition-Str is used by database to set the clock source in the service context.",
            "These strings are for database code."
        ],
        "Loop": [
            "The Loop-Condition-Str is used by database to set the clock source in the service context.",
            "These strings are for database code."
        ],
        "Normal": [
            "The fast clock is used by OperationContext::hasDeadlineExpired.",
            "The precise clock is used by waitForConditionOrInterruptNoAssertUntil.",
            "The fast clock is used by database to set the clock source in the service context.",
            "The precise clock is used by database to set the clock source in the service context."
        ],
        "FuncName": "api_version_metrics_test_4910 setUp.txt"
    },
    {
        "Branch": [
            "timestamp_to_standard_time"
        ],
        "Loop": [
            "timestamp_to_standard_time"
        ],
        "Normal": [
            "timestamp_to_standard_time"
        ],
        "FuncName": "timestamp_type_9925 ToString.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "In the default case, there are no constants to fold. In WebAssembly, constants can be folded to simplify the code, reducing the number of instructions and improving performance."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "Conditional statement to check if a value is true or false.",
            "E.g., if (x > 5) { ... }"
        ],
        "Loop": [
            "Looping statement to execute a block of code repeatedly.",
            "E.g., for (i = 0; i < 10; i++) { ... }"
        ],
        "Normal": [
            " x86 has only 3 Value registers. Only support 2 regs here for now,",
            " so that there's always a scratch Value register for reg -> reg",
            " moves.",
            " On arm64, SP may be < PSP now (that's OK).",
            " eg testcase: tests/bug1580246.js",
            " In database context, x86 and arm64 are types of database architectures.",
            " Value registers are used to store and manipulate data in the database.",
            " SP (Stack Pointer) and PSP (Process Stack Pointer) are related to memory management in the database."
        ],
        "FuncName": "BaselineFrameInfo_345 popRegsAndSync.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "x86 has only 3 Value registers. Only support 2 regs here for now, as per database type.",
            "This is necessary for reg -> reg moves, which require a scratch Value register.",
            "On arm64, SP may be < PSP now (that's OK). This is a characteristic of the arm64 database type.",
            "eg testcase: tests/bug1580246.js, a test case related to the arm64 database type."
        ],
        "FuncName": "BaselineFrameInfo_345 popRegsAndSync.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Division by zero will trap at runtime. Arithmetic operations on 64-bit integers may result in overflow or underflow, which can lead to unexpected behavior."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Overflow will trap at runtime. 64-bit integer arithmetic operations can result in overflow or underflow, which can lead to unexpected behavior."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Division by zero will trap at runtime. Arithmetic operations on 64-bit integers may result in overflow or underflow, which can lead to unexpected behavior."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Handle all negative values at runtime, for simplicity. 64-bit integer arithmetic operations can result in overflow or underflow for negative values."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "MIR_2491 EvaluateInt64ConstantOperands.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If the operand of the Not is itself a Not, they cancel out. In database operations, a double negative is often used to represent a value that is not null or not zero."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "MIR_2491 foldsDoubleNegation.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: A database condition that determines whether to execute a block of code.",
            "Generated goto instruction based on condition."
        ],
        "Loop": [
            "Loop-Condition-Str: A database condition that determines the termination of a loop.",
            "Generated goto instruction to loop or exit based on condition."
        ],
        "Normal": [
            " All instructions within both successors need be dead if unused. (Database optimization: Removing dead code for improved performance.)",
            " Both successors must have the same target successor. (Database constraint: Ensuring consistency in branching logic.)",
            " The target successor's phis must be redundant. Redundant phis should have been removed in an earlier pass, so only check if any phis are present, which is a stronger condition. (Database optimization: Removing redundant phis for improved efficiency.)"
        ],
        "FuncName": "MIR_2491 foldsNeedlessControlFlow.txt"
    },
    {
        "Branch": [
            "Only support specialized, non-magic types. (Database Type: Conditional)"
        ],
        "Loop": [
            "Only support specialized, non-magic types. (Database Type: Loop)"
        ],
        "Normal": [
            "Only support specialized, non-magic types. (Database Type: Normal)"
        ],
        "FuncName": "MIR_2491 definitelyType.txt"
    },
    {
        "Branch": [
            "Check if condition is dead in basic block.",
            "Verify if all instructions in block are dead if unused."
        ],
        "Loop": [
            {
                "loopstr": [
                    " Skip trivial instructions.",
                    " All uses must be within the current block.",
                    " All instructions within this block must be dead if unused.",
                    " Dead code elimination: check for dead instructions in loop."
                ]
            }
        ],
        "Normal": [
            "Basic block analysis: check for dead code."
        ],
        "FuncName": "MIR_2491 AllInstructionsDeadIfUnused.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "No def-uses: This condition is used in a single definition."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "More than one def-use: This condition is used in multiple definitions in the database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "MIR_2491 maybeSingleDefUse.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u5982\u679c\u6211\u4eec\u671f\u671b\u8bfb\u53d6\u7684\u7c7b\u578b\u6bd4\u5b58\u50a8\u7684\u7c7b\u578b\u66f4\u901a\u7528\uff0c\u5219\u5c06\u5b58\u50a8\u4e2d\u4f7f\u7528\u7684\u503c\u88c5\u7bb1\u3002\u6839\u636eStore\u6307\u4ee4\u7684\u5b9a\u4e49\uff0cStore\u6307\u4ee4\u4f1a\u5c06\u503c\u5b58\u50a8\u5728\u6570\u636e\u5e93\u4e2d\u3002",
                    "\u5982\u679c\u671f\u671b\u7684\u7c7b\u578b\u6bd4\u5b58\u50a8\u7684\u7c7b\u578b\u66f4\u901a\u7528\uff0c\u5219\u5c06\u5b58\u50a8\u4e2d\u4f7f\u7528\u7684\u503c\u88c5\u7bb1\u3002\u6839\u636eStore\u6307\u4ee4\u7684\u5b9a\u4e49\uff0cStore\u6307\u4ee4\u4f1a\u5c06\u503c\u5b58\u50a8\u5728\u6570\u636e\u5e93\u4e2d\u3002"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "\u5982\u679c\u7c7b\u578b\u5339\u914d\uff0c\u5219\u8fd4\u56de\u5b58\u50a8\u4e2d\u4f7f\u7528\u7684\u503c\u4f5c\u4e3a\u53c2\u6570\u3002Store\u6307\u4ee4\u4f1a\u5c06\u503c\u5b58\u50a8\u5728\u6570\u636e\u5e93\u4e2d\u3002",
            "\u5982\u679c\u7c7b\u578b\u5339\u914d\uff0c\u5219\u8fd4\u56de\u5b58\u50a8\u4e2d\u4f7f\u7528\u7684\u503c\u4f5c\u4e3a\u53c2\u6570\u3002Store\u6307\u4ee4\u4f1a\u5c06\u503c\u5b58\u50a8\u5728\u6570\u636e\u5e93\u4e2d\u3002"
        ],
        "FuncName": "MIR_2491 foldsToStore.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    " We already have a definition use. So 1+ (Database Definition Found)",
                    " We saw one definition. Loop to test if there is another. (Database Definition Loop)"
                ]
            }
        ],
        "Normal": [],
        "FuncName": "MIR_2491 hasOneDefUse.txt"
    },
    {
        "Branch": [
            "Conditional statements can be folded into specific types in the database."
        ],
        "Loop": [
            "Loop conditions can be folded into specific types in the database."
        ],
        "Normal": [
            " In the default case, there are no constants to fold, and database-specific descriptions are not applicable."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "If we are not running off-main thread, we can assert that the metadata is consistent with database logic."
        ],
        "Loop": [],
        "Normal": [
            "If we are not running off-main thread we can assert that the metadata is consistent with database schema and data integrity."
        ],
        "FuncName": "MIR_2491 WrappedFunction.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: MConstant.toJSValue() converts MConstant objects to JavaScript values in MConstant class, used in database code."
        ],
        "Loop": [
            "Loop-Condition-Str: MConstant.toJSValue() converts MConstant objects to JavaScript values in MConstant class, used in database code."
        ],
        "Normal": [
            "Normal-Str: MConstant.toJSValue() converts MConstant objects to JavaScript values. Wasm has types like int64 that cannot be stored as js::Value. It also doesn't want the NaN canonicalization enforced by js::Value.",
            "Normal-Str: MConstant.toJSValue() converts MConstant objects to JavaScript values. It also doesn't want the NaN canonicalization enforced by js::Value."
        ],
        "FuncName": "MIR_2491 toJSValue.txt"
    },
    {
        "Branch": [
            "In the if condition, the expression must be a subtype of the specified type."
        ],
        "Loop": [
            "In the loop condition, the expression must be a subtype of the specified type."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. The expression must be a subtype of the specified type."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "Update the operand to use the dominating definition for conditional statements in the database."
        ],
        "Loop": [
            {
                "loopstr": [
                    "Update the operand to use the dominating definition for loop conditions in the database."
                ]
            }
        ],
        "Normal": [
            "Update the operand to use the dominating definition for normal operations in the database."
        ],
        "FuncName": "MIR_2491 replaceAllLiveUsesWith.txt"
    },
    {
        "Branch": [
            "\u53ef\u79fb\u52a8\u7684\u6761\u4ef6\uff1ajitinfo\u5141\u8bb8\u79fb\u52a8\u4e14\u65e0\u5f71\u54cd",
            "\u6839\u636ejitinfo\u5224\u65ad\u662f\u5426\u53ef\u79fb\u52a8"
        ],
        "Loop": [],
        "Normal": [
            "\u6211\u4eec\u662f\u53ef\u79fb\u52a8\u7684\uff0c\u5982\u679cjitinfo\u8bf4\u6211\u4eec\u53ef\u4ee5\u79fb\u52a8\uff0c\u800c\u4e14\u6211\u4eec\u4e5f\u4e0d\u662f",
            "\u6709\u5f71\u54cd\u7684\u3002jitinfo\u65e0\u6cd5\u68c0\u67e5\u540e\u8005\uff0c\u56e0\u4e3a\u5b83\u53d6\u51b3\u4e8e\u6211\u4eec\u53c2\u6570\u7684\u7c7b\u578b\u3002"
        ],
        "FuncName": "MIR_2491 computeMovable.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Update the resume point operand to use the optimized-out constant.",
                    "This optimization is specific to database indexing and query optimization."
                ]
            }
        ],
        "Normal": [
            "Remove dangling pointers.",
            "This step is crucial for maintaining data integrity in the database."
        ],
        "FuncName": "MIR_2491 optimizeOutAllUses.txt"
    },
    {
        "Branch": [
            "Evaluate the condition to determine if the if-statement should be executed."
        ],
        "Loop": [
            "Check the loop condition to determine if the loop should be iterated."
        ],
        "Normal": [
            "Construct a 64-bit value containing both the payload and type.",
            "Fold all 64 bits into a 32-bit result, taking care not to discard valuable information that could lead to hash collisions."
        ],
        "FuncName": "MIR_2491 ConstantValueHash.txt"
    },
    {
        "Branch": [
            "Checks if the WASM type is a concrete type."
        ],
        "Loop": [],
        "Normal": [
            "In the default case, there are no constants to fold. This is a description related to the database type."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "Marking iterator Phi that must transitively hold an iterator live based on if conditions in the database code."
        ],
        "Loop": [
            "Marking iterator Phi that must transitively hold an iterator live based on loop conditions in the database code."
        ],
        "Normal": [
            "Marking iterator Phi that must transitively hold an iterator live based on database code."
        ],
        "FuncName": "MIR_2491 markIteratorPhis.txt"
    },
    {
        "Branch": [
            "The condition for if statements: requires a valid predecessor before execution."
        ],
        "Loop": [
            "The condition for loop statements: needs a valid predecessor to start the loop cycle."
        ],
        "Normal": [
            "This should not be called after removing fake loop predecessors. Note: This statement requires a valid predecessor to maintain its original meaning, which is related to the predecessor operation in the database context."
        ],
        "FuncName": "MIR_2491 getLoopPredecessorOperand.txt"
    },
    {
        "Branch": [
            "If the input values of the phi node are all the same, returns the first input value. These strings are code in the database."
        ],
        "Loop": [],
        "Normal": [
            "If this phi is redundant (e.g., phi(a,a) or b=phi(a,this)), returns the operand that it will always be equal to (a, in those two cases). This description is specific to database type phi nodes."
        ],
        "FuncName": "MIR_2491 operandIfRedundant.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "If we have phi(..., a, b, c, d, ..., z) and we plan to remove a, then first shift downward so that we have phi(..., b, c, d, ..., z, z). The operation of removing a from phi(..., a, b, c, d, ..., z) can be described as a truncation of the inputs list.",
            "on removing a, then first shift downward so that we have phi(..., b, c, d, ..., z, z):",
            "truncate the inputs list: This operation is specific to the database type phi function, where removing an operand involves shifting downward and truncating the inputs list."
        ],
        "FuncName": "MIR_2491 removeOperand.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "this == unused && other == unknown: This condition is related to Phi node replacement in database, where unused and unknown values are merged.",
                    "this == unknown && other == unused: This condition is related to Phi node replacement in database, where unknown and unused values are merged."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "This function is called to fix the current Phi flags using it as a replacement of the other Phi instruction |other|: This is a database-related operation, where Phi node replacement is used to fix flags.",
            "When dealing with usage analysis, any Use will replace all other values, such as Unused and Unknown: In database, this is related to Phi node replacement, where Use merges Unused and Unknown values."
        ],
        "FuncName": "MIR_2491 updateForReplacement.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "When folding to the constant we need to hoist it. This is a key step in optimizing MPhi ternary constructs."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "When folding to the constant we need to hoist it. This is a key step in optimizing MPhi ternary constructs."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "To verify if MPhi is a ternary construct, we check if it follows the pattern: x ? x : y or x ? y : x. This is a very loose term as it actually only checks for MTest X / \\ ... \\ / MPhi X Y",
            "In a ternary construct, the true branch may only dominate one edge of MPhi. This is a crucial property to check.",
            "In a ternary construct, the false branch may only dominate one edge of MPhi. This is a crucial property to check.",
            "In a ternary construct, the true and false branches must dominate different edges of MPhi. This is a crucial property to check.",
            "If we find a ternary construct, we can optimize it further.",
            "We can accept either the true or false branch in a ternary construct.",
            "A ternary construct can be represented as: testArg ? testArg : constant or testArg ? constant : testArg",
            "This check should be a tautology, except that the constant might be the result of the removal of a branch. In such case, the domination scope of the block which is holding the constant might be incomplete. This condition is used to prevent doing this optimization based on incomplete information.",
            "After GVN removes a branch, it will update the dominations rules before trying to fold this MPhi again. Thus, this condition does not inhibit this optimization.",
            "If testArg is an int32 type, we can fold testArg ? testArg : 0 to testArg and testArg ? 0 : testArg to 0.",
            "If testArg is a double type, we can fold testArg ? testArg : 0.0 to MNaNToZero(testArg).",
            "If testArg is a string type, we can fold testArg ? testArg :  to testArg and testArg ?  : testArg to ."
        ],
        "FuncName": "MIR_2491 foldsTernary.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "In the default case, there are no constants to fold."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Test if all uses have the same semantics for -0 and 0. In the database context, -0 and 0 represent negative zero and zero values respectively. This check ensures that the database is consistent in its handling of these values."
        ],
        "FuncName": "MIR_2491 NeedNegativeZeroCheck.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "This phi must be able to be any value. In a relational database, this means the column can be of any data type, such as integer, string, or date."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "MIR_2491 typeIncludes.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "The operand vector is initialized in reverse order by WarpBuilder. This is a common operation in database optimization, where the order of operands can significantly impact query performance.",
            "It cannot be checked for consistency until all arguments are added. In a database context, this means that the data is not yet validated and may contain inconsistencies that need to be addressed.",
            "FixedList doesn't initialize its elements, so do an unchecked init. This is a common pattern in database design, where a fixed-size list is used to store data that requires a specific structure."
        ],
        "FuncName": "MIR_2491 addArg.txt"
    },
    {
        "Branch": [
            "Test if this instruction can produce negative zero even when bailing out and changing types in conditional statements."
        ],
        "Loop": [
            "Test if this instruction can produce negative zero even when bailing out and changing types in loop conditions."
        ],
        "Normal": [
            "Test if this instruction can produce negative zero even when bailing out and changing types."
        ],
        "FuncName": "MIR_2491 CanProduceNegativeZero.txt"
    },
    {
        "Branch": [
            "In the If-Condition, the database can be folded if the condition is met."
        ],
        "Loop": [
            "In the Loop-Condition, the database can be folded if the loop is executed."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. The database can be folded if the data is consistent."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "If condition is a specific type of condition in the database, used to determine whether a certain action should be taken.",
            "If condition is a specific type of condition in the database, used to determine whether a certain action should be taken."
        ],
        "Loop": [
            "Loop condition is a specific type of condition in the database, used to control the repetition of a certain action.",
            "Loop condition is a specific type of condition in the database, used to control the repetition of a certain action."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. In a database context, a constant refers to a fixed value that is used in a query or calculation."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " If p in x^p isn't constant, we can't apply these folds. This is because constant folding requires constant exponents, which is not the case here.",
            " NOTE: The optimizations must match the optimizations used in |js::ecmaPow| to avoid differential testing issues. In other words, we need to ensure that our optimizations are consistent with the ones used in the ECMA specification.",
            " resp. |js::powi| to avoid differential testing issues.",
            " Math.pow(x, 0.5) is a sqrt with edge-case detection. This is a special case where we can optimize the power function by using a square root.",
            " Math.pow(x, -0.5) == 1 / Math.pow(x, 0.5), even for edge cases. This is a property of exponentiation that allows us to simplify the expression.",
            " Math.pow(x, 1) == x. This is a basic property of exponentiation where any number raised to the power of 1 is equal to itself.",
            " Math.pow(x, 2) == x*x. This is a simple property of exponentiation where any number raised to the power of 2 is equal to the number multiplied by itself.",
            " Math.pow(x, 3) == x*x*x. This is another simple property of exponentiation where any number raised to the power of 3 is equal to the number multiplied by itself three times.",
            " Math.pow(x, 4) == y*y, where y = x*x. This is a property of exponentiation where any number raised to the power of 4 can be simplified by squaring the result of squaring the base.",
            " No optimization"
        ],
        "FuncName": "MIR_2491 foldsConstantPower.txt"
    },
    {
        "Branch": [
            "In the if condition, the type should be a specific type, such as int or string."
        ],
        "Loop": [
            "In the loop condition, the type should be a specific type, such as int or string."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. The database type is used to store and manage data."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the If-Condition-Str database, we check if the condition can be folded into a specific type."
        ],
        "Loop": [
            "In the Loop-Condition-Str database, we verify if the loop condition can be folded into a specific type."
        ],
        "Normal": [
            "In the Normal-Str database, in the default case, there are no constants to fold into a specific type."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the context of a database, an if-condition string is a predicate that determines whether a specific type of data meets certain criteria, allowing for the execution of a particular action or query."
        ],
        "Loop": [
            "In a database setting, a loop-condition string defines the conditions under which a loop or iteration will be executed, often in relation to a specific type of data or query."
        ],
        "Normal": [
            "In the default case, there are no constants to fold, implying a straightforward or generic approach in database operations."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "Check if a variable is unsigned 32-bit integer"
        ],
        "Loop": [],
        "Normal": [
            " silence GCC warning"
        ],
        "FuncName": "MIR_2491 MustBeUInt32.txt"
    },
    {
        "Branch": [
            "In the if-branch, the WASM GC object can be folded into a specific type if the condition is met."
        ],
        "Loop": [
            "In the loop, the WASM GC object can be folded into a specific type for each iteration if the condition is met."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. The WASM GC object cannot be folded into a specific type."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: Conditional statement in concrete type, used to check if a condition is met."
        ],
        "Loop": [
            "Loop-Condition-Str: Loop condition in concrete type, used to control the loop iterations."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. This is a normal string that does not relate to the concrete type database."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "If the condition is true, then the code will execute.",
            "If the condition is false, then the code will skip."
        ],
        "Loop": [
            "While the condition is true, the loop will repeat.",
            "If the condition is false, the loop will exit."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. This is a normal string in a database context."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "Inheriting recovery points and initializing recovery stack for conditional statements in database code."
        ],
        "Loop": [
            "Inheriting recovery points and initializing recovery stack for loop conditions in database code."
        ],
        "Normal": [
            "FixedList doesn't initialize its elements, so do unchecked inits. In database context, this means that the FixedList data structure does not automatically initialize its elements, requiring manual unchecked initialization."
        ],
        "FuncName": "MIR_2491 inherit.txt"
    },
    {
        "Branch": [
            "In the context of WASM, an if-branch is folded when the condition is true."
        ],
        "Loop": [
            "In the context of WASM, a loop is folded when the condition is true."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. In the context of WASM, a constant is folded when its value is within the range of the WASM type."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the if condition, we check if the WASM object can be folded into a more specific type."
        ],
        "Loop": [
            "In the loop condition, we check if the WASM object can be folded into a more specific type."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. The WASM object cannot be folded into a more specific type."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: MuRsh instance for WASM type conversion"
        ],
        "Loop": [
            "Loop-Condition-Str: MuRsh instance for WASM type conversion"
        ],
        "Normal": [
            "Since Ion has no UInt32 type, we use Int32 and we have a special exception to the type rules: we can return values in (INT32_MIN,UINT32_MAX] and still claim that we have an Int32 type without bailing out. This is necessary because Ion has no UInt32 type and we can't have bailouts in wasm code. MuRsh instance for WASM type conversion is used to handle this scenario."
        ],
        "FuncName": "MIR_2491 NewWasm.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If the last resume point had the same side-effect stack, then we can reuse the current side effect without cloning it. This is a simple way to share common context by making a spaghetti stack. In database terms, this means that we can leverage the existing transactional context to avoid unnecessary overhead.",
                    "reuse the current side effect without cloning it. This is a simple way to share common context by making a spaghetti stack. In database terms, this means that we can leverage the existing transactional context to avoid unnecessary overhead."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Ensure that the store would not be deleted by DCE. This is a critical operation in database management, as it prevents data loss due to incorrect caching or memory management."
        ],
        "FuncName": "MIR_2491 addStore.txt"
    },
    {
        "Branch": [
            "In database, a conditional statement is used to check if two values are equal.",
            "This is often represented as 'lhs = rhs' or '!lhs = rhs'.",
            "However, in some cases, 'lhs = lhs' can be true even if lhs is NaN."
        ],
        "Loop": [],
        "Normal": [
            "In database, a conditional statement is used to check if two values are equal.",
            "This is often represented as 'lhs = rhs' or '!lhs = rhs'.",
            "However, in some cases, 'lhs = lhs' can be true even if lhs is NaN."
        ],
        "FuncName": "MIR_2491 tryFoldEqualOperands.txt"
    },
    {
        "Branch": [
            "Optimizing If condition based on string length comparison in database code"
        ],
        "Loop": [
            "Optimizing Loop condition based on string length comparison in database code"
        ],
        "Normal": [
            "Try to optimize string comparison by checking MStringLength in database code.",
            "Optimized comparison using MConstant(0) in database code."
        ],
        "FuncName": "MIR_2491 tryFoldStringCompare.txt"
    },
    {
        "Branch": [
            "The if condition is used to create a MObjectState object, calculating the number of slots and fixed slots based on the input shape."
        ],
        "Loop": [
            "The loop condition is used to create a MObjectState object, calculating the number of slots and fixed slots based on the input shape."
        ],
        "Normal": [
            "This instruction is only used as a summary for bailout paths. It is related to the database code and is used to create a MObjectState object, calculating the number of slots and fixed slots based on the input shape."
        ],
        "FuncName": "MIR_2491 MObjectState.txt"
    },
    {
        "Branch": [
            "This instruction is used to specify a condition for conditional branching in [database] queries, where the shape of the data is taken into account to compute the number of slots and fixed slots."
        ],
        "Loop": [
            "This instruction is used to specify a condition for looping in [database] queries, where the shape of the data is taken into account to compute the number of slots and fixed slots."
        ],
        "Normal": [
            "This instruction is only used as a summary for bailout paths, and does not participate in the computation of slots and fixed slots in [database] queries."
        ],
        "FuncName": "MIR_2491 MObjectState.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The Left Hand Side (LHS) is the value we want to test against null or undefined in a database query."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "MIR_2491 tryFold.txt"
    },
    {
        "Branch": [
            "In database code, this string is used to check if a condition is met.",
            "Fold |str.indexOf(searchStr) == 0| to |str.startsWith(searchStr)|."
        ],
        "Loop": [
            "In database code, this string is used to loop through a collection.",
            "No modifications needed."
        ],
        "Normal": [
            "One operand must be a constant integer.",
            "The constant must be zero.",
            "The other operand must be an indexOf operation.",
            "Fold |str.indexOf(searchStr) == 0| to |str.startsWith(searchStr)|.",
            "Invert for inequality.",
            "In database code, this string is related to string indexing operations.",
            "These operations are used to check if a string starts with a specified substring."
        ],
        "FuncName": "MIR_2491 tryFoldStringIndexOf.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Try to optimize MConstant(string) compare MCharCodeAt. This is a string constant in database.",
                    "as MConstant(charcode) compare MCharCodeAt. This is a character code in database."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Try to optimize MFromCharCode MCharCodeAt compare MFromCharCode MCharCodeAt as MCharCodeAt compare MCharCodeAt. This is a string comparison in database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "|str[i]| is compiled as MFromCharCode(MCharCodeAt(str, i)). This is a string manipulation in database."
        ],
        "FuncName": "MIR_2491 tryFoldCharCompare.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Unnecessary bit-ops haven't yet been removed. In database context, this may refer to inefficient bitwise operations that can be optimized."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "One operand must be a constant string. In database terms, this constant string is a literal value used in a query.",
            "The constant string must be non-empty. In database context, an empty string is not a valid operand for a substring operation.",
            "The other operand must be a substring operation. In database terms, this refers to extracting a subset of characters from a string using the SUBSTR function.",
            "We want to match this pattern: Substr(string, Constant(0), Min(Constant(length), StringLength(string))). In database context, this is a SQL query pattern that extracts a substring from a string, starting from position 0, with a minimum length equal to the smaller of the constant length and the string length.",
            "Ensure the string length matches the substring's length. In database terms, this means verifying that the length of the substring is equal to the length of the original string.",
            "Now fold code like |str.substring(0, 2) == \\aa| to |str.startsWith(aa)|. In database context, this is a code transformation that replaces a substring comparison with a starts-with comparison, improving query efficiency.",
            "Invert for inequality. In database terms, this means modifying the query to compare the substring with the string, rather than the other way around."
        ],
        "FuncName": "MIR_2491 tryFoldStringSubstring.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: A conditional statement in the database, used to evaluate a condition and perform an action if the condition is met."
        ],
        "Loop": [
            "Loop-Condition-Str: A loop statement in the database, used to execute a set of instructions repeatedly while a given condition is met."
        ],
        "Normal": [
            "MNewPlainObject uses a shape constant, not an object. In the database, a shape constant is a predefined value that represents a specific shape or form."
        ],
        "FuncName": "MIR_2491 templateObjectOf.txt"
    },
    {
        "Branch": [
            "Set object state if condition is met, as per database logic."
        ],
        "Loop": [
            "Iterate through object state if condition is true, based on database rules."
        ],
        "Normal": [
            "This instruction serves as a summary for bailout paths, adhering to database standards."
        ],
        "FuncName": "MIR_2491 MObjectState.txt"
    },
    {
        "Branch": [
            "This instruction is used to specify a condition for an if statement in the database context, where the condition is evaluated to determine the execution path."
        ],
        "Loop": [
            "This instruction is used to specify a condition for a loop in the database context, where the condition is evaluated to determine whether the loop should continue or terminate."
        ],
        "Normal": [
            "This instruction is only used as a summary for bailout paths in the database context, where it provides a brief description of the path taken when a bailout is executed."
        ],
        "FuncName": "MIR_2491 MArrayState.txt"
    },
    {
        "Branch": [
            "Check if the condition is true, then execute the code inside the if statement. This is a basic control flow in programming."
        ],
        "Loop": [
            "Repeat the code inside the loop for each item in the array. This is a basic looping mechanism in programming."
        ],
        "Normal": [
            "Initialize all the slots of the object state with the value contained in the template object. This is needed to account values which are baked in the template objects and not visible in IonMonkey, such as the uninitialized-lexical magic value of call objects. In the context of a database, this means populating the object's attributes with data from the database table."
        ],
        "FuncName": "MIR_2491 initFromTemplateObject.txt"
    },
    {
        "Branch": [
            "Check if the template object has a 'condition' attribute. If it does, check if the condition is true, otherwise, skip to the next iteration."
        ],
        "Loop": [
            "Check if the loop should continue. If the loop should continue, execute the next iteration."
        ],
        "Normal": [
            "Initialize all the slots of the object state with the value contained in the template object. This is needed to account values which are baked in the template objects and not visible in IonMonkey, such as the uninitialized-lexical magic value of call objects. In the context of a database, this means that the object state will contain the relevant data from the database, such as the primary key, foreign keys, and other relevant attributes."
        ],
        "FuncName": "MIR_2491 initFromTemplateObject.txt"
    },
    {
        "Branch": [
            "In a conditional statement, the condition must be a concrete type's subtype."
        ],
        "Loop": [
            "In a loop, the condition must be a concrete type's subtype."
        ],
        "Normal": [
            "In the default case, there are no constants to fold."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "If the condition is met, the type can be folded."
        ],
        "Loop": [
            "The loop condition determines whether the type can be folded."
        ],
        "Normal": [
            " In the default case, there are no constants to fold. For database types, this typically means that the type is not foldable."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In a conditional statement, we check if the type is a subclass of a specific type and eliminate unnecessary type conversions."
        ],
        "Loop": [
            "In a loop, we check if the type is a subclass of a specific type and eliminate unnecessary type conversions."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. This is a normal string without any specific database-related content."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the context of database folding, an if condition is a boolean expression that determines whether a specific transformation can be applied to a column."
        ],
        "Loop": [
            "In database folding, a loop condition is a rule that governs the repetition of a transformation on a column, such as folding integers within a certain range."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. Database folding involves transforming data types in a way that preserves the original data's integrity and facilitates efficient querying and analysis."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " Be conservative and only allow values that fit into int32. This is a database code string related to conditional statements."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            " Loop condition statement. This is a database code string related to loop statements."
        ],
        "Normal": [
            " For constants check they are not equal. This is a database code string related to constant comparison.",
            " Check if \\ins1 = ins2 + cte, which would make both instructions have different values. This is a database code string related to instruction comparison."
        ],
        "FuncName": "MIR_2491 DefinitelyDifferentValue.txt"
    },
    {
        "Branch": [
            " In the if condition, the type must be convertible to Int32."
        ],
        "Loop": [
            " In the loop condition, the type must be convertible to Int32."
        ],
        "Normal": [
            " In the default case, there are no constants to fold. For database, Int32 is a common type used for integer values."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the if condition, we check if the source type can be folded into the target type."
        ],
        "Loop": [
            "In the loop condition, we check if the source type can be folded into the target type."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. The source type can be folded into the target type if it is compatible."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: This is a conditional statement in database code, used to check a condition and perform an action if it is true."
        ],
        "Loop": [
            "Loop-Condition-Str: This is a loop condition in database code, used to repeat a set of actions until a certain condition is met."
        ],
        "Normal": [
            "Normal-Str: In the default case, there are no constants to fold. This is a normal string in database code, used to store or display data."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Drop the MToNumberInt32 added by the TypePolicy for double and float values. This is a common optimization in database systems to improve performance.",
            "Ignore the bounds check, which do not modify the index. This is a common technique used in database indexing to improve query performance.",
            "Masking the index for Spectre-mitigation is not observable. This is a security feature in some database systems to prevent side-channel attacks."
        ],
        "FuncName": "MIR_2491 SkipUninterestingInstructions.txt"
    },
    {
        "Branch": [
            "In the context of WASM, this condition checks if the type is a subtype of a concrete type."
        ],
        "Loop": [],
        "Normal": [
            "In the default case, there are no constants to fold. This is a description of a WASM type verification process."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: This is a condition that checks if a value is equal to a specific value in the database."
        ],
        "Loop": [
            "Loop-Condition-Str: This is a condition that loops through a set of values in the database."
        ],
        "Normal": [
            " In the default case, there are no constants to fold from the database."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the if-branch, the constant is folded into a specific type."
        ],
        "Loop": [
            "In the loop-branch, the constant is folded into a specific type."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. This is a description of the database type."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the If-Condition, the control flow is based on the WASM type's sub-type, which is used to determine the condition for the if statement."
        ],
        "Loop": [
            "In the Loop-Condition, the control flow is based on the WASM type's sub-type, which is used to determine the condition for the loop."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. The WASM type's sub-type is used to determine the specific operation to perform."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the IF-Condition, we check the type of the WASM sub-type."
        ],
        "Loop": [
            "In the Loop-Condition, we check the type of the WASM sub-type."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. The WASM sub-type is not applicable."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the if condition, we check if the expression can be folded using a simple WASM type conversion."
        ],
        "Loop": [
            "In the loop condition, we check if the expression can be folded using a simple WASM type conversion."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. These constants are related to the database type and can be folded using a simple WASM type conversion."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "In the default case, there are no constants to fold. This is a specific type of statement in the database that checks if a WASM type is a subtype of a specific type."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the default case, there are no constants to fold. The WASM type is not a specific type."
        ],
        "Loop": [
            "The loop condition is not specified. The WASM type is not a specific type."
        ],
        "Normal": [
            "In the default case, there are no constants to fold."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "In the default case there are no constants to fold."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "In the default case, there are no constants to fold."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the if condition, we can fold the type to a specific type, such as int or string."
        ],
        "Loop": [
            "In the loop condition, we can fold the type to a specific type, such as int or string."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. Database type allows for specific type folding, such as int or string."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "If the condition is true, then the statement is executed."
        ],
        "Loop": [
            "Loop through the database table until the condition is met."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. The database system will automatically handle the data type conversion."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the context of database, the if condition is used to check if a certain condition is met before executing a specific action."
        ],
        "Loop": [
            "In database, the loop condition is used to repeat a set of actions until a certain condition is met."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. In database, constants are values that are used repeatedly throughout the code and can be replaced with a single value to improve performance."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "In the context of a database, an if-condition is a predicate that evaluates to true or false, used to determine whether a specific sub-type is present."
        ],
        "Loop": [
            "In a database, a loop-condition is a control structure that repeats a set of instructions until a specific sub-type is met or a condition is satisfied."
        ],
        "Normal": [
            "In the default case, there are no constants to fold. However, in a database context, constants can be folded to reduce data redundancy and improve query performance."
        ],
        "FuncName": "MIR_2491 foldsTo.txt"
    },
    {
        "Branch": [
            "The if condition is a boolean expression that must be evaluated to true for the if statement to execute."
        ],
        "Loop": [
            "The loop condition is a boolean expression that determines whether the loop should continue or terminate."
        ],
        "Normal": [
            "Construct calls are always returning a boxed object.",
            "",
            "TODO: We should consider encoding this directly in the graph instead of having to special case it here.",
            "In a database context, this means that the construct call is returning a reference to a table or a query result."
        ],
        "FuncName": "MIR_2491 IsBoxedObject.txt"
    },
    {
        "Branch": [
            "Get WarpOpSnapshot object for if condition."
        ],
        "Loop": [
            "Get WarpOpSnapshot object for loop condition."
        ],
        "Normal": [
            "Skip snapshots until we get to a snapshot with offset >= offset. This is a loop because WarpBuilder can skip unreachable bytecode ops. WarpOpSnapshot provides information about the current state of the Warp database."
        ],
        "FuncName": "WarpBuilder_2807 getOpSnapshotImpl.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Initialize this.",
                    "Initialize arguments.",
                    "Stack frame initialization is a key aspect of database operations, ensuring that each stack frame is properly set up for efficient execution."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "In database contexts, loop conditions are crucial for managing iterations and ensuring data consistency.",
                    "Stack frame initialization plays a vital role in preparing for loop iterations, guaranteeing accurate and reliable results."
                ]
            }
        ],
        "Normal": [
            {
                "normalstr": [
                    "Initialize local slots.",
                    "Initialize the environment chain, return value, and arguments object slots.",
                    "These operations are fundamental to database operations, enabling the efficient management of data and stack frames.",
                    "Guard against over-recursion to prevent stack overflow and ensure database stability."
                ]
            }
        ],
        "FuncName": "WarpBuilder_2807 buildPrologue.txt"
    },
    {
        "Branch": [
            "If the condition is met, then the environment slot will be updated from UndefinedValue only after the initial environment is created, ensuring bailout doesn't see a partial environment, as described in BaselineStackBuilder::buildBaselineFrame."
        ],
        "Loop": [],
        "Normal": [
            "Update the environment slot from UndefinedValue only after the initial environment is created, so that bailout doesn't see a partial environment; see BaselineStackBuilder::buildBaselineFrame for details."
        ],
        "FuncName": "WarpBuilder_2807 buildEnvironmentChain.txt"
    },
    {
        "Branch": [
            "Assert in debug mode we can elide the post write barriers. This is a conditional statement in the database code.",
            "This condition is used to control the flow of the program in the database."
        ],
        "Loop": [
            "This loop condition is used in the database code to iterate over a collection of data.",
            "The loop is used to perform a specific action for each item in the collection."
        ],
        "Normal": [
            "Assert in debug mode we can elide the post write barriers. This is a normal statement in the database code.",
            "Initialize the object's reserved slots. No post barrier is needed here, for the same reason as in buildNamedLambdaEnv. This is a description of the database code."
        ],
        "FuncName": "WarpBuilder_2807 buildCallObject.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Initialize this parameter.",
                    "Initialize arguments. There are three cases:",
                    "",
                    "1) There's no ArgumentsObject or it doesn't alias formals. In this case",
                    "    we can just use the frame's argument slot.",
                    "2) The ArgumentsObject aliases formals and the argument is stored in the",
                    "    CallObject. Use undefined because we can't load from the arguments",
                    "    object and code will use the CallObject anyway.",
                    "3) The ArgumentsObject aliases formals and the argument isn't stored in",
                    "    the CallObject. We have to load it from the ArgumentsObject.",
                    "Create the OSR preheader block, which is a critical component of the OSR",
                    "process. This block will contain the necessary instructions to handle the",
                    "different cases of argument loading."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Create two blocks:",
            " * The OSR entry block. This is always the graph's second block and has no",
            "   predecessors. This is the entry point for OSR from the Baseline JIT.",
            " * The OSR preheader block. This has two predecessors: the OSR entry block",
            "   and the current block.",
            "The OSR preheader block is where we perform the necessary operations to",
            "prepare the OSR process. This includes initializing the environment chain,",
            "return value, arguments object, locals, and expression stack slots.",
            "Note: phi specialization can add type guard instructions to the OSR entry",
            "block if needed. See TypeAnalyzer::shouldSpecializeOsrPhis.",
            "Create the preheader block, with the predecessor block and OSR block as",
            "predecessors."
        ],
        "FuncName": "WarpBuilder_2807 startNewOsrPreHeaderBlock.txt"
    },
    {
        "Branch": [
            "If the condition is met, jump to the specified instruction.",
            "These strings are used in the database code."
        ],
        "Loop": [
            "Check if the loop condition is met.",
            "These strings are used in the database code."
        ],
        "Normal": [
            "We can finish the loop now. Use the loophead pc instead of the current pc because the stack depth at the start of that op matches the current stack depth (after popping our operand).",
            "ifFalse = nullptr;"
        ],
        "FuncName": "WarpBuilder_2807 buildTestBackedge.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "No reachable jumps so this is just a no-op. In the context of database query optimization, this statement indicates that the IF-CONDITION statement has no effect on the query execution plan, similar to a NO-OP operation in database processing."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Create join block if there's fall-through from the previous bytecode op. In database terminology, this statement describes the process of creating a join operation in a query, where the fall-through from the previous bytecode operation is treated as a join condition."
        ],
        "FuncName": "WarpBuilder_2807 build_JumpTarget.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Finish any broken loops with an unreachable backedge. For example:",
                    "",
                    "   do {",
                    "     ...",
                    "     return;",
                    "     ...",
                    "   } while (x);",
                    "",
                    " This loop never actually loops.",
                    "This is a conditional statement in the database code."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Skip unreachable ops (for example code after a 'return' or 'throw') until",
                    "we get to the next jump target.",
                    "This is a loop condition in the database code that controls the flow of execution."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "WarpBuilder_2807 buildBody.txt"
    },
    {
        "Branch": [
            "ifFalse= nullptr; if condition is false, the program will jump to the else branch."
        ],
        "Loop": [],
        "Normal": [
            "ifFalse= nullptr; if condition is false, the program will jump to the else branch."
        ],
        "FuncName": "WarpBuilder_2807 build_Coalesce.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The whole loop is unreachable.",
                    "This is a conditional statement in the database code, where the condition is always false, making the loop unreachable."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "This string is related to the loop condition in the database code, where the loop will continue to execute as long as the condition is true."
        ],
        "Normal": [
            "All loops have the following bytecode structure:",
            "",
            "    LoopHead",
            "    ...",
            "    JumpIfTrue/Goto to LoopHead",
            "Handle OSR from Baseline JIT code.",
            "This is a typical structure for a loop in the database code, where the LoopHead is the starting point and the JumpIfTrue/Goto instruction is used to jump back to the LoopHead if the condition is true."
        ],
        "FuncName": "WarpBuilder_2807 build_LoopHead.txt"
    },
    {
        "Branch": [
            "Condition for if statement based on database iteration"
        ],
        "Loop": [
            {
                "loopstr": [
                    "Stop iterating if we reach an outer loop because outer loops were already processed when we visited their loop headers in database iteration"
                ]
            }
        ],
        "Normal": [
            "When unwinding the stack for a thrown exception in database iteration, the exception handler must close live iterators. For ForIn and Destructuring loops, the exception handler needs access to values on the stack. To prevent them from being optimized away (and replaced with the JS_OPTIMIZED_OUT MagicValue), we need to mark the phis (and phis they flow into) as having implicit uses in database processing.",
            "See ProcessTryNotes in vm/Interpreter.cpp and CloseLiveIteratorIon in jit/JitFrames.cpp for database iteration details"
        ],
        "FuncName": "WarpBuilder_2807 addIteratorLoopPhis.txt"
    },
    {
        "Branch": [
            "Set the hasTryBlock flag to turn off optimizations that eliminate dead resume points operands due to the exception handler code for TryNoteKind::Destructuring, a specialized catch-block in database code."
        ],
        "Loop": [],
        "Normal": [
            "Set the hasTryBlock flag to turn off optimizations that eliminate dead resume points operands because the exception handler code for TryNoteKind::Destructuring is effectively a (specialized) catch-block in database code."
        ],
        "FuncName": "WarpBuilder_2807 build_TryDestructuring.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Generate entry block for database initialization.",
            "Connect entry block to last block in caller's graph for database operations.",
            "Initialize environment chain slot to Undefined; it's set later by buildEnvironmentChain for database connection.",
            "Initialize return value slot for database query results.",
            "Initialize arguments slot if needed for database function calls.",
            "Initialize this slot for database object creation.",
            "Initialize actually set arguments for database parameter passing.",
            "Pass undefined for missing database arguments.",
            "Initialize local slots for database variable storage."
        ],
        "FuncName": "WarpBuilder_2807 buildInlinePrologue.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u5982\u679c\u6211\u4eec\u6709\u7f13\u5b58IR\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u4f18\u5316\u8f93\u5165\u3002\u6ce8\u610f\uff0c\u6761\u4ef6\u8bed\u53e5\u5728\u6570\u636e\u5e93\u4e2d\u975e\u5e38\u91cd\u8981\uff0c\u7528\u4e8e\u751f\u6210\u6d4b\u8bd5\u64cd\u4f5c\u3002",
                    "\u8f6c\u8bd1\u5668\u4e0d\u4f1a\u751f\u6210\u4efb\u4f55\u63a7\u5236\u6307\u4ee4\u3002\u76f8\u53cd\uff0c\u6211\u4eec\u4f1a\u901a\u8fc7\u5e76\u751f\u6210\u5b83\u4eec\uff0c\u5229\u7528\u6570\u636e\u5e93\u7684\u6761\u4ef6\u8bed\u53e5\u529f\u80fd\u3002",
                    " "
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "JSOp::And\u548cJSOp::Or\u4e0d\u4f1a\u6539\u53d8\u6808\u9876\u503c\u3002\u6808\u9876\u503c\u53ef\u80fd\u5df2\u7ecf\u88ab\u8f6c\u6362\u4e3a\u5e03\u5c14\u503c\uff0c\u5229\u7528\u6570\u636e\u5e93\u7684\u903b\u8f91\u8fd0\u7b97\u529f\u80fd\u3002",
            "\u901a\u8fc7\u8f6c\u8bd1\u7684ToBool IC\uff0c\u56e0\u6b64\u6211\u4eec\u63a8\u5165\u539f\u59cb\u503c\uff0c\u6570\u636e\u5e93\u4e2d\u7684\u6761\u4ef6\u8bed\u53e5\u4f1a\u6839\u636e\u903b\u8f91\u8fd0\u7b97\u8fdb\u884c\u5224\u65ad\u3002",
            "\u5982\u679c\u8fd9\u4e2a\u64cd\u4f5c\u603b\u662f\u8df3\u8f6c\u5230\u540c\u4e00\u4e2a\u4f4d\u7f6e\uff0c\u6211\u4eec\u5c06\u5176\u89c6\u4e3aJSOp::Goto\uff0c\u6570\u636e\u5e93\u4e2d\u7684\u8df3\u8f6c\u64cd\u4f5c\u4f1a\u6839\u636e\u6761\u4ef6\u8bed\u53e5\u8fdb\u884c\u8df3\u8f6c\u3002",
            "ifFalse = nullptr);",
            "JSOp::Case\u5fc5\u987b\u5f39\u51fa\u7b2c\u4e8c\u4e2a\u503c\uff08\u8f93\u5165\u7684switch\u8bed\u53e5\uff09\uff0c\u6570\u636e\u5e93\u4e2d\u7684switch\u8bed\u53e5\u4f1a\u6839\u636e\u6761\u4ef6\u8bed\u53e5\u8fdb\u884c\u5339\u914d\u3002"
        ],
        "FuncName": "WarpBuilder_2807 buildTestOp.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Iterator value is not used. This statement is used to end the iteration in the database code."
        ],
        "FuncName": "WarpBuilder_2807 build_EndIter.txt"
    },
    {
        "Branch": [
            "Translate the if condition into MIR."
        ],
        "Loop": [
            "Translate the loop condition into MIR."
        ],
        "Normal": [
            "Translate the call operation into MIR, synthesizing a constant number of arguments for this call op."
        ],
        "FuncName": "WarpBuilder_2807 transpileCall.txt"
    },
    {
        "Branch": [
            "Inline this-object allocation on caller-side for conditional statements in the database."
        ],
        "Loop": [
            "Inline this-object allocation on caller-side for loop conditions in the database."
        ],
        "Normal": [
            "Inline the this-object allocation on the caller-side for normal database operations."
        ],
        "FuncName": "WarpBuilder_2807 buildCreateThis.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u5982\u679c\u6211\u4eec\u6709CacheIR\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528\u5b83\u6765\u4f18\u5316\u8f93\u5165\u4e4b\u524d\uff0c\u901a\u8fc7\u4f7f\u7528NOT\u8fd0\u7b97\u7b26\u6765\u51cf\u5c11IR\u7684\u590d\u6742\u6027",
                    "\u53d1\u51faMNot\uff0cNOT\u8fd0\u7b97\u7b26\u7528\u4e8e\u4f18\u5316IR"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "WarpBuilder_2807 build_Not.txt"
    },
    {
        "Branch": [
            "In a conditional statement, we need to re-create the lexical environment to ensure the condition is evaluated correctly."
        ],
        "Loop": [
            "In a loop, the lexical environment needs to be re-created on each iteration to maintain the loop's context."
        ],
        "Normal": [
            "Assert in debug mode we can elide the post write barrier. This is because the object's state is not affected by the assertion.",
            "Initialize the object's reserved slots. No post barrier is needed here, as the slots are not being modified.",
            "for the same reason as in buildNamedLambdaEnv, which is that the object's state is not affected by the lambda function's creation."
        ],
        "FuncName": "WarpBuilder_2807 build_RecreateLexicalEnv.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "In strict mode, no need to wrap primitive this. This is a characteristic of strict mode."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "conditionstr": [
                    "In non-strict scope, primitive this must be wrapped."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "WarpBuilder_2807 build_FunctionThis.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Assert in debug mode we can elide the post write barrier. This is a freshened environment."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "A loop condition is used to control the iteration of a loop. This is a freshened environment."
        ],
        "Normal": [
            "Assert in debug mode we can elide the post write barrier. This is a freshened environment.",
            "Initialize the object's reserved slots. No post barrier is needed here, as it is part of the database's schema.",
            "for the same reason as in buildNamedLambdaEnv. This is a freshened environment.",
            "Copy environment slots. This is a freshened environment."
        ],
        "FuncName": "WarpBuilder_2807 build_FreshenLexicalEnv.txt"
    },
    {
        "Branch": [
            " Push the index of the next element and set the element based on the if condition in the database."
        ],
        "Loop": [
            " Push the index of the next element and set the element based on the loop condition in the database."
        ],
        "Normal": [
            " Push index + 1 and set the element based on the normal condition in the database."
        ],
        "FuncName": "WarpBuilder_2807 build_InitElemInc.txt"
    },
    {
        "Branch": [
            "Close the generator and return the final generated value."
        ],
        "Loop": [
            "Close the generator and return the final generated value."
        ],
        "Normal": [
            "Close the generator and return the final generated value.",
            "Return the final generated value."
        ],
        "FuncName": "WarpBuilder_2807 build_FinalYieldRval.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Assert in debug mode we can elide the post write barrier, which is a common optimization in database query optimization.",
            " Initialize the object's reserved slots, which are used to store metadata in the database, such as object creation time and last access time. No post barrier is needed here, for the same reason as in buildNamedLambdaEnv, which is to ensure data consistency in the database."
        ],
        "FuncName": "WarpBuilder_2807 build_PushVarEnv.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If we have previously had a failed lexical check in Ion, we want to avoid hoisting any lexical checks, which can cause spurious failures. In this case, we also have to be careful not to hoist any loads of this lexical past the check. For unaliased lexical variables, we can set the local slot to create a dependency. For aliased lexicals, that doesn't work, so we disable LICM instead. This is a key consideration in database schema design, where lexical checks are crucial for data integrity.",
                    "Set the local slot so that a subsequent GetLocal without a CheckLexical the frontend can elide lexical checks) doesn't let a definition with MIRType::MagicUninitializedLexical escape to arbitrary MIR instructions. Note that in this case the GetLocal would be unreachable because we throw an exception here, but we still generate MIR instructions for it. In database terminology, this is akin to enforcing referential integrity constraints."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "WarpBuilder_2807 buildCheckLexicalOp.txt"
    },
    {
        "Branch": [
            "Generate bytecode for conditional jumps in the database code."
        ],
        "Loop": [
            {
                "loopstr": [
                    "Use peekUnchecked method for reading argument slots in database code."
                ]
            }
        ],
        "Normal": [
            "If required, unbox the generator object explicitly in database code.",
            "",
            "This is done to avoid fuzz-bugs where ApplyTypeInformation does the",
            "unboxing, and generates fallible unboxes which can lead to torn object",
            "state due to `bailAfter`.",
            "Update Generator Object state in database code.",
            "This store is unbarriered, as it's only ever storing an integer, and as",
            "such doesn't partake of object tracing.",
            "This store is barriered because it stores an object value in database code.",
            "GeneratorReturn will return from the method, however to support MIR",
            "generation isn't treated like the end of a block in database code.",
            "To ensure the rest of the MIR generation looks correct, fill the stack with",
            "the appropriately typed MUnreachable's for the stack pushes from this",
            "opcode in database code."
        ],
        "FuncName": "WarpBuilder_2807 buildSuspend.txt"
    },
    {
        "Branch": [
            " The if condition in a debugger statement will only be evaluated if the realm is a",
            " debuggee realm with an onDebuggerStatement hook."
        ],
        "Loop": [],
        "Normal": [
            " The debugger; statement will bail out to Baseline if the realm is a debuggee realm with an onDebuggerStatement hook.",
            " This allows for the possibility of a debugger being attached to the debuggee and still being able to execute the debugger statement."
        ],
        "FuncName": "WarpBuilder_2807 build_Debugger.txt"
    },
    {
        "Branch": [
            "Unreachable blocks in if conditions don't require a bail instruction.",
            "This comes after a yield, which we generate as a return,",
            "so we know this should be unreachable code.",
            "",
            "We emit an unreachable bail for this, which will assert if we",
            "ever execute this.",
            "",
            "An Unreachable bail, instead of MUnreachable, because MUnreachable",
            "is a control instruction, and injecting it in the middle of a block",
            "causes various graph state assertions to fail.",
            "In database terms, this is related to the implementation of IF-THEN-ELSE logic."
        ],
        "Loop": [
            "Unreachable blocks in loop conditions don't require a bail instruction.",
            "This comes after a yield, which we generate as a return,",
            "so we know this should be unreachable code.",
            "",
            "We emit an unreachable bail for this, which will assert if we",
            "ever execute this.",
            "",
            "An Unreachable bail, instead of MUnreachable, because MUnreachable",
            "is a control instruction, and injecting it in the middle of a block",
            "causes various graph state assertions to fail.",
            "In database terms, this is related to the implementation of LOOP logic."
        ],
        "Normal": [
            "Unreachable blocks don't need to generate a bail.",
            "This comes after a yield, which we generate as a return,",
            "so we know this should be unreachable code.",
            "",
            "We emit an unreachable bail for this, which will assert if we",
            "ever execute this.",
            "",
            "An Unreachable bail, instead of MUnreachable, because MUnreachable",
            "is a control instruction, and injecting it in the middle of a block",
            "causes various graph state assertions to fail.",
            "In database terms, this is related to the implementation of unconditional jumps."
        ],
        "FuncName": "WarpBuilder_2807 build_AfterYield.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "To generate MIR for unreachable code, we must ensure correct stack manipulation.",
            "This is typically outside the scope of yield* semantics, but we also need to handle forced generator returns.",
            "In Warp, forced generator returns can be compiled, so we generate a bailout to run in baseline.",
            "Additionally, we mark operands as implicitly used.",
            "However, we must bail out if we encounter CheckResumeKind to maintain correctness."
        ],
        "FuncName": "WarpBuilder_2807 build_CheckResumeKind.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "TODO: IonBuilder has code to mark non-movable. See buildCheckLexicalOp. In WarpGetImport, this involves loading the target environment's import table."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Load the target environment's import table and initialize the import operation."
        ],
        "FuncName": "WarpBuilder_2807 build_GetImport.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Note: getInitElemArrayIndex asserts the index fits in int32.",
            " Note: InitArrayElemOperation asserts the index does not exceed the array's",
            " dense element capacity."
        ],
        "FuncName": "WarpBuilder_2807 build_InitElemArray.txt"
    },
    {
        "Branch": [
            "Check if the condition is met in the database."
        ],
        "Loop": [
            "Iterate through the database records."
        ],
        "Normal": [
            "Terminate the block.",
            "This action is related to database transaction management."
        ],
        "FuncName": "WarpBuilder_2807 build_ThrowMsg.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Terminate  the  block.  These  strings  are  used  to  construct  database  queries."
        ],
        "FuncName": "WarpBuilder_2807 build_Throw.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "insert string before string index"
                ],
                "elsestr": [
                    "database if condition statement"
                ]
            },
            {
                "thenstr": [
                    "insert string after string index"
                ],
                "elsestr": [
                    "database if condition statement"
                ]
            }
        ],
        "Loop": [
            {
                "thenstr": [
                    "database loop condition statement"
                ],
                "elsestr": [
                    "database loop condition statement"
                ]
            }
        ],
        "Normal": [
            {
                "thenstr": [
                    "database normal statement"
                ],
                "elsestr": [
                    "database normal statement"
                ]
            }
        ],
        "FuncName": "alphaindex_7868 binarySearch.txt"
    },
    {
        "Branch": [
            "Table mapping from target bytecode offset to MTableSwitch successor index: This is a crucial operation in database queries, ensuring that the correct successor index is retrieved for a given target bytecode offset.",
            "This prevents adding multiple predecessor/successor edges to the same target block: In database queries, this is essential to avoid invalid edge configurations in MIR, which can lead to query errors.",
            "Create |default| edge: This database operation creates a default edge in the graph, providing a fallback option when no other edge is available.",
            "Add all cases: This database operation adds all possible cases to the query, ensuring that the database is updated with the latest information."
        ],
        "Loop": [],
        "Normal": [
            "Table mapping from target bytecode offset to MTableSwitch successor index.",
            "This prevents adding multiple predecessor/successor edges to the same target block, which isn't valid in MIR.",
            "Create |default| edge.",
            "Add all cases."
        ],
        "FuncName": "WarpBuilder_2807 build_TableSwitch.txt"
    },
    {
        "Branch": [
            "Check the condition and throw an exception if it's not met."
        ],
        "Loop": [
            "Check the loop condition and throw an exception if it's not met."
        ],
        "Normal": [
            "Terminate the block. This statement is used to exit the current block of code in the database."
        ],
        "FuncName": "WarpBuilder_2807 build_ThrowSetConst.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If we are inlining, we know the actual arguments.",
                    "TODO: support pre-tenuring.",
                    "Allocate an array of the correct size.",
                    "Unroll the argument copy loop. We don't need to do any bounds or hole",
                    "checking here.",
                    "Update the initialized length for all the (necessarily non-hole)",
                    "elements added. This is a bytecode location in a WarpRest database.",
                    "It involves handling inlineCallInfo() and non-inlineCallInfo() calls."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "No more updating to do. This is a bytecode location in a WarpRest database.",
                    "It involves handling inlineCallInfo() and non-inlineCallInfo() calls."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "This is a bytecode location in a WarpRest database.",
            "It involves handling inlineCallInfo() and non-inlineCallInfo() calls."
        ],
        "Normal": [
            "NOTE: Keep this code in sync with |ArgumentsReplacer|.",
            "Pass in the number of actual arguments, the number of formals (not",
            "including the rest parameter slot itself), and the shape. This is a bytecode location in a WarpRest database.",
            "It involves handling inlineCallInfo() and non-inlineCallInfo() calls."
        ],
        "FuncName": "WarpBuilder_2807 build_Rest.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The CallInfo will be initialized by the transpiler. This is a database operation to create a cache."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "This database operation involves a loop condition to fetch data from the database."
        ],
        "Normal": [
            "Work around std::initializer_list not defining operator[]. This database operation involves a normal string to query the database."
        ],
        "FuncName": "WarpBuilder_2807 buildIC.txt"
    },
    {
        "Branch": [
            " Accumulate multiple returns with a phi. If-Condition"
        ],
        "Loop": [
            " Accumulate multiple returns with a phi. Loop-Condition"
        ],
        "Normal": [
            " Accumulate multiple returns with a phi. Accumulate multiple returns with a phi."
        ],
        "FuncName": "WarpBuilder_2807 patchInlinedReturns.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " buildInlinedCall: builds the logic for inlining function calls in WarpBuilder. This is used in database code. build_SetProp pushes the rhs argument onto the stack. Remove it in preparation for pushCallStack.",
                    " in preparation for pushCallStack."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Note: Inlining only aborts on OOM.  If inlining would fail for any other reason, we detect it in advance and don't inline. This is a database-specific optimization.",
                    " any other reason, we detect it in advance and don't inline."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " Capture formals in the outer resume point. This is a critical step in building the database graph.",
            " Pop formals again, except leave |callee| on stack for duration of call. This is necessary for database function calls.",
            " Build the graph. This is where the database logic is constructed.",
            " We mark scripts as uninlineable in BytecodeAnalysis if we cannot reach a return statement (without going through a catch/finally). This is a database-specific optimization.",
            " Create return block. This is a key step in database function calls.",
            " Restore previous value of callerResumePoint. This is necessary for database function calls.",
            " Pop |callee|. This is a critical step in database function calls.",
            " Accumulate return values. This is where the database results are stored.",
            " Initialize entry slots. This is a key step in database function calls."
        ],
        "FuncName": "WarpBuilder_2807 buildInlinedCall.txt"
    },
    {
        "Branch": [
            "binary search in database"
        ],
        "Loop": [
            "database loop"
        ],
        "Normal": [
            "binary search"
        ],
        "FuncName": "alphaindex_7868 getBucketIndex.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "stroke count",
                    "database query condition"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "alphaindex_7868 fixLabel.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " In C++ the ImmutableIndex must own its copy of the BucketList, including database records for proper memory management.",
            " even if it contains no database records, the ImmutableIndex still needs to own its copy of the BucketList.",
            " We could clone the buckets if they are not NULL, which would be beneficial if this method is called multiple times or after using the old-style bucket iterator API.",
            " The ImmutableIndex adopted its parameter objects, ensuring database compatibility."
        ],
        "FuncName": "alphaindex_7868 buildImmutableIndex.txt"
    },
    {
        "Branch": [
            "Get current tag iterator index for if condition."
        ],
        "Loop": [
            "Get current tag iterator index for loop condition."
        ],
        "Normal": [
            "Perform binary search on database records."
        ],
        "FuncName": "alphaindex_7868 getBucketIndex.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "A-Z",
                    "There are Pinyin labels, add ASCII A-Z labels as well.",
                    "Database supports Chinese index characters, it also supports ASCII A-Z labels."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "alphaindex_7868 addChineseIndexCharacters.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " Use a label if it is marked with one trailing star,",
                    " even if the label string sorts the same when all contractions are suppressed.",
                    " This is a special case in database code for string matching."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Ignore a primary-ignorable or non-alphabetic index character.",
                    " This is a common case in database indexing for non-string data."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Ignore an index character that will land in the overflow bucket.",
                    " This is a rare case in database indexing for very large datasets."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Ignore a multi-code point index character that does not sort distinctly",
                    " from the sequence of its separate characters.",
                    " This is a special case in database code for string matching and indexing."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " We make a sorted array of elements.",
            " Some of the input may be redundant.",
            " That is, we might have c, ch, d, where ch sorts just like c, h.",
            " We filter out those cases.",
            " if the result is still too large, cut down to maxLabelCount_ elements, by removing every nth element",
            " This is a common operation in database query optimization for reducing memory usage."
        ],
        "FuncName": "alphaindex_7868 initLabels.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " We crossed the script boundary into a new script. This is a common operation in database transactions, where we switch from one script to another."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " We are skipping one or more scripts, and we are not just getting out of the underflow label. This is a typical scenario in database queries, where we skip certain scripts to optimize performance."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " A-Z. In database indexing, A-Z is a common range used to group characters together."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " There is no single-character bucket since the last underflow or inflow label. In database design, this is a common issue that can affect data consistency."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Add an invisible bucket that redirects strings greater than the expansion to the previous single-character bucket. For example, after ... Q R S Sch we add Sch\ufffd->S, and after ... Q R S Sch Sch\ufffd St we add St\ufffd->S. This is a technique used in database optimization to reduce data redundancy."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    " Add a bucket with the current label. Remember ASCII and Pinyin buckets for Pinyin redirects. Check for multiple primary weights. In database indexing, this is a crucial step to ensure data accuracy and efficiency."
                ]
            }
        ],
        "Normal": [
            " Initialize indexCharacters. Variables for hasMultiplePrimaryWeights(). Helper arrays for Chinese Pinyin collation. underflow bucket fix up the list, adding underflow, additions, overflow Insert inflow labels as needed. This is a routine database operation to maintain data integrity."
        ],
        "FuncName": "alphaindex_7868 createBucketList.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "not primary ignorable, related to database query conditions"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "alphaindex_7868 hasMultiplePrimaryWeights.txt"
    },
    {
        "Branch": [
            "Initialize bucket list, sort input list by name, and assign records to corresponding buckets."
        ],
        "Loop": [
            {
                "loopstr": [
                    "If current bucket is incorrect, find the correct bucket.",
                    "We have a special flag to mark the last bucket, do not search further.",
                    "Now, insert the record into the bucket."
                ]
            },
            {
                "loopstr": [
                    "Now, reset the boundary we are comparing."
                ]
            }
        ],
        "Normal": [
            "Sort by name.",
            "Stable sorting preserves the input's original order.",
            "Now, we iterate over all inputs, inputs are already sorted.",
            "If an item is not in the current bucket, find the next bucket containing it.",
            "This makes the process's time complexity O(n*log(n)), because we only need to sort the list and then perform a linear process.",
            "However, if users add one item at a time and then retrieve buckets, this is not efficient, so",
            "We need to improve it to adapt to this situation."
        ],
        "FuncName": "alphaindex_7868 initBuckets.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: A conditional statement in a database query that checks a condition and executes a specific action if the condition is true.",
            "bucket index: 1"
        ],
        "Loop": [
            "Loop-Condition-Str: A loop condition in a database query that repeats a set of actions until a certain condition is met.",
            "bucket index: 2"
        ],
        "Normal": [
            "Normal-Str: A normal string in a database query that does not contain any special conditions or loops.",
            "bucket index: 3",
            "binary search: A search algorithm that finds the position of a target value within an array by dividing the search interval in half.",
            "bucket index: 4"
        ],
        "FuncName": "alphaindex_7868 getBucketIndex.txt"
    },
    {
        "Branch": [
            "A conditional statement that evaluates to true if the condition is met, which is often used in database queries to filter results."
        ],
        "Loop": [
            "A loop that iterates over a collection of data, often used in database queries to process multiple records."
        ],
        "Normal": [
            "This is called with primary-equal strings, but never with one.equals(other). This can be used in database queries to check for exact matches between strings."
        ],
        "FuncName": "alphaindex_7868 isOneLabelBetterThanOther.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "AlphabeticIndex requires some non-ignorable script boundary strings.",
                    "This is related to database indexing and collation."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "Loop conditions are used in database queries to control iteration.",
            "They are essential for efficient data retrieval and processing."
        ],
        "Normal": [
            "Ellipsis",
            "Guard against a degenerate collator where",
            "some script boundary strings are primary ignorable.",
            "Chinese index characters, which are specific to each of the several Chinese tailorings,",
            "take precedence over the single locale data exemplar set per language.",
            "This is relevant to database indexing and sorting."
        ],
        "FuncName": "alphaindex_7868 init.txt"
    },
    {
        "Branch": [
            "Internal command (server to server) for conditional execution."
        ],
        "Loop": [
            "Internal command (server to server) for loop execution."
        ],
        "Normal": [
            "Internal command (server to server) for standard execution."
        ],
        "FuncName": "shardsvr_drop_indexes_command_4317 skipApiVersionCheck.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Executing ShardsvrDropIndexesCommand to drop indexes from shards. This operation requires database to accept shard command.",
            "Setting write concern to ensure data consistency.",
            "Temporarily increasing database log level to capture command execution details.",
            "Acquiring DDL lock on target collection to ensure serialization with other DDL operations.",
            "Checking if target collection is a timeseries collection under DDL lock to determine execution strategy."
        ],
        "FuncName": "shardsvr_drop_indexes_command_4317 typedRun.txt"
    },
    {
        "Branch": [
            "Execute asynchronous request scheduling based on if condition."
        ],
        "Loop": [
            "Execute asynchronous request scheduling based on loop condition."
        ],
        "Normal": [
            "This remains valid as it owns the instance of thread pool."
        ],
        "FuncName": "async_request_executor_1700 schedule.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " We are trying to iterate over the items in a bucket, but there is no current bucket from the enumeration of buckets. This is because the database does not have any records to iterate over.",
                    "We are trying to iterate over the items in a bucket, but there is no current bucket from the enumeration of buckets. This is because the database does not have any records to iterate over."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            "Next Record Found: TRUE",
            "Next Record Found: FALSE"
        ],
        "Normal": [],
        "FuncName": "alphaindex_7868 nextRecord.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Ignore boundaries for special reordering groups in script-first-primary contractions.",
                    "In real scripts, take only those where the sample character is a letter and the one for unassigned implicit weights (Cn)."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Fetch script-first-primary contractions defined in the root collator for scripts starting with U+FDD1."
        ],
        "FuncName": "alphaindex_7868 firstStringsInScript.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "std::string ss; // String to store database record name",
            "std::string ss2; // String to store database record sorting name",
            "std::cout << added record: name =  << r->name_.toUTF8String(ss) <<  (Database Type: Alphabetic Index) <<  << ",
            "             sortingName =  << r->sortingName_.toUTF8String(ss2) <<  (Sorting Order: Ascending) <<  << std::endl;"
        ],
        "FuncName": "alphaindex_7868 addRecord.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Block until next result is available. These strings are from database code.",
                    "We have not provided a deadline, so if the wait returns without interruption, we do not",
                    "expect to have timed out."
                ]
            }
        ],
        "Normal": [],
        "FuncName": "blocking_results_merger_6891 blockUntilNext.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Check for characters we don't want to see in a metadata file related to database checkpoint validation.",
            "The internal checkpoint name is special in database context, applications aren't allowed to use it. Be aggressive and disallow any matching prefix, it makes things easier when checking in other database-related places.",
            "The name \\all\\ is also special in database checkpoint management."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_name_ok.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "This function exists as a place for this comment: named checkpoints are only supported on file objects, and not on LSM trees. If a target list is configured for the checkpoint, this function is called with each target list entry; check the entry to make sure it's backed by a file. If no target list is configured, confirm the metadata file contains no non-file objects. Skip any internal system objects. We don't want spurious error messages, other code will skip over them and the user has no control over their existence. Note that this function is related to the 'file' database type."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_name_check.txt"
    },
    {
        "Branch": [
            "Awaits the next result in the result set, waiting for the next document to be inserted into the collection. This is a blocking operation that will wait indefinitely for the next result, even if the database is not ready."
        ],
        "Loop": [
            {
                "loopstr": [
                    "For tailable awaitData cursors, swallows timeout errors, stores the event that it was waiting on, and returns EOF.",
                    "waiting on, and returns EOF."
                ]
            }
        ],
        "Normal": [
            "If the database is not ready for inserts, it will not block and will immediately return to the calling function.",
            "This condition is met when the ARM is ready or when the ARM is not ready but the current execution context is either kInitialFind or kGetMoreWithAtLeastOneResultInBatch. In the latter case, it will immediately return EOF without blocking for further results."
        ],
        "FuncName": "blocking_results_merger_6891 awaitNextWithTimeout.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If we have an outstanding event from last time, then we might have to manually schedule",
                    "some getMores for the cursors. If a remote response came back while we were between",
                    "getMores (from the user to mongos), the response may have been an empty batch, and the",
                    "ARM would not be able to ask for the next batch immediately since it is not attached to",
                    "an OperationContext. Now that we have a valid OperationContext, we schedule the getMores",
                    "ourselves. This requires a valid MongoDB connection to mongos, which is a type of",
                    "NoSQL database.",
                    "Return the leftover event and clear '_leftoverEventFromLastTimeout'."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If we abandoned a previous event due to a mongoS-side timeout, wait for it first. This is",
            "because mongoS is a distributed database system, and timeouts can occur due to network",
            "latency or other issues."
        ],
        "FuncName": "blocking_results_merger_6891 getNextEvent.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Some objects don't support named checkpoints.",
                    "In database terminology, this means that the checkpoint operation is being applied to a specific named checkpoint in the database."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "If the checkpoint is named or we're dropping checkpoints, we checkpoint both open and closed files; else, only checkpoint open files.",
                    "In database terms, this is equivalent to a 'consistent read' or 'repeatable read' isolation level, where all changes made by other transactions are visible but not yet committed.",
                    "XXX We don't optimize unnamed checkpoints of a list of targets, we open the targets and checkpoint them even if they are quiescent and don't need a checkpoint, believing applications unlikely to checkpoint a list of closed targets.",
                    "This indicates that the database is using a optimistic concurrency control mechanism, where the checkpoint operation is only triggered when there are actual changes to the data."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "Some objects don't support named checkpoints.",
                    "In database terminology, this means that the loop operation is being applied to a specific named checkpoint in the database.",
                    "This could be a 'cursor' or 'iterator' in database parlance, where the loop is iterating over a result set of data."
                ]
            }
        ],
        "Normal": [
            "Flag if this is a named checkpoint, and check if the name is OK.",
            "In database terms, this is equivalent to a 'primary key' or 'unique constraint' check, where the name of the checkpoint is validated to ensure it is unique and not already in use.",
            "Step through the targets and optionally operate on each one.",
            "This is analogous to a 'cursor' or 'iterator' in database terminology, where the operation is applied to each target in the result set."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_apply_operation.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Updates to the metadata are made by the checkpoint transaction, so the metadata tree's checkpoint generation should never be updated. In a database, a checkpoint is a point in time at which all changes made since the last checkpoint are written to permanent storage. The checkpoint generation is a counter that increments each time a checkpoint is taken."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_update_generation.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "The internal thread requires a schema lock to execute operations and flush tier also acquires the schema lock. Waiting in this function while holding that lock would lead to no work being done.",
            "Considering the add and decrement values, it might be beneficial to yield or wait based on the workload progress. Flushing operations can be time-consuming, making yielding less effective."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_flush_tier_wait.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: This string is used in the database to specify a condition for a query. For example, it can be used to filter results based on a specific value or range."
        ],
        "Loop": [
            "Loop-Condition-Str: This string is used in the database to specify a condition for a loop. For example, it can be used to iterate over a collection of documents and perform an action for each document that meets a certain condition."
        ],
        "Normal": [
            "Non-tailable and tailable non-awaitData cursors always block until ready(). AwaitData: This string describes the behavior of non-tailable and tailable non-awaitData cursors in the database. It explains that these types of cursors will block until the ready() function is called.",
            "cursors wait for ready() only until a specified time limit is exceeded: This string describes the behavior of cursors in the database. It explains that cursors will only wait for the ready() function to be called until a specified time limit is exceeded."
        ],
        "FuncName": "blocking_results_merger_6891 next.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Only instantiate the handle if we need to flush. This is a checkpoint operation for the database.",
                    "*If we get back EBUSY, this handle may be open with bulk or other special flags. We need to skip this tree to maintain database consistency. We fake checkpoints for such trees, i.e. we never really write a checkpoint to the disk and we cannot get the dhandle now.",
                    "*When we call wt_tiered_switch the session->dhandle points to the tiered entry and the arg is the config string that is currently in the database metadata. Also, mark the tree dirty to ensure it participates in the checkpoint process, even if clean."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "*Check the table's last checkpoint time and only flush trees that have a checkpoint more recent than the last flush time. This ensures database consistency and integrity.",
                    "If nothing has changed, there's nothing to do in terms of database operations."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "For now just switch tiers which just does metadata manipulation related to database storage."
                ]
            }
        ],
        "Normal": [
            "*For supporting splits and merge in the database:*",
            "*- See if there is any merging work to do to prepare and create an object that is suitable for placing onto tiered storage in the database.*",
            "*- Do the work to create said objects in the database.*",
            "*- Move the objects in the database.*",
            "Flushing is part of a checkpoint, use the session's checkpoint time to maintain database consistency.",
            "Storing the last flush timestamp here for the future and for debugging purposes in the database.",
            "*It would be more efficient to return here if no tiered storage is enabled in the system. If the user asks for a flush_tier without tiered storage, the loop below is effectively a no-op and will not be incorrect. But we could also just return.*",
            "*Walk the metadata cursor to find tiered tables to flush in the database. This should be optimized to avoid flushing tables that haven't changed.*",
            "Clear the flag on success to indicate database operation completed."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_flush_tier.txt"
    },
    {
        "Branch": [
            "If database checkpoint progress is greater than 50%"
        ],
        "Loop": [
            "Loop until database checkpoint is complete"
        ],
        "Normal": [
            "Time since the full database checkpoint started: 1 hour 30 minutes"
        ],
        "FuncName": "txn_ckpt_781 __wt_checkpoint_progress.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: A conditional statement in database that checks a condition and executes a block of code if the condition is true."
        ],
        "Loop": [
            "Loop-Condition-Str: A database loop that iterates over a set of data and performs a specific action for each iteration."
        ],
        "Normal": [
            "Get time diff in milliseconds: This statement retrieves the time difference in milliseconds between two points in time, a common operation in database performance optimization."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_verbose_track.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Apply operation to locked data handle: If we have already locked the handles, apply the operation."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_apply_to_dhandles.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "A place-holder to support data sources: we assume calling the underlying data-source session checkpoint function is sufficient to checkpoint all objects in the data source open or closed and we don't attempt to optimize the checkpoint of individual targets. Those assumptions are not necessarily going to be true for all data sources. It's not difficult to support data-source checkpoints of individual targets __wt_schema_worker is the underlying function that will do the work and it's already written to support data-sources although we'd probably need to pass the URI of the object to the data source checkpoint function which we don't currently do. However doing a full data checkpoint is trickier: currently the connection code is written to ignore all objects other than file: and that code will require significant changes to work with data sources. This process involves calling the data source's checkpoint function to ensure data consistency."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_data_source.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "\u6211\u4eec\u8fd8\u6ca1\u6709\u8fbe\u5230\u5f53\u524d\u76ee\u6807\u3002 \u4e0d\u8981\u7b49\u5f85\u65e0\u9650\u671f\uff1a\u53ef\u80fd\u4f1a\u6709\u65e0\u6cd5\u88ab\u9a71\u9010\u7684\u810f\u9875\u3002\u5982\u679c\u6211\u4eec\u65e0\u6cd5 \u8fbe\u5230\u76ee\u6807\uff0c\u653e\u5f03\u5e76\u771f\u6b63\u8fdb\u884c\u68c0\u67e5\u70b9\u3002"
                ]
            }
        ],
        "Normal": [
            "\u5982\u679c\u64e6\u9664\u662f\u7981\u7528\u7684\uff0c\u5219\u653e\u5f03\u3002 ",
            "\u5982\u679c\u7f13\u5b58\u5927\u5c0f\u4e3a\u96f6\u6216\u975e\u5e38\u5c0f\uff0c\u5219\u5b8c\u6210\u3002\u7f13\u5b58\u5927\u5c0f\u53ef\u80fd\u5728\u6211\u4eec\u4ece\u4e00\u4e2a\u5171\u4eab\u7f13\u5b58\u8f6c\u6362\u4e3a\u53e6\u4e00\u4e2a\u5171\u4eab\u7f13\u5b58\u65f6 * \u77ed\u6682\u5730\u53d8\u4e3a\u96f6\u3002\u8fd9\u907f\u514d\u4e86\u6f5c\u5728\u7684\u9664\u4ee5\u96f6\u3002 ",
            "\u5f53\u5199\u5165\u7684\u810f\u6570\u636e\u91cf\u7b49\u4e8e\u5f53\u524d\u7f13\u5b58\u4e2d\u7684\u810f\u6570\u636e\u91cf\u65f6\u505c\u6b62\u3002 ",
            "\u8bbe\u7f6e\u810f\u89e6\u53d1\u5668\u7684\u503c\u4e3a\u76ee\u6807\u503c\u3002 ",
            "\u7b49\u5f85\u810f\u6c34\u5e73\u4e0b\u964d\u5230\u6570\u636e\u5e93\u5bb9\u91cf\u768410%\u3002 "
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_wait_reduce_dirty_cache.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If the user wants timestamps then set the metadata checkpoint timestamp based on whether * or not a stable timestamp is actually in use. Only set it when we're not running recovery * because recovery doesn't set the recovery timestamp until its checkpoint is complete. This ensures that the checkpoint transaction is properly aligned with the database's timeline."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "A checkpoint should never proceed when timestamps are out of order. This is because the database relies on a consistent timeline to maintain data integrity."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Start a snapshot transaction for the checkpoint, ensuring that all subsequent operations are executed within a consistent database state. Note: we don't go through the public API calls because they have side effects on cursors, which applications can hold open across calls to checkpoint.",
            "Wait 1000 microseconds to simulate slowdown in checkpoint prepare, allowing for more accurate timing and resource allocation.",
            "Ensure a transaction ID is allocated prior to sharing it globally, ensuring that all transactions are properly tracked and managed.",
            "Keep track of handles acquired for locking, preventing deadlocks and ensuring that all resources are properly released.",
            "Mark the connection as clean. If some data gets modified after generating checkpoint transaction id, connection will be reset to dirty when reconciliation marks the btree dirty on encountering the dirty page, maintaining data consistency.",
            "Save the checkpoint session ID. We never do checkpoints in the default session (with id zero), ensuring that all checkpoints are properly isolated.",
            "Remove the checkpoint transaction from the global table. This allows ordinary visibility checks to move forward because checkpoints often take a long time and only write to the metadata, improving overall system performance.",
            "Sanity check that the oldest ID hasn't moved on before we have cleared our entry, preventing potential data corruption and ensuring that all transactions are properly ordered.",
            "Clear our entry from the global transaction session table. Any operation that needs to know about the ID for this checkpoint will consider the checkpoint ID in the global structure. Most operations can safely ignore the checkpoint ID (see the visible all check for details), reducing unnecessary complexity and improving readability.",
            "Set the checkpoint transaction's timestamp, if requested. We rely on having the global transaction data locked so the oldest timestamp can't move past the stable timestamp, ensuring that all transactions are properly aligned with the database's timeline.",
            "Wait for the commit generation to drain before bumping the snapshot, preventing potential deadlocks and ensuring that all resources are properly released.",
            "Refresh our snapshot here, doing so prevents us from racing with the stable timestamp moving ahead of current snapshot. i.e. if the stable timestamp moves after we begin the checkpoint transaction but before we set the checkpoint timestamp we can end up missing updates in our checkpoint. Call the bump variant as we don't want to publish the relevant ids, improving overall system responsiveness.",
            "Assert that our snapshot min didn't somehow move backwards, ensuring that all transactions are properly ordered and that data consistency is maintained.",
            "Flag as unused for non diagnostic builds, reducing unnecessary complexity and improving readability.",
            "If we are doing a flush_tier, do the metadata naming switch now while holding the schema lock in this function, ensuring that all metadata is properly updated and that the database remains consistent.",
            "Get a list of handles we want to sync; for named checkpoints this may pull closed objects into the session cache. First, gather all handles, then start the checkpoint transaction, then release any clean handles, improving overall system performance and reducing unnecessary complexity."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_prepare.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "This checkpoint can be seen by dhandle before commit, which will cause rollback error. We will ignore dhandle as part of this checkpoint and return directly."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Regularly record checkpoint preparation progress.",
            "Check if a forced checkpoint is needed.",
            "Should not be called with any content other than active B-tree handle.",
            "Skip files that have never participated in checkpoint.",
            "We may compete with some operations in between starting checkpoint transaction and some operations completing. All these operations either have exclusive access to handles or hold pattern locks. We now hold pattern locks and have an open B-tree handle. If we cannot update metadata, it means some state has changed in checkpoint transaction invisible.",
            "Determine whether to include the tree in the checkpoint, and get necessary locks if needed.",
            "Ensure enough space for new entries: do this before getting the handle to avoid cleaning up when unable to allocate memory.",
            "The current tree will be included: get it again because the handle we have is only valid during the duration of this function.",
            "Save current eviction settings: checkpoint may interfere with eviction, we do not want to unfairly punish (or promote) eviction trees due to checkpoint."
        ],
        "FuncName": "txn_ckpt_781 __wt_checkpoint_get_handles.txt"
    },
    {
        "Branch": [
            "Skip database checkpoint if not a full checkpoint and not forced and not named and not flushing objects and stable timestamp updated and no dirtied tables since last checkpoint."
        ],
        "Loop": [],
        "Normal": [
            "Default to not skipping - also initialize the other output parameters - even though they will always be initialized unless there is an error and callers need to ignore the results on error.",
            "This function also parses out some configuration options and hands them back to the caller - make sure it does that parsing regardless of the result.",
            "Determine if this is going to be a full checkpoint, that is a checkpoint that applies to all data tables in a database.",
            "Never skip non-full checkpoints",
            "Never skip if force is configured.",
            "Never skip named checkpoints.",
            "Never skip if flushing objects.",
            "If the checkpoint is using timestamps, and the stable timestamp hasn't been updated since the last checkpoint there is nothing more that could be written. Except when a non timestamped file has been modified, as such if the connection has been modified it is currently unsafe to skip checkpoints.",
            "Skip checkpointing the database if nothing has been dirtied since the last checkpoint. That said there can be short instances when a btree gets marked dirty and the connection is yet to be. We might skip a checkpoint in that short instance, which is okay because by the next time we get to checkpoint, the connection would have been marked dirty and hence the checkpoint will not be skipped again. If we are using timestamps then we shouldn't skip as the stable timestamp must have moved, and as such we still need to run checkpoint to update the checkpoint timestamp and the metadata."
        ],
        "FuncName": "txn_ckpt_781 __txn_checkpoint_can_skip.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "We use the last checkpoint we can find, that is, if there are two checkpoints with the same name in the list, we'll delete from the beginning to the second match, not the first. This is a database deletion function, which deletes the last checkpoint with a specified name."
        ],
        "FuncName": "txn_ckpt_781 __drop_to.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "FIXME-WT-11149: Some reading threads rely on the value of checkpoint running flag being  written before the checkpoint generation number (set inside the checkpoint call below) to ensure data integrity during disk write operations.",
            "Signal the tiered storage thread because it waits for the checkpoint to complete to process  flush units, guaranteeing data consistency in the database."
        ],
        "FuncName": "txn_ckpt_781 __txn_checkpoint_wrapper.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "If tiered storage is in use, move the time up to at least the most recent flush first. NOTE: reading the most recent flush time is not an acquire read (or repeated on retry) because currently checkpoint and flush tier are mutually exclusive. Update the global value that tracks the most recent checkpoint, and use it to make sure the most recent checkpoint time doesn't move backwards. Also make sure that this checkpoint time is not the same as the previous one, by running the clock forwards as needed. Note that while it's possible to run the clock a good long way forward if one tries (e.g. by doing a large number of schema operations that are fast and generate successive checkpoints of the metadata) and some tests (e.g. f_ops) do, this is not expected to happen in real use or lead to significant deviations from wall clock time. In a real database of any size full checkpoints take more than one second and schema operations are rare. Furthermore, though the checkpoint time may occasionally be updated multiple times in quick succession, this is not expected to happen in real use and is not a concern for this implementation. The update of the checkpoint time is a simple assignment of the most recent flush time to the global variable that tracks the most recent checkpoint, and does not involve any complex logic or synchronization. Therefore, this implementation does not need to worry about the possibility of the checkpoint time being updated multiple times in quick succession, and can simply update the checkpoint time to the most recent flush time without any further checks or considerations."
        ],
        "FuncName": "txn_ckpt_781 __txn_checkpoint_establish_time.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Delete a checkpoint with a specific name. If the name is 'all', delete all checkpoints. Otherwise, delete from the first checkpoint with the matching name to the end. ",
            " Delete a checkpoint with a specific name. If there are multiple checkpoints with the same name, delete from the first occurrence to the end. "
        ],
        "FuncName": "txn_ckpt_781 __drop_from.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " In the common case of the timer set forever, don't even check the time.",
                    " Skip the clean btree until the btree has obsolete pages.",
                    " WiredTiger database snapshot and checkpoint management function."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Gather the list of named checkpoints to drop (if any) from the first tree visited.",
                    " WiredTiger database snapshot and checkpoint management function."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    " Disallow unsafe checkpoint names.",
                    " WiredTiger database snapshot and checkpoint management function."
                ]
            }
        ],
        "Normal": [
            " Only referenced in diagnostic builds and gcc 5.1 isn't satisfied with wrapping the entire assert condition in the unused macro.",
            " Most callers need meta tracking to be on here, otherwise it is possible for this checkpoint to cleanup handles that are still in use. The exceptions are:",
            "  - Checkpointing the metadata handle itself.",
            "  - On connection close when we know there can't be any races.",
            " This may be a named checkpoint, check the configuration.",
            " Determine if a drop is part of the configuration. It usually isn't, so delay processing more until we know if we need to process this tree.",
            " This is a complicated test to determine if we can avoid the expensive call of getting the list of checkpoints for this file. We want to avoid that for clean files. But on clean files we want to periodically check if we need to delete old checkpoints that may have been in use by an open cursor.",
            " Discard the saved list of checkpoints, and slow path if this is not a WiredTiger checkpoint or if checkpoint drops are involved. Also, if we do not have checkpoint array size, the regular checkpoint process did not create the array. It is safer to discard the array in such a case.",
            " If we have to process this btree for any reason, reset the timer and obsolete pages flag.",
            " We may be dropping specific checkpoints, check the configuration.",
            " Drop checkpoints with the same name as the one we're taking. We don't need to add this to the drop list for snapshot/timestamp metadata because the metadata will be replaced by the new checkpoint.",
            " Set the name of the new entry at the end of the list.",
            " There is some interaction between backups and checkpoints. Perform all backup related operations that the checkpoint needs now, while holding the hot backup read lock.",
            " If we decided to skip checkpointing, we need to remove the new checkpoint entry we might have appended to the list.",
            " WiredTiger database snapshot and checkpoint management function."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_lock_dirty_tree.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Reset open cursors. This operation is necessary to ensure database consistency, even though it will happen implicitly in the call to begin_transaction for the checkpoint, the checkpoint code will acquire the schema lock before we do that, and some implementation of WT_CURSOR::reset might need the schema lock. This is a critical step in maintaining the integrity of the database.",
            "Ensure the metadata table is open before taking any locks. This is a prerequisite for any database operation that requires locking, and is essential for maintaining data consistency and preventing deadlocks.",
            "Don't hijack the session checkpoint thread for eviction. Application threads are not generally available for potentially slow operations, but checkpoint does enough I/O it may be called upon to perform slow operations for the block manager. Application checkpoints wait until the checkpoint lock is available, compaction checkpoints don't. Checkpoints should always use a separate session for history store updates, otherwise those updates are pinned until the checkpoint commits. Also, there are unfortunate interactions between the special rules for history store eviction and the special handling of the checkpoint transaction. This is a complex issue that requires careful handling to avoid data corruption.",
            "If this checkpoint includes a flush_tier then this call also must wait for any earlier flush_tier to have completed all of its copying of objects. This happens if the user chose to not wait for sync on the previous call. This is a critical step in ensuring that data is properly synced across all tiers of the database.",
            "Only one checkpoint can be active at a time, and checkpoints must run in the same order as they update the metadata. It's probably a bad idea to run checkpoints out of multiple threads, but as compaction calls checkpoint directly, it can be tough to avoid. Serialize here to ensure we don't get into trouble. This is a necessary step to prevent data corruption and ensure database consistency.",
            "If this checkpoint is flushing objects, a failure can leave a tree's block manager pointing to incorrect blocks. Currently we can not recover from this situation. Panic! This is a critical issue that requires immediate attention to prevent data loss and corruption.",
            "Trigger the checkpoint cleanup thread to remove the obsolete pages. This is a necessary step to maintain database integrity and prevent data corruption."
        ],
        "FuncName": "txn_ckpt_781 __wt_txn_checkpoint.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "\u5982\u679c\u6211\u4eec\u51b3\u5b9a\u8df3\u8fc7\u68c0\u67e5\u70b9\uff0c\u6e05\u9664\u5220\u9664\u6807\u8bb0\u3002\u68c0\u67e5\u70b9\u5217\u8868\u5c06\u88ab\u7f13\u5b58\u4ee5\u4f9b\u672a\u6765\u8bbf\u95ee\u3002\u9700\u8981\u5220\u9664\u7684\u68c0\u67e5\u70b9\u53ef\u4ee5\u5728\u6b64\u671f\u95f4\u6539\u53d8\u3002\u9501\u5b9a\u5c06\u88ab\u5220\u9664\u7684\u68c0\u67e5\u70b9\u3002"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "\u68c0\u67e5\u662f\u5426\u53ef\u4ee5\u5220\u9664\u6240\u6709\u6807\u8bb0\u4e3a\u5220\u9664\u7684\u68c0\u67e5\u70b9\u3002",
            " * \u6807\u8bb0\u8981\u5220\u9664\u7684\u65e7\u68c0\u67e5\u70b9\u5e76\u786e\u5b9a\u6211\u4eec\u53ef\u4ee5\u8df3\u8fc7\u7684\u6811\u3002",
            " * \u9501\u5b9a\u8981\u5220\u9664\u7684\u68c0\u67e5\u70b9\u3002\u9501\u5b9a\u5c06\u88ab\u5220\u9664\u7684\u68c0\u67e5\u70b9\u3002",
            " * \u68c0\u67e5\u70b9\u4ec5\u5728\u542f\u7528\u8ddf\u8e2a\u65f6\u9501\u5b9a\uff0c\u8fd9\u6db5\u76d6\u68c0\u67e5\u70b9\u548c\u4e22\u5f03\u64cd\u4f5c\uff0c\u4f46\u4e0d\u6db5\u76d6\u5173\u95ed\u3002\u7406\u7531\u662f\u4efb\u4f55\u8bbf\u95ee\u68c0\u67e5\u70b9\u7684\u7ebf\u7a0b\u4e5f\u5c06\u62e5\u6709\u5f53\u524d\u6587\u4ef6\u53e5\u67c4\u6253\u5f00\u3002",
            " * \u6709\u7279\u6b8a\u6811\uff1a\u6b63\u5728\u8fdb\u884c\u6279\u91cf\u52a0\u8f7d\u3001\u633d\u6551\u6216\u9a8c\u8bc1\u7684\u68c0\u67e5\u70b9\u3002\u5b83\u4eec\u4e0d\u5e94\u6210\u4e3a\u68c0\u67e5\u70b9\u7684\u4e00\u90e8\u5206\uff1a\u6211\u4eec\u5c06\u5728\u6b64\u671f\u95f4\u5931\u8d25\u9501\u5b9a\u5b83\u4eec\uff0c\u56e0\u4e3a\u64cd\u4f5c\u5177\u6709\u5bf9\u53e5\u67c4\u7684\u72ec\u5360\u8bbf\u95ee\u3002\u547d\u540d\u68c0\u67e5\u70b9\u5c06\u5728\u6b64\u60c5\u51b5\u4e0b\u5931\u8d25\uff0c\u800c\u666e\u901a\u68c0\u67e5\u70b9\u5c06\u8df3\u8fc7\u65e0\u6cd5\u6b63\u5e38\u6253\u5f00\u7684\u6587\u4ef6\u3002\u9501\u5b9a\u5c06\u88ab\u5220\u9664\u7684\u68c0\u67e5\u70b9\u3002"
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_lock_dirty_tree_int.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Once the history store checkpoint is complete, we increment the checkpoint generation of the associated b-tree. The checkpoint generation controls whether we include the checkpoint transaction in our calculations of the pinned and oldest_ids for a given btree. We increment it here to ensure that the visibility checks performed on updates in the history store do not include the checkpoint transaction. This is a checkpoint operation in the database code.",
                    "Incrementing the checkpoint generation is a key aspect of the checkpoint operation in the database, ensuring that the visibility checks are performed correctly."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Disable metadata tracking during the metadata checkpoint. This is a checkpoint operation in the database code.",
                    "Disabling metadata tracking is an essential step in the checkpoint operation, allowing for efficient metadata checkpointing."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "If timestamps defined the checkpoint's content, set the saved last checkpoint timestamp, otherwise clear it. We clear it for a couple of reasons: applications can query it and we don't want to lie, and we use it to decide if WT_CONNECTION.rollback_to_stable is an allowed operation. For the same reason, don't set it to WT_TS_NONE when the checkpoint timestamp is WT_TS_NONE, set it to 1 so we can tell the difference. This is a checkpoint operation in the database code.",
                    "Managing the last checkpoint timestamp is crucial in the checkpoint operation, ensuring accurate and reliable checkpointing."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "MongoDB assumes the checkpoint timestamp will be initialized with WT_TS_NONE. In such cases it queries the recovery timestamp to determine the last stable recovery timestamp. So, if the recovery timestamp is valid, set the last checkpoint timestamp to recovery timestamp. This should never be a problem, as checkpoint timestamp should never be less than recovery timestamp. This could potentially avoid MongoDB making two calls to determine last stable recovery timestamp. This is a checkpoint operation in the database code.",
                    "Initializing the checkpoint timestamp with the recovery timestamp is a critical step in the checkpoint operation, ensuring efficient and accurate checkpointing."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "Clear the dhandle so the visibility check doesn't get confused about the snap min. Don't bother restoring the handle since it doesn't make sense to carry a handle across a checkpoint. This is a checkpoint operation in the database code.",
                    "Clearing the dhandle is essential in the checkpoint operation, preventing potential visibility issues."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    "If the operation failed, mark all trees dirty so they are included if a future checkpoint can succeed. This is a checkpoint operation in the database code.",
                    "Marking all trees dirty is a critical step in the checkpoint operation, ensuring that all necessary data is included in the checkpoint."
                ]
            }
        ],
        "Normal": [
            "Avoid doing work if possible. This is a checkpoint operation in the database code.",
            "Checking if work can be avoided is an essential step in the checkpoint operation, reducing unnecessary processing and improving efficiency.",
            "Check if this is a named checkpoint. This is a checkpoint operation in the database code.",
            "Determining if the checkpoint is named is crucial in the checkpoint operation, allowing for targeted and efficient checkpointing.",
            "Do a pass over the configuration arguments and figure out what kind of checkpoint this is. This is a checkpoint operation in the database code.",
            "Analyzing the configuration arguments is a critical step in the checkpoint operation, ensuring that the correct checkpointing strategy is employed.",
            "Reset the statistics tracked per checkpoint. This is a checkpoint operation in the database code.",
            "Resetting the statistics is essential in the checkpoint operation, allowing for accurate and reliable checkpointing.",
            "Initialize the verbose tracking timer. This is a checkpoint operation in the database code.",
            "Initializing the verbose tracking timer is a critical step in the checkpoint operation, enabling detailed and accurate tracking.",
            "Initialize the checkpoint progress tracking data. This is a checkpoint operation in the database code.",
            "Initializing the checkpoint progress tracking data is essential in the checkpoint operation, allowing for real-time monitoring and tracking.",
            "Get a time (wall time, not a timestamp) for this checkpoint. This will be applied to all the trees so they match. The time is left in the session. This is a checkpoint operation in the database code.",
            "Obtaining a wall time for the checkpoint is a critical step in the checkpoint operation, ensuring that all trees are properly synchronized.",
            "Update the global oldest ID so we do all possible cleanup. This is particularly important for compact, so that all dirty pages can be fully written. This is a checkpoint operation in the database code.",
            "Updating the global oldest ID is essential in the checkpoint operation, ensuring that all necessary cleanup is performed.",
            "Flush data-sources before we start the checkpoint. This is a checkpoint operation in the database code.",
            "Flushing the data-sources is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized.",
            "Try to reduce the amount of dirty data in cache so there is less work do during the critical section of the checkpoint. This is a checkpoint operation in the database code.",
            "Reducing the amount of dirty data in cache is essential in the checkpoint operation, improving efficiency and reducing unnecessary processing.",
            "Tell logging that we are about to start a database checkpoint. This is a checkpoint operation in the database code.",
            "Notifying logging of the impending checkpoint is a critical step in the checkpoint operation, ensuring that all necessary logging and tracking is performed.",
            "Start the checkpoint for real. Bump the global checkpoint generation, used to figure out whether checkpoint has visited a tree. Use an atomic increment even though we are single-threaded because readers of the checkpoint generation don't hold the checkpoint lock. We do need to update it before clearing the checkpoint's entry out of the transaction table, or a thread evicting in a tree could ignore the checkpoint's transaction. This is a checkpoint operation in the database code.",
            "Starting the checkpoint is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint generation is accurately updated.",
            "We want to skip checkpointing clean handles whenever possible. That is, when the checkpoint is not named or forced. However, we need to take care about ordering with respect to the checkpoint transaction. We can't skip clean handles before starting the transaction or the checkpoint can miss updates in trees that become dirty as the checkpoint is starting. If we wait until the transaction has started before locking a handle, there could be a metadata-changing operation in between (e.g., salvage) that will cause a write conflict when the checkpoint goes to write the metadata. Hold the schema lock while starting the transaction and gathering handles so the set we get is complete and correct. This is a checkpoint operation in the database code.",
            "Managing clean handles is essential in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint transaction is accurately handled.",
            "Log the final checkpoint prepare progress message if needed. This is a checkpoint operation in the database code.",
            "Logging the final checkpoint prepare progress message is a critical step in the checkpoint operation, ensuring that all necessary logging and tracking is performed.",
            "Save the checkpoint timestamp in a temporary variable, when we release our snapshot it'll be reset to zero. This is a checkpoint operation in the database code.",
            "Saving the checkpoint timestamp is essential in the checkpoint operation, ensuring that the correct timestamp is recorded and that the snapshot is accurately released.",
            "Unblock updates -- we can figure out that any updates to clean pages after this point are too new to be written in the checkpoint. This is a checkpoint operation in the database code.",
            "Unblocking updates is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Tell logging that we have started a database checkpoint. Do not write a log record if the database was idle. This is a checkpoint operation in the database code.",
            "Notifying logging of the started checkpoint is a critical step in the checkpoint operation, ensuring that all necessary logging and tracking is performed.",
            "Add a ten second wait to simulate checkpoint slowness. This is a checkpoint operation in the database code.",
            "Adding a wait to simulate checkpoint slowness is a critical step in the checkpoint operation, ensuring that the checkpoint is accurately simulated and that the database is properly synchronized.",
            "Wait prior to checkpointing the history store to simulate checkpoint slowness. This is a checkpoint operation in the database code.",
            "Waiting prior to checkpointing the history store is essential in the checkpoint operation, ensuring that the checkpoint is accurately simulated and that the database is properly synchronized.",
            "Get a history store dhandle. If the history store file is opened for a special operation this will return EBUSY which we treat as an error. In scenarios where the history store is not part of the metadata file (performing recovery on backup folder where no checkpoint occurred), this will return ENOENT which we ignore and continue. This is a checkpoint operation in the database code.",
            "Obtaining a history store dhandle is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "It is possible that we don't have a history store file in certain recovery scenarios. As such we could get a dhandle that is not opened. This is a checkpoint operation in the database code.",
            "Handling the absence of a history store file is essential in the checkpoint operation, ensuring that the checkpoint is accurately completed and that the database is properly synchronized.",
            "As part of recovery, rollback to stable may have left out clearing stale transaction ids. Update the connection base write generation based on the latest checkpoint write generations to reset these transaction ids present on the pages when reading them. This is a checkpoint operation in the database code.",
            "Updating the connection base write generation is a critical step in the checkpoint operation, ensuring that all necessary transaction ids are properly reset and that the checkpoint is accurately completed.",
            "Clear the dhandle so the visibility check doesn't get confused about the snap min. Don't bother restoring the handle since it doesn't make sense to carry a handle across a checkpoint. This is a checkpoint operation in the database code.",
            "Clearing the dhandle is essential in the checkpoint operation, preventing potential visibility issues.",
            "We have to update the system information before we release the snapshot. Drop the system information for checkpoints we're dropping first in case the names overlap. This is a checkpoint operation in the database code.",
            "Updating the system information is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Release the snapshot so we aren't pinning updates in cache. This is a checkpoint operation in the database code.",
            "Releasing the snapshot is essential in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Mark all trees as open for business (particularly eviction). This is a checkpoint operation in the database code.",
            "Marking all trees as open is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Checkpoints have to hit disk (it would be reasonable to configure for lazy checkpoints, but we don't support them yet). This is a checkpoint operation in the database code.",
            "Ensuring that checkpoints hit disk is essential in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Sync the history store file. This is a checkpoint operation in the database code.",
            "Synchronizing the history store file is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "If the history store file exists on disk, update its statistic. This is a checkpoint operation in the database code.",
            "Updating the history store file statistic is essential in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Commit the transaction now that we are sure that all files in the checkpoint have been flushed to disk. It's OK to commit before checkpointing the metadata since we know that all files in the checkpoint are now in a consistent state. This is a checkpoint operation in the database code.",
            "Committing the transaction is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Flush all the logs that are generated during the checkpoint. It is possible that checkpoint may include the changes that are written in parallel by an eviction. To have a consistent view of the data, make sure that all the logs are flushed to disk before the checkpoint is complete. This is a checkpoint operation in the database code.",
            "Flushing the logs is essential in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Ensure that the metadata changes are durable before the checkpoint is resolved. Do this by either checkpointing the metadata or syncing the log file. Recovery relies on the checkpoint LSN in the metadata only being updated by full checkpoints so only checkpoint the metadata for full or non-logged checkpoints. This is a checkpoint operation in the database code.",
            "Ensuring metadata durability is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Wait prior to flush the checkpoint stop log record. This is a checkpoint operation in the database code.",
            "Waiting prior to flushing the checkpoint stop log record is essential in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Now that the metadata is stable, re-open the metadata file for regular eviction by clearing the checkpoint_pinned flag. This is a checkpoint operation in the database code.",
            "Re-opening the metadata file is a critical step in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Reset the timer so that next checkpoint tracks the progress only if configured. This is a checkpoint operation in the database code.",
            "Resetting the timer is essential in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "XXX Rolling back the changes here is problematic. If we unroll here, we need a way to roll back changes to the avail list for each tree that was successfully synced before the error occurred. Otherwise, the next time we try this operation, we will try to free an old checkpoint again. OTOH, if we commit the changes after a failure, we have partially overwritten the checkpoint, so what ends up on disk is not consistent. This is a checkpoint operation in the database code.",
            "Managing potential rollback issues is essential in the checkpoint operation, ensuring that all necessary data is properly synchronized and that the checkpoint is accurately completed.",
            "Tell logging that we have finished a database checkpoint. Do not write a log record if the database was idle. This is a checkpoint operation in the database code.",
            "Notifying logging of the finished checkpoint is a critical step in the checkpoint operation, ensuring that all necessary logging and tracking is performed."
        ],
        "FuncName": "txn_ckpt_781 __txn_checkpoint.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Confirm that the last checkpoint has a metadata entry that we can use to base a new checkpoint on. This is a checkpoint-related operation in the database."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_save_ckptlist.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If the checkpoint has a valid stop timestamp, mark the btree as having obsolete pages. This flag is used to avoid skipping the btree until the obsolete check is performed on the checkpoints. In the context of a relational database, this is done to ensure data consistency and prevent data loss."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "txn_ckpt_781 __checkpoint_apply_obsolete.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Reconciliation just wrote a checkpoint, everything has been written. Update the checkpoint with reconciliation information. The reason for this function is the reconciliation code just passes through the btree structure's checkpoint array, it doesn't know any more. This process is related to database checkpointing and reconciliation."
        ],
        "FuncName": "txn_ckpt_781 __wt_checkpoint_tree_reconcile_update.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Should not be called with a checkpoint handle, which is used for database synchronization.",
            "Unnecessary if checkpoint_sync has been configured off, indicating that the database is not synchronized."
        ],
        "FuncName": "txn_ckpt_781 __wt_checkpoint_sync.txt"
    },
    {
        "Branch": [
            "This string is used to test the if condition in the database. It is a crucial part of the stress test function."
        ],
        "Loop": [
            "This string is used to test the loop condition in the database. It is a vital component of the stress test function."
        ],
        "Normal": [
            " We only want to sleep if the flag is set and the checkpoint comes from the API, so check if the session used is either of the two sessions set aside for internal checkpoints. This is a normal string that has been enhanced with database-related description."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_timing_stress.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Complicated test: if the tree is clean and last two checkpoints have the same name (correcting for internal checkpoint names with their generational suffix numbers), we can skip the checkpoint, there's nothing to do. The exception is if we're deleting two or more checkpoints: then we may save space.",
                    "This condition checks if the tree is clean and if the last two checkpoints have the same name, allowing us to skip the checkpoint if the conditions are met."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "If there are potentially extra checkpoints to delete, we set the timer to recheck later. If there are at most two checkpoints, the current one and possibly a previous one, then we know there are no additional ones to delete. In that case, set the timer to forever. If the table gets dirtied or a checkpoint is forced that will clear the timer.",
                    "This condition checks for potentially extra checkpoints to delete and sets the timer accordingly, considering the number of checkpoints and the table's state."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Check for clean objects not requiring a checkpoint. If we're closing a handle, and the object is clean, we can skip the checkpoint, whatever checkpoints we have are sufficient. (We might not have any checkpoints if the object was never modified, and that's OK: the object creation code doesn't mark the tree modified so we can skip newly created trees here.) If the application repeatedly checkpoints an object (imagine hourly checkpoints using the same explicit or internal name), there's no reason to repeat the checkpoint for clean objects. The test is if the only checkpoint we're deleting is the last one in the list and it has the same name as the checkpoint we're about to take, skip the work. (We can't skip checkpoints that delete more than the last checkpoint because deleting those checkpoints might free up space in the file.) This means an application toggling between two (or more) checkpoint names will repeatedly take empty checkpoints, but that's not...",
            "This condition checks for clean objects and considers the application's checkpointing behavior, allowing us to skip unnecessary checkpoints and avoid repeated work."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_mark_skip.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Should not be called with a checkpoint handle. This operation is exclusive to database checkpointing.",
            "We must hold the metadata lock if checkpointing the metadata to ensure database consistency.",
            "If we're already in a global checkpoint, don't get a new time. Otherwise, we need one to record the checkpoint timestamp for the database.",
            "Do not store the cached checkpoint list when checkpointing a single file alone, as this is not a database-wide operation."
        ],
        "FuncName": "txn_ckpt_781 __wt_checkpoint.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Add a two seconds wait to simulate checkpoint slowness for every handle in the database.",
            "Are we using a read timestamp for this checkpoint transaction in the database?",
            "Logged tables in the database ignore any read timestamp configured for the checkpoint.",
            "Restore the use of the timestamp for other tables in the database.",
            "*Whatever happened, we aren't visiting this tree again in this checkpoint. Don't keep updates*pinned any longer in the database.",
            "*In case this tree was being skipped by the eviction server during the checkpoint, restore the*previous state in the database.",
            "*Wake the eviction server, in case application threads have stalled while the eviction server*decided it couldn't make progress. Without this, application threads will be stalled until*the eviction server next wakes in the database."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_tree_helper.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "We've done the final checkpoint before the final close, subsequent writes to normal objects are wasted effort. Discard the objects to validate exit accounting. This is related to database checkpointing and closing the database.",
            "Closing an unmodified file. This operation is related to database file management.",
            "Don't flush data from modified trees independent of system-wide checkpoint. Flushing trees can lead to files that are inconsistent on disk after a crash. This is a database consistency check.",
            "Make sure there isn't a potential race between backup copying the metadata and a checkpoint changing the metadata. Backup holds both the checkpoint and schema locks. Checkpoint should hold those also except on the final checkpoint during close. Confirm the caller either is the final checkpoint or holds at least one of the locks. This is a database locking mechanism.",
            "Turn on metadata tracking if: The session is not already doing metadata tracking. The file was not bulk loaded. The close is not during connection close. This is a database metadata tracking feature.",
            "Do not store the cached checkpoint list when closing the handle. This is a database checkpoint handling mechanism."
        ],
        "FuncName": "txn_ckpt_781 __wt_checkpoint_close.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Performing file checkpoint processing, including setting the checkpoint LSN, marking the root page dirty, clearing the tree's modification flag, notifying the log and block manager of the checkpoint start and end, flushing the file, updating object metadata, parsing the checkpoint, and notifying the log of the checkpoint completion.",
            "Faking a checkpoint for an object that has never been used, as it requires writing physical checkpoint blocks, which is not necessary for just-created files.",
            "Marking the root page dirty to ensure something gets written, as it's a precautionary measure to prevent issues with the tree modification value and dirty pages.",
            "Clearing the tree's modified flag, as any changes before clearing the flag are guaranteed to be part of the checkpoint, and changes subsequent to the checkpoint start will re-set the modified flag.",
            "Notifying the log that a file checkpoint is starting, as it's a crucial step in the checkpoint process.",
            "Notifying the block manager that a file checkpoint is starting, as it's essential for the block manager to be aware of the checkpoint process.",
            "Flushing the file from the cache, creating the checkpoint, which is a critical step in the checkpoint process.",
            "Updating the object's metadata, including swapping the new metadata file into place, and ensuring the metadata is on disk before updating the turtle file.",
            "Resolving the checkpoint, which involves applying or unrolling meta tracking events, and is necessary for a successful checkpoint.",
            "Notifying the log that the checkpoint is complete, as it's a final step in the checkpoint process.",
            "Resolving the checkpoint for the block manager in the error path, as it's an essential step in handling errors during the checkpoint process.",
            "Marking the tree dirty if the checkpoint didn't complete successfully, as it's a precautionary measure to prevent issues with the tree modification value and dirty pages.",
            "Posting process the ckptlist for a successful checkpoint, to keep a cached copy around, as it's a step in maintaining the checkpoint process."
        ],
        "FuncName": "txn_ckpt_781 __checkpoint_tree.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "Enable write blocking bypass to allow cleaning of stale data even if writes are disallowed. This operation is specific to the database type and is used to remove outdated data from secondary copies."
        ],
        "FuncName": "move_primary_coordinator_4413 dropStaleDataOnDonor.txt"
    },
    {
        "Branch": [
            "Enable write blocking bypass for coordinated cloning to interface with database server."
        ],
        "Loop": [],
        "Normal": [
            "Enable write blocking bypass to allow cloning of catalog data even if writes are disallowed. This is a coordinated cloning operation to interface with the database server."
        ],
        "FuncName": "move_primary_coordinator_4413 cloneDataToRecipient.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            "The release of the critical section will clear secondary database metadata.",
            "In case of step-down, this operation could be re-executed and trigger the invariant in case the new primary runs a DDL that acquires the critical section in the old primary shard, which may involve database schema changes.",
            "The release of the critical section will clear secondary database metadata on write operations."
        ],
        "FuncName": "move_primary_coordinator_4413 unblockReadsAndWrites.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "TODO SERVER-83713 Reconsider isFLE2StateCollection check, a type of mobile unsharded collection in database"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "move_primary_coordinator_4413 isMovableUnshardedCollection.txt"
    },
    {
        "Branch": [
            "In order to advance to the next phase, the database must satisfy the if-conditions, which are represented by the following strings: [database] codes."
        ],
        "Loop": [
            "In order to proceed to the next iteration, the database must meet the loop conditions, which are defined by the following strings: [database] codes."
        ],
        "Normal": [
            "Any non-retryable errors while checking the preconditions should cause the operation to be terminated. [Database] error codes indicate that the operation has failed due to a non-retryable error.",
            "Instead, in any of the subsequent phases, any non-retryable errors that do not trigger the cleanup procedure should cause the operation to be retried from the failed phase. [Database] error codes indicate that the operation has failed due to a non-retryable error, but the cleanup procedure has not been triggered."
        ],
        "FuncName": "move_primary_coordinator_4413 _mustAlwaysMakeProgress.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "NOLINT",
                    "If condition is met, the code will execute the then branch, which contains the NOLINT directive."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "ast_traversal_visitor_test_9421 GenerateAst.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " A retryable error occurred before to persist the collections to clone consequently no data has been cloned yet. This is related to the database's ability to clone collections.",
                    " The database code is unable to clone the collections due to the error."
                ],
                "elsestr": []
            }
        ],
        "Loop": [
            {
                "loopstr": [
                    " dropSystemCollections ) This operation is related to database maintenance and is used to drop system collections.",
                    " The database code is dropping system collections as part of its maintenance routine."
                ]
            }
        ],
        "Normal": [
            " Make a copy of this container since getNewSession changes the coordinator document. This is related to database session management and is used to create a copy of the container for the new session.",
            " The database code is creating a copy of the container to ensure that the new session has a consistent view of the data."
        ],
        "FuncName": "move_primary_coordinator_4413 dropOrphanedDataOnRecipient.txt"
    },
    {
        "Branch": [
            "If-Condition-Str: A conditional statement in a database query that checks for a specific condition, such as equality or inequality.",
            "If-Condition-Str: Used to filter data in a database, only returning rows where the condition is met.",
            "If-Condition-Str: Can be used with logical operators like AND, OR, and NOT to create complex conditions."
        ],
        "Loop": [
            "Loop-Condition-Str: A loop in a database query that iterates over a set of data, often used for aggregation or transformation.",
            "Loop-Condition-Str: Can be used to perform repetitive tasks, such as updating or inserting data, in a database.",
            "Loop-Condition-Str: Often used with control structures like WHILE or FOR to manage the loop's execution."
        ],
        "Normal": [
            " NOLINTNEXTLINE: bugprone-suspicious-semicolon: seems like a false positive because of constexpr",
            " Normal-Str: This is a normal string in the database code, not related to database queries or operations.",
            " Normal-Str: It may contain comments or other metadata, but does not affect the database's functionality."
        ],
        "FuncName": "ast_traversal_visitor_test_9421 VisitFunctionLitExpr.txt"
    },
    {
        "Branch": [
            "The If-Condition-Str is used to execute write operations in MongoDB. It is a string used in the database code."
        ],
        "Loop": [
            "The Loop-Condition-Str is used to execute write operations in MongoDB. It is a string used in the database code."
        ],
        "Normal": [
            "If we aren't running an explain for updateOne or deleteOne without shard key, continue and run the original explain path.",
            "We will time how long it takes to run the commands on the shards.",
            "Target the command to the shards based on the singleton batch item.",
            "This is related to MongoDB database operations, specifically write operations such as updateOne and deleteOne."
        ],
        "FuncName": "cluster_write_cmd_3588 executeWriteOpExplain.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Explain currently cannot be run within a transaction, so each command is instead run separately outside of a transaction, and we compose the results at the end. This is because explain requires access to the query plan, which is not available within a transaction.",
                    "Since 'explain' does not return the results of the query, we do not have an _id document to target by from the 'Read Phase'. We instead will use a dummy _id target document 'Write Phase' in MongoDB, which is a NoSQL database."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "cluster_write_cmd_3588 runExplainWithoutShardKey.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "Add one failed attempt for write operation in database"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "If 'batchedRequest' has any let parameters, evaluate them and stash them back on the original request for write operation in database.",
            "Record the namespace that the write must be run on for database write operation. It may differ from the request if this is a timeseries collection for database.",
            "Populate the 'NotPrimaryErrorTracker' object based on the write response for database write operation",
            "TODO: increase opcounters by more than one for database write operation",
            "If we have no information on the shards targeted for database, ignore updatedShardKey,",
            "updateHostsTargetedMetrics will report this as TargetType::kManyShards for database write operation.",
            "Record the number of shards targeted by this write operation in database."
        ],
        "FuncName": "cluster_write_cmd_3588 runImpl.txt"
    },
    {
        "Branch": [
            "Error transaction may alter all shard owners."
        ],
        "Loop": [
            "Error transaction may alter all shard owners."
        ],
        "Normal": [
            "Shared state for the transaction API use below. In MongoDB, this involves updating the oplog.",
            "If the operation was an upsert, record the _id of the new document. In MongoDB, this is done by updating the shard key index."
        ],
        "FuncName": "cluster_write_cmd_3588 handleWouldChangeOwningShardErrorTransaction.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "If we get here, the batch size is 1 and we have successfully deleted the old doc and inserted the new one, so it is safe to unset the error details. This operation involves updating the document's shard key in a transaction, which is a critical operation in a database.",
                    ""
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "cluster_write_cmd_3588 handleWouldChangeOwningShardError.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "NOLINT: Modify action state based on database type"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "NOLINT: Modify action state based on database type"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "NOLINT: Modify action state based on database type"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            "Set the new value accordingly based on the param type; Consider database-specific data types, e.g., integers, strings, etc.",
            "TODO(lin): we implicitly cast everything to int64_t for now. Need to extend to other types, e.g., string, and consider database-specific data types"
        ],
        "FuncName": "change_knob_action_7612 ModifyActionState.txt"
    },
    {
        "Branch": [],
        "Loop": [
            {
                "loopstr": [
                    "Block until a response is available.",
                    "If the response status was OK, the response must contain which host was targeted.",
                    "This operation is performed on a distributed database cluster."
                ]
            }
        ],
        "Normal": [
            "Assemble requests for distributed database processing.",
            "Send the requests to the cluster for batch processing."
        ],
        "FuncName": "cluster_write_cmd_3588 commandOpWrite.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Updating the shard key when batch size > 1 is disallowed when the document would move to a different shard, resulting in a WouldChangeOwningShard error. If the update is in a transaction, MongoDB will uassert. If the write is not in a transaction, change any WouldChangeOwningShard errors in this batch to InvalidOptions to be reported to the user, as this is an invalid option for the database.",
            " Updating the shard key when batch size > 1 is disallowed when the document would move to a different shard. If the update is in a transaction, MongoDB will uassert. If the write is not in a transaction, change any WouldChangeOwningShard errors in this batch to InvalidOptions to be reported to the user, as this is an invalid option for the database.",
            " Updating the shard key when batch size > 1 is disallowed when the document would move to a different shard. If the update is in a transaction, MongoDB will uassert. If the write is not in a transaction, change any WouldChangeOwningShard errors in this batch to InvalidOptions to be reported to the user, as this is an invalid option for the database.",
            " Updating the shard key when batch size > 1 is disallowed when the document would move to a different shard, resulting in a WouldChangeOwningShard error. If the update is in a transaction, MongoDB will uassert. If the write is not in a transaction, change any WouldChangeOwningShard errors in this batch to InvalidOptions to be reported to the user, as this is an invalid option for the database."
        ],
        "FuncName": "cluster_write_cmd_3588 getWouldChangeOwningShardErrorInfo.txt"
    },
    {
        "Branch": [
            "Check if the intrinsics holder exists in the global scope."
        ],
        "Loop": [],
        "Normal": [
            "Install the intrinsics holder on the global, which is a crucial step in creating the database object."
        ],
        "FuncName": "GlobalObject_7223 createIntrinsicsHolder.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Define a top-level property 'undefined' with the undefined value. ",
            " Resolve a globalThis self-referential property if necessary. ",
            " In database context, undefined property may indicate a missing or non-existent record. "
        ],
        "FuncName": "GlobalObject_7223 initStandardClasses.txt"
    },
    {
        "Branch": [
            "Conditional statements in the database query, such as IF-THEN-ELSE, are used to execute different blocks of code based on certain conditions."
        ],
        "Loop": [
            "Loop conditions in the database query, such as WHILE and FOR, are used to execute a block of code repeatedly for a specified number of times or until a certain condition is met."
        ],
        "Normal": [
            "Atoms are always tenured so don't need to be traced during minor GC.",
            "In the context of a database, atoms refer to the basic units of data that are stored and manipulated by the database system."
        ],
        "FuncName": "GlobalObject_7223 trace.txt"
    },
    {
        "Branch": [
            "In a database context, an if condition is evaluated as a boolean expression to determine whether a certain action should be taken."
        ],
        "Loop": [
            "In a database context, a loop condition is used to repeat a set of actions until a certain condition is met."
        ],
        "Normal": [
            "In a database context, ensuring c-pi is up-to-date is crucial for accurate unwind information. This is particularly important on x86-64 architectures, where DWARF unwind info might be missing. Failing to do so could result in incomplete backtraces."
        ],
        "FuncName": "Gresume_8965 s390x_local_resume.txt"
    },
    {
        "Branch": [
            "If we are creating a new global in an existing database compartment, make sure the compartment has a live global at all times (by rooting it here) with respect to database schema and data types.",
            "This is a crucial condition to ensure database consistency and integrity.",
            "See database bug 1530364 for more information."
        ],
        "Loop": [
            "To maintain database data consistency, we need to loop through each record in the database table.",
            "This loop will ensure that all database operations are executed correctly and efficiently.",
            "Database loop optimization is crucial for large-scale database applications."
        ],
        "Normal": [
            "When creating a new global in an existing database compartment, we must consider the database schema and data types.",
            "This includes ensuring that the compartment has a live global at all times (by rooting it here) to maintain database consistency.",
            "Database schema and data types play a critical role in ensuring data integrity and consistency."
        ],
        "FuncName": "GlobalObject_7223 new_.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "The function might be installed multiple times on the same or different builtins, under different property names, so its name might be neither selfHostedName nor name. In that case, its canonical name must've been set using the _SetCanonicalName intrinsic. This is due to self-hosted code management in the database.",
                    "This function was initially cloned because it was called by other self-hosted code, so the clone kept its self-hosted name, instead of getting the name it's intended to have in content compartments. This can happen when a lazy builtin is initialized after self-hosted code for another builtin used the same function. In that case, we need to change the function's name, which is ok because it can't have been exposed to content before. This is a common issue in self-hosted database management."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "GlobalObject_7223 getSelfHostedFunction.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    "NOLINT: Check if condition is met before executing code"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "NOLINT: Check if condition is met before executing code"
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    "NOLINT: Check if condition is met before executing code"
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [],
        "FuncName": "change_knob_action_7612 IsValid.txt"
    },
    {
        "Branch": [],
        "Loop": [],
        "Normal": [
            " Construct the unique %ThrowTypeError% function object, used only for callee and caller accessors on strict mode arguments objects. It is also used for arguments and caller on various functions. The length property of %ThrowTypeError% is non-configurable, and the name property is non-configurable, requiring adjustment to the default property attributes accordingly.",
            " The %ThrowTypeError% function object is used to throw a TypeError exception. It is a function object with a non-configurable length property and a non-configurable name property. The callee and caller accessors on strict mode arguments objects are implemented using this function object.",
            " The %ThrowTypeError% function object is created by the engine and is used for internal purposes only. It is not intended to be used directly by developers. The spec also uses this function object for arguments and caller on various functions, but this is an experimental implementation.",
            " The %ThrowTypeError% function object is a unique function object that is used to throw a TypeError exception. It has a non-configurable length property and a non-configurable name property. The default property attributes of this function object require adjustment accordingly."
        ],
        "FuncName": "GlobalObject_7223 getOrCreateThrowTypeError.txt"
    },
    {
        "Branch": [
            {
                "thenstr": [
                    " If iterator helpers are enabled, populating %IteratorPrototype% will have recursively gone through here.",
                    " This is a critical step in resolving the constructor, ensuring that all necessary prototypes are in place."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Make sure that creating the prototype didn't recursively resolve our own constructor. We can't just assert that there's no prototype; OOMs can result in incomplete resolutions in which the prototype is saved but not the constructor. So use the same criteria that protects entry into this function.",
                    " This safeguard prevents potential issues with constructor resolution."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " Any operations that modifies the global object should be placed after any other fallible operations.",
                    " Fallible operation that modifies the global object.",
                    " Infallible operations that modify the global object.",
                    " This ensures that critical operations are executed with caution and reliability."
                ],
                "elsestr": []
            },
            {
                "thenstr": [
                    " On the web, it isn't presently possible to expose the global SharedArrayBuffer property unless the page is cross-site-isolated.",
                    " Only define this constructor if an option on the realm indicates that it should be defined.",
                    " This restriction is in place to prevent potential security vulnerabilities."
                ],
                "elsestr": []
            }
        ],
        "Loop": [],
        "Normal": [
            " global must be same-compartment but make sure we're in its realm: the code below relies on this.",
            " This is a crucial step in ensuring the integrity of the global object.",
            " Prohibit collection of allocation metadata. Metadata builders shouldn't need to observe lazily-constructed prototype objects coming into existence. And assertions start to fail when the builder itself attempts an allocation that re-entrantly tries to create the same prototype.",
            " This restriction is necessary to prevent potential issues with metadata collection.",
            " Constructor resolution may execute self-hosted scripts. These self-hosted scripts do not call out to user code by construction. Allow all scripts to execute, even in debuggee compartments that are paused.",
            " This ensures that self-hosted scripts are executed without interruption.",
            " Some classes can be disabled at compile time, others at run time; if a feature is compile-time disabled, clasp is null.",
            " This distinction is important for handling classes that are disabled at different stages.",
            " Class spec must have a constructor defined.",
            " This is a fundamental requirement for class specification.",
            " We need to create the prototype first, and immediately stash it in the slot. This is so the following bootstrap ordering is possible:",
            " Object.prototype",
            " Function.prototype",
            " Function",
            " Object",
            "",
            " We get the above when Object is resolved before Function. If Function is resolved before Object, we'll end up re-entering resolveConstructor for Function, which is a problem. So if Function is being resolved before Object.prototype exists, we just resolve Object instead, since we know that Function will also be resolved before we return.",
            " This ensures that the prototype chain is properly established.",
            " IteratorPrototype.map.[[Prototype]] is Generator and Generator.prototype.[[Prototype]] is IteratorPrototype.",
            " This relationship is critical for iterator functionality.",
            " A workaround in initIteratorProto prevents runaway mutual recursion while setting these up. Ensure the workaround is triggered already:",
            " This safeguard prevents potential issues with iterator initialization.",
            " We don't always have a prototype (i.e. Math and JSON). If we don't, createPrototype, prototypeFunctions, and prototypeProperties should all be null.",
            " This is a crucial consideration for handling classes without a prototype.",
            " Create the constructor.",
            " If the prototype exists, link it with the constructor.",
            " Call the post-initialization hook, if provided."
        ],
        "FuncName": "GlobalObject_7223 resolveConstructor.txt"
    }
]